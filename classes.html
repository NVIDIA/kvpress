<html>
  <body>
    <script src="https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js"></script>
      <div class="mermaid">
    
        classDiagram
          class A {
            func(x)
          }
          class AHContext {
            context_dict : Dict[str, Value]
            features : List[AHFeature]
            add_feature(name: str, value: Value, is_categorical: bool) None
            apply_operations(operations: List[AHOperation]) None
            get_feature_names_csv() str
            get_feature_values_csv() str
            get_numerical_and_categorical_features() Tuple[List[str], List[str]]
            get_value(name: str) Value
          }
          class AHFeature {
            is_categorical : bool
            name : str
            value : Any
          }
          class AHMetadata {
            choices : List[Choice]
            device_capa : Tuple[int, int]
            name : str
            shared_memory : Any
            to_dict() Dict[str, Value]
          }
          class AHOperation {
            func : Callable[[Any], Value]
            is_categorical : bool
            name : str
            apply_operation(data: Any) None
          }
          class AOTAutogradCache {
            clear()
            get_remote_cache() Optional[RemoteCache[JsonDataTy]]
            load(dispatch_and_compile: Callable, mod: Union[torch.fx.GraphModule, torch._dynamo.utils.GmWrapper], args, aot_config: AOTConfig, cudagraphs: BoxedBool, local: bool, remote: bool) Callable
            save(key: str, entry: AOTAutogradCacheEntry, remote: bool)
          }
          class AOTAutogradCacheDetails {
            aot_config
            autograd_config : NoneType
            deterministic_algorithms
            disable_amp
            grad_enabled
          }
          class AOTAutogradCacheEntry {
            aot_backward_graph_str : Optional[str]
            aot_forward_graph_str : Optional[str]
            aot_joint_graph_str : Optional[str]
            backward_time_taken_ns : int
            compiled_bw : Optional[CompiledBackward]
            compiled_fw
            dispatch_wrappers : List[CompilerWrapper]
            forward_time_taken_ns : int
            indices_of_inps_to_detach : List[int]
            maybe_subclass_meta : Optional[SubclassMeta]
            num_fw_outs_saved_for_bw : Optional[int]
            runtime_metadata
            wrap_post_compile(args: List[torch.Tensor], aot_config: AOTConfig, fx_config: _CompileFxKwargs) Callable
          }
          class AOTAutogradCacheInfo {
            cache_key : str
            start_time_ns : int
          }
          class AOTAutogradCachePickler {
            dispatch_table : Dict
          }
          class AOTConfig {
            aot_autograd_arg_pos_to_source : Optional[List[Source]]
            aot_id : int
            bw_compiler : Callable
            cache_info : Optional[AOTAutogradCacheInfo]
            decompositions : Dict[OpOverload, Callable]
            dynamic_shapes : bool
            enable_log : bool
            fw_compiler : Callable
            inference_compiler : Optional[Callable]
            is_export : bool
            keep_inference_input_mutations : bool
            no_tangents : bool
            num_params_buffers : int
            partition_fn : Callable
            pre_dispatch : bool
            static_input_indices : Optional[List[int]]
          }
          class AOTDedupeWrapper {
            add_dupe_map : List[int]
            keep_arg_mask : List[bool]
            needs_post_compile : bool
            old_input_metadata : List[InputAliasInfo]
            add_dupe_args(args)
            post_compile(compiled_fn, aot_config: AOTConfig)
            pre_compile(flat_fn, flat_args: List[Tensor], aot_config: AOTConfig) Tuple[Callable, List[Tensor], ViewAndMutationMeta]
            remove_dupe_args(args)
          }
          class AOTDispatchAutograd {
            post_compile(compiled_fw_func, compiled_bw_func, maybe_subclass_meta: Optional[SubclassMeta], num_symints_saved_for_bw_: int, backward_state_indices: List[int], disable_amp: bool, indices_of_inps_to_detach: List[int], lazy_backward_info: Optional[AutogradLazyBackwardCompileInfo], aot_config: AOTConfig)
            process_runtime_tangent(x, meta: Union[PlainTensorMeta, SubclassCreationMeta])
          }
          class AOTDispatchCompiler {
          }
          class AOTDispatchSubclassWrapper {
            fw_only : Optional[Callable]
            maybe_subclass_meta : Optional[SubclassMeta]
            num_fw_outs_saved_for_bw : Optional[int]
            trace_joint : bool
            post_compile(compiled_fn, _aot_config: AOTConfig)
            pre_compile(flat_fn, flat_args: List[Tensor], aot_config: AOTConfig)
          }
          class AOTICompiledModel {
            loader
            get_constant_fqns() List[str]
            get_metadata() Dict[str, str]
            load_constants(constants_map: Dict[str, torch.Tensor]) None
          }
          class AOTIMinifierError {
            original_exception
          }
          class AOTModule {
            orig_module
            forward()
          }
          class AOTSyntheticBaseWrapper {
            aliased_arg_idx_with_metadata_mutations : List[int]
            needs_post_compile : bool
            old_input_info
            trace_joint : bool
            post_compile(compiled_fn, aot_config: AOTConfig)
            pre_compile(flat_fn, flat_args: List[Any], aot_config: AOTConfig) Tuple[Callable, List[Tensor], ViewAndMutationMeta]
          }
          class ASGD {
            step(closure)
          }
          class AST_Rewriter {
            rewrite(fn: FunctionType)
            visit_AnnAssign(node)
            visit_Assert(node)
          }
          class ATenExportArtifact {
            constants : Dict[str, Union[torch.Tensor, FakeScriptObject, torch.ScriptObject]]
            gm
            sig
          }
          class AbsTransform {
            codomain
            domain
          }
          class Access {
            aliases : List[str]
            is_output : bool
            operator : str
            seq_num : int
            stack_trace : StackSummary
            stream : int
            type
          }
          class AccessType {
            name
          }
          class AccuracyError {
          }
          class Action {
            name
          }
          class ActivationSparsifier {
            data_groups : Dict[str, Dict]
            defaults : Dict[str, Any]
            model
            state : Dict[str, Any]
            get_mask(name: Optional[str], layer: Optional[nn.Module])
            load_state_dict(state_dict: Dict[str, Any]) None
            register_layer(layer: nn.Module, aggregate_fn, reduce_fn, mask_fn, features, feature_dim)
            squash_mask(attach_sparsify_hook)
            state_dict() Dict[str, Any]
            step()
            unregister_layer(name)
            update_mask(name, data, configs)
          }
          class ActivationWrapper {
            forward()*
            named_parameters() Iterator[Tuple[str, torch.nn.Parameter]]
          }
          class ActivationsTestModel {
            dequant
            elu
            hardswish
            qconfig
            quant
            forward(x)
          }
          class Adadelta {
            step(closure)
          }
          class Adafactor {
            step(closure)
          }
          class Adagrad {
            share_memory()
            step(closure)
          }
          class Adam {
            step(closure)
          }
          class AdamW {
            step(closure)
          }
          class Adamax {
            step(closure)
          }
          class AdaptiveAvgPool1d {
            output_size : Union
            forward(input: Tensor) Tensor
          }
          class AdaptiveAvgPool2d {
            output_size : Union
            forward(input: Tensor) Tensor
          }
          class AdaptiveAvgPool3d {
            output_size : Union
            forward(input: Tensor) Tensor
          }
          class AdaptiveLogSoftmaxWithLoss {
            cutoffs : List[int]
            div_value : float
            head
            head_bias : bool
            head_size
            in_features : int
            n_classes : int
            n_clusters
            shortlist_size
            tail
            forward(input_: Tensor, target_: Tensor) _ASMoutput
            log_prob(input: Tensor) Tensor
            predict(input: Tensor) Tensor
            reset_parameters() None
          }
          class AdaptiveMaxPool1d {
            output_size : Union
            forward(input: Tensor)
          }
          class AdaptiveMaxPool2d {
            output_size : Union
            forward(input: Tensor)
          }
          class AdaptiveMaxPool3d {
            output_size : Union
            forward(input: Tensor)
          }
          class AddInplaceAdd {
            forward(x, y)
          }
          class AddMulScalar {
            forward(x)
          }
          class AddParenHandler {
          }
          class Address {
            absolute_address : int
            fully_qualified_name : Optional[str]
            index : int
            kind : Optional[str]
            length : Optional[int]
            name : Optional[str]
            offset_from_parent : Optional[int]
            parent_index : int
            properties : Optional[_property_bag.PropertyBag]
            relative_address : Optional[int]
          }
          class AffineTransform {
            bijective : bool
            event_dim
            loc
            scale
            sign
            codomain()
            domain()
            forward_shape(shape)
            inverse_shape(shape)
            log_abs_det_jacobian(x, y)
            with_cache(cache_size)
          }
          class Agent {
            agent_rref : RRef
            eps
            ob_rrefs : list
            optimizer
            policy
            reward_threshold : float
            rewards : dict
            running_reward : int
            saved_log_probs : dict
            finish_episode()
            report_reward(ob_id, reward)
            run_episode(n_steps)
            select_action(ob_id, state)
          }
          class AlgorithmSelectorCache {
            feedback_saver_fns : List[Callable[[Dict[ChoiceCaller, float], str, List[Any], List[ChoiceCaller]], None]]
            precompile_cache : Dict[str, Callable[[], None]]
            add_feedback_saver(fn: Callable[[Dict[ChoiceCaller, float], str, List[Any], List[ChoiceCaller]], None])
            benchmark_example_value(node)
            generate_example_value(size, stride, device, dtype, extra_size)
            key_of(node)
            log_results(name: str, input_nodes: List[ir.IRNode], timings: Dict[ChoiceCaller, float], elapse: float, precompile_elapse: float)
            make_benchmark_fn(choices, input_nodes, layout, input_gen_fns)
          }
          class AliasInfo {
            alias_set : Set[str]
            is_write : bool
            name : Optional[str]
          }
          class AliasInfo {
            inplace_variant
            method_variant
            name
            op : object
          }
          class AliasOfInputHandler {
            base_idx
            functional_tensor
            replay_views : bool
            requires_grad
            unwrap_out
          }
          class AliasOfIntermediateHandler {
            base_idx
            functional_tensor
            replay_views : bool
            requires_grad
            unwrap_out
          }
          class AliasViewInfo {
            regenerate_view(bases_list: List[Tensor])
          }
          class AliasesNewOutput {
            index : int
          }
          class AliasesPriorGraphOutput {
            index : Tuple
          }
          class AllGather {
            work(data)
          }
          class AllGatherResult {
            all_gather_event : Optional[torch.Event]
            all_gather_input_split_sizes : List[int]
            all_gather_output
            all_gather_work : Optional[dist.distributed_c10d.Work]
            param_all_gather_input_dtypes : List[List[torch.dtype]]
            param_all_gather_input_numels : List[List[int]]
          }
          class AllGatherState {
            all_gather_result : AllGatherResult
            event
          }
          class AllGatherStates {
            gathered_objects : dict
            proceed_signal : Event
          }
          class AllOrAnyReductionTypePromotionRule {
            preview_type_promotion(args: tuple, kwargs: dict) TypePromotionSnapshot
          }
          class AllReduce {
            op
            work(data)
          }
          class AllReduceState {
            all_reduce_input
            event
          }
          class AllToAll {
            work(data)
          }
          class AllToAllBase {
            work(data)
          }
          class AllocFromPoolLine {
            is_first_pool_usage : bool
            codegen(code: IndentedBuffer)
          }
          class AllocateLine {
            node : Union
            codegen(code: IndentedBuffer) None
            plan(state: MemoryPlanningState) MemoryPlanningLine
          }
          class Allocation {
            allocated : bool
            device
            live_range
            node : Union
            offset : Optional[sympy.Expr]
            pool : Optional[AllocationPool]
            size_hint : int
            symbolic_size : Expr
            codegen_alloc_from_pool(wrapper)
            finalize(pool, offset)
            get_live_ranges()
            get_size_hint()
            get_symbolic_size()
            mark_allocated()
          }
          class AllocationPool {
            can_expand : bool
            creation_cache : Dict[str, str]
            device
            name : Optional[str]
            names_to_del : List[str]
            restrict_live_range : Optional[LiveRange]
            root
            allocate(block: Allocation, is_last: bool)
            allocate_at_end(block)
            codegen_create(wrapper, code: IndentedBuffer)
            codegen_destroy(wrapper, code: IndentedBuffer)
            finalize(name)
          }
          class AllocationPools {
            device_to_pools : Dict[torch.device, List[AllocationPool]]
            allocate(block: Allocation)
            allocate_output(block: Allocation)
            finalize()
            get_pools(block)
            pprint()
          }
          class AllocationTreeNode {
            allocate(block: Allocation, is_last: bool) bool
            finalize(pool, offset) AllocationTreeNode
            get_live_ranges()* LiveRanges
            get_size_hint()* int
            get_symbolic_size()* sympy.Expr
            is_empty()
          }
          class AlphaDropout {
            forward(input: Tensor) Tensor
          }
          class AlwaysWarnTypedStorageRemoval {
            always_warn
            always_warn_restore : bool
          }
          class AlwaysWrapNestedWrappedModule {
            init(group: dist.ProcessGroup, fsdp_init_mode: FSDPInitMode, device_init_mode: DEVICEInitMode, fsdp_kwargs: Optional[Dict[str, Any]], deterministic: bool)
          }
          class AmbiguityWarning {
          }
          class Analysis {
            diagnostic_context
            module
            onnxfunction_dispatcher
            analyze(diagnostic_level: diagnostics.infra.Level)* AnalysisResult
          }
          class AnalysisResult {
          }
          class Animal {
            name
          }
          class AnnotateTypesWithSchema {
            annotate_functionals : bool
            annotate_get_attrs : bool
            annotate_modules : bool
            call_function(target: Target, args: Tuple[Argument, ...], kwargs: Dict[str, Any])
            call_module(target: Target, args: Tuple[Argument, ...], kwargs: Dict[str, Any])
            get_attr(target: torch.fx.node.Target, args: Tuple[Argument, ...], kwargs: Dict[str, Any])
          }
          class AnnotatedConvBnModel {
            bn
            conv
            dequant
            qconfig
            quant
            forward(x)
            get_example_inputs() Tuple[Any, ...]
          }
          class AnnotatedConvBnReLUModel {
            bn
            conv
            dequant
            qconfig
            quant
            relu
            forward(x)
            fuse_model()
            get_example_inputs() Tuple[Any, ...]
          }
          class AnnotatedConvModel {
            conv
            dequant
            qconfig
            quant
            forward(x)
            get_example_inputs() Tuple[Any, ...]
          }
          class AnnotatedConvTransposeModel {
            conv
            dequant
            qconfig
            quant
            forward(x)
            get_example_inputs() Tuple[Any, ...]
          }
          class AnnotatedCustomConfigNestedModel {
            fc3
            sub1
            sub2
            forward(x)
          }
          class AnnotatedNestedModel {
            fc3
            sub1
            sub2
            forward(x)
          }
          class AnnotatedSingleLayerLinearModel {
            fc1
            qconfig
            forward(x)
            get_example_inputs() Tuple[Any, ...]
          }
          class AnnotatedSkipQuantModel {
            fc
            qconfig
            sub
            forward(x)
            fuse_modules()
          }
          class AnnotatedSubNestedModel {
            fc3
            sub1
            sub2
            forward(x)
          }
          class AnnotatedTwoLayerLinearModel {
            fc1
            fc2
            forward(x)
            get_example_inputs() Tuple[Any, ...]
          }
          class AotAutograd {
            kwargs : dict
          }
          class AotCodeCompiler {
            compile(graph: GraphLowering, source_code: str, serialized_extern_kernel_nodes: Optional[str], device_type: str, additional_files: List[str]) Union[List[str], str]
          }
          class AotEagerAndRecordGraphs {
            bw_graphs : List[torch.fx.GraphModule]
            fw_graphs : List[torch.fx.GraphModule]
            graphs : List[torch.fx.GraphModule]
          }
          class AppDirs {
            appauthor : NoneType
            appname : NoneType
            multipath : bool
            roaming : bool
            site_config_dir
            site_data_dir
            user_cache_dir
            user_config_dir
            user_data_dir
            user_log_dir
            user_state_dir
            version : NoneType
          }
          class ApplyBroadcasting {
            input1
            input2
            res1
            res2
          }
          class ApplyTemplate {
            backward(ctx)
            forward(ctx)
          }
          class Arg {
          }
          class ArgMappingException {
          }
          class ArgsKwargsPair {
            args : Tuple[Any, ...]
            kwargs : Dict[str, Any]
          }
          class ArgsMismatchError {
          }
          class Argument {
            as_bool : Annotated[bool, 170]
            as_bools : Annotated[List[bool], 180]
            as_custom_obj : Annotated[CustomObjArgument, 210]
            as_device : Annotated[Device, 160]
            as_float : Annotated[float, 80]
            as_floats : Annotated[List[float], 90]
            as_graph : Annotated[GraphArgument, 200]
            as_int : Annotated[int, 50]
            as_ints : Annotated[List[int], 70]
            as_layout : Annotated[Layout, 150]
            as_memory_format : Annotated[MemoryFormat, 140]
            as_none : Annotated[bool, 10]
            as_operator : Annotated[str, 220]
            as_optional_tensors : Annotated[List[OptionalTensorArgument], 190]
            as_scalar_type : Annotated[ScalarType, 130]
            as_string : Annotated[str, 100]
            as_strings : Annotated[List[str], 101]
            as_sym_bool : Annotated[SymBoolArgument, 182]
            as_sym_bools : Annotated[List[SymBoolArgument], 184]
            as_sym_float : Annotated[SymFloatArgument, 230]
            as_sym_floats : Annotated[List[SymFloatArgument], 240]
            as_sym_int : Annotated[SymIntArgument, 110]
            as_sym_ints : Annotated[List[SymIntArgument], 120]
            as_tensor : Annotated[TensorArgument, 20]
            as_tensors : Annotated[List[TensorArgument], 30]
          }
          class ArgumentHandler {
            dataptrs_read : Set[DataPtr]
            dataptrs_written : Set[DataPtr]
            outputs : Set[DataPtr]
            tensor_aliases : Dict[DataPtr, List[str]]
            parse_inputs(schema: torch.FunctionSchema, args: Tuple[Any, ...], kwargs: Dict[str, Any]) None
            parse_outputs(schema: torch.FunctionSchema, outputs: Any) None
          }
          class Artifact {
            contents : Optional[_artifact_content.ArtifactContent]
            description : Optional[_message.Message]
            encoding : Optional[str]
            hashes : Optional[Any]
            last_modified_time_utc : Optional[str]
            length : int
            location : Optional[_artifact_location.ArtifactLocation]
            mime_type : Optional[str]
            offset : Optional[int]
            parent_index : int
            properties : Optional[_property_bag.PropertyBag]
            roles : Optional[List[Literal['analysisTarget', 'attachment', 'responseFile', 'resultFile', 'standardStream', 'tracedFile', 'unmodified', 'modified', 'added', 'deleted', 'renamed', 'uncontrolled', 'driver', 'extension', 'translation', 'taxonomy', 'policy', 'referencedOnCommandLine', 'memoryContents', 'directory', 'userSpecifiedConfiguration', 'toolSpecifiedConfiguration', 'debugOutputFile']]]
            source_language : Optional[str]
          }
          class ArtifactChange {
            artifact_location
            properties : Optional[_property_bag.PropertyBag]
            replacements : List[_replacement.Replacement]
          }
          class ArtifactContent {
            binary : Optional[str]
            properties : Optional[_property_bag.PropertyBag]
            rendered : Optional[_multiformat_message_string.MultiformatMessageString]
            text : Optional[str]
          }
          class ArtifactLocation {
            description : Optional[_message.Message]
            index : int
            properties : Optional[_property_bag.PropertyBag]
            uri : Optional[str]
            uri_base_id : Optional[str]
          }
          class AsStridedViewInfo {
            size : Sequence[Union[int, torch.SymInt]]
            storage_offset : int
            stride : Sequence[Union[int, torch.SymInt]]
            regenerate_view(bases_list: List[Tensor])
          }
          class AssertRaisesContextIgnoreNotImplementedError {
          }
          class AssertScalar {
            msg
            scalar
            codegen(wrapper) None
            get_reads() OrderedSet[Dep]
            get_unbacked_symbol_uses()
            has_side_effects() bool
            should_allocate() bool
          }
          class AssociativeScanHigherOrderVariable {
            call_function(tx: 'InstructionTranslator', args: List[VariableTracker], kwargs: Dict[str, VariableTracker]) VariableTracker
          }
          class AssociativeScanOp {
          }
          class AssumeConstantResult {
            forward(x, y)
            get_item(y)
          }
          class AsyncClosureHandler {
            run(closure)
            start_event_loop()
          }
          class AsyncCollectiveTensor {
            completed : bool
            elem
            numpy()
            tolist()
            trigger_wait()
            wait() torch.Tensor
          }
          class AsyncCompile {
            cpp(source_code: str)
            cpp_pybinding(argtypes: List[str], source_code: str)
            cuda(source_code, dst_file_ext, aot_compile)
            halide(meta: HalideMeta, source_code: str)
            multi_kernel() Any
            pool() ThreadPoolExecutor
            process_pool() SubprocPool
            rocm(source_code, dst_file_ext, aot_compile)
            submit(task: Callable[..., Any]) Any
            triton(kernel_name: str, source_code: str, device_str: str)
            wait(scope: Dict[str, Any]) None
            warm_pool() None
          }
          class AsyncExecutionClass {
            bound_async_add(to, x, y, z)
            class_async_add(to, x, y, z)
            static_async_add(to, x, y, z)
          }
          class AsyncStager {
            should_synchronize_after_execute
            stage(state_dict: STATE_DICT_TYPE)* STATE_DICT_TYPE
            synchronize_staging()* None
          }
          class Attachment {
            artifact_location
            description : Optional[_message.Message]
            properties : Optional[_property_bag.PropertyBag]
            rectangles : Optional[List[_rectangle.Rectangle]]
            regions : Optional[List[_region.Region]]
          }
          class Attention {
            dropout_p
            head_dim
            n_heads
            resid_dropout
            use_attn_mask
            wk
            wo
            wq
            wv
            forward(x)
          }
          class AttrProxy {
            get_base() Module
            reset_proxy_mapping(base: Module, path: str) None
          }
          class AttrProxySource {
            guard_source()
            name()
            reconstruct(codegen)
          }
          class AttrSource {
            member : str
            guard_source()
            name()
            reconstruct(codegen)
          }
          class Attribute {
            attr : str
            node
            root
            tracer
          }
          class AttributeMutation {
          }
          class AttributeMutationError {
          }
          class AttributeMutationExisting {
          }
          class AttributeMutationNew {
            cls_source : Optional[Source]
          }
          class AttributeParameter {
            default : ir.Attr | None
            name : str
            required : bool
            type
            has_default() bool
          }
          class AttributeTypeIsSupportedChecker {
            class_level_annotations : list
            visiting_class_level_ann : bool
            check(nn_module: torch.nn.Module) None
            visit_AnnAssign(node)
            visit_Assign(node)
            visit_Call(node)
          }
          class AutoDynamic {
            name
          }
          class AutoFunctionalizeHigherOrderVariable {
            call_function(tx, args: 'List[VariableTracker]', kwargs: 'Dict[str, VariableTracker]') 'VariableTracker'
          }
          class AutoFunctionalized {
          }
          class AutoFunctionalizedV2 {
          }
          class AutoHeuristic {
            augment_context : Optional[List[AHOperation]]
            choices : List[Choice]
            collected_feedback : Dict[Choice, Feedback]
            context
            fallback : Callable[[], Choice]
            feedback : Optional[LocalFeedback]
            log_path : str
            metadata
            name : str
            precondition : Optional[Callable[[AHMetadata, AHContext], bool]]
            get_choice() Choice
            get_collected_feedback(choice: Choice) Any
            get_default_log_path() str
            get_device_identifier() str
            get_top_k_choices(top_k: int, always_included: Optional[List[str]]) Optional[List[Choice]]
            satisfies_precondition() bool
            save_data(choice: Choice, feedback_val: Feedback) None
            serialize_metadata() str
          }
          class AutoHeuristicSelectAlgorithm {
            choicestr2choice : Dict[str, ChoiceCaller]
            input_nodes : List[Any]
            get_choice_caller() Optional[ChoiceCaller]
            get_top_k_choices_caller(top_k: int, always_included: Optional[List[str]]) Optional[List[ChoiceCaller]]
            register_global_feedback(input_nodes: List[Any], choices: List[ChoiceCaller]) None
          }
          class AutoUnset {
            name
          }
          class AutocastCPUTestLists {
            methods_expect_builtin_promote : list
            nn_16 : list
            nn_fp32 : list
            torch_16 : list
            torch_expect_builtin_promote : list
            torch_fp32 : list
            torch_need_autocast_promote : list
          }
          class AutocastModeVariable {
            target_values
            create(func, args, kwargs)
            enter(tx)
            exit(tx: 'InstructionTranslator')
            fn_name()
            module_name()
          }
          class AutocastTestLists {
            banned : list
            linalg_fp16 : list
            methods_expect_builtin_promote : list
            methods_fp16 : list
            methods_fp32 : list
            nn_fp16 : list
            nn_fp32 : list
            torch_expect_builtin_promote : list
            torch_fp16 : list
            torch_fp32 : list
            torch_need_autocast_promote : list
          }
          class AutogradCompilerInstance {
            aot_graph_cls_name : Optional[str]
            aot_graph_infos : Dict[int, Dict[str, Any]]
            close
            compiler_fn
            fake_tensor_mode
            fx_tracer
            hooks_proxy : Optional[Proxy]
            proxy_mode
            shape_env
            stack : ExitStack
            begin_capture(inputs: List[torch.Tensor], sizes: List[int], scalars: List[Union[int, float]], origins: List[List[Tuple[int, str]]])
            bind_backward_state(index: int)
            bind_tensors_to_proxies(tensors, proxies, origins: Optional[List[Tuple[int, str]]])
            dce()
            end_capture(outputs)
            get_all_nodes(args)
            is_placeholder(node)
            is_sym_node(node)
            move_graph_nodes_to_cuda(graph) List[int]
            post_acc_grad_hook(input, hook_id)
            post_hook(outputs, inputs, hook_id)
            pre_hook(inputs, hook_id)
            proxy_call_backward(inputs, output_metadatas, saved_tensors, backward_idx: int)
            proxy_call_hook(hook)
            rename_aot_dispatcher_nodes()
            reorder_accumulate_grad_nodes()
            reorder_post_acc_grad_hook_nodes()
            reorder_post_hook_nodes()
            reorder_pre_hook_nodes_to_mimic_eager()
            reorder_pre_hook_nodes_to_schedule_asap()
            reorder_tensor_pre_hook_nodes()
            set_node_origin(node_name: str, nodecall_index: int, pyobj: Optional[torch.autograd.Function])
            source(name, idx) GetItemSource
            tensor_pre_hook(inputs, hook_id, i: int)
            to_proxy(t)
            wrap_fake(x, source)
          }
          class AutogradEngineVariable {
            call_method(tx, name, args: 'List[VariableTracker]', kwargs: 'Dict[str, VariableTracker]') 'VariableTracker'
          }
          class AutogradFunction {
            forward(x)
          }
          class AutogradFunctionApply {
          }
          class AutogradFunctionApplyVariable {
            bwd_graph
            fwd_graph
            parent_source
            call_function(tx: 'InstructionTranslator', args: 'List[VariableTracker]', kwargs: 'Dict[str, VariableTracker]') 'VariableTracker'
          }
          class AutogradFunctionContextVariable {
            inference : bool
            needs_input_grad : NoneType
            non_differentiable : NoneType, tuple
            proxy : NoneType
            saved_tensors : NoneType
            as_proxy()
            call_method(tx, name, args: 'List[VariableTracker]', kwargs: 'Dict[str, VariableTracker]') 'VariableTracker'
            create(tx: 'InstructionTranslator', args, kwargs)
            var_getattr(tx: 'InstructionTranslator', name)
          }
          class AutogradFunctionVariable {
            fn_cls
            call_apply(tx: 'InstructionTranslator', args, kwargs)
            call_backward(tx: 'InstructionTranslator', args, kwargs)
            call_function(tx: 'InstructionTranslator', args, kwargs)
            call_method(tx, name, args: 'List[VariableTracker]', kwargs: 'Dict[str, VariableTracker]')
          }
          class AutogradLazyBackwardCompileInfo {
            bw_module : Callable
            placeholder_list : List[Any]
            saved_compile_context : Optional[CompileContext]
            saved_context : Optional[TracingContext]
          }
          class AutogradStateOpsFailSafeguard {
          }
          class AutotuneArgs {
            expected : Optional[torch.Tensor]
            extern
            triton
            from_choice_args(example_inputs: List[torch.Tensor], example_inputs_extern: List[torch.Tensor], out: torch.Tensor, out_extern: torch.Tensor, expected: Optional[torch.Tensor]) _T
            get_benchmark_tensors(extern) BenchmarkTensors
            verify()
          }
          class AutotuneCache {
            configs_hash : str
            local_cache : Optional[Tuple[RemoteCache[JsonDataTy], str]]
            remote_cache : Optional[Tuple[RemoteCache[JsonDataTy], str]]
            create(inductor_meta: _InductorMetaTy, filename: str, configs_hash: str) Optional[AutotuneCache]
            read_best(inductor_meta: _InductorMetaTy, configs: List[Config]) Optional[Config]
            save(config: Config, time_taken_ns: int, found_by_coordesc: bool) None
          }
          class AutotuneCacheBundler {
            begin_compile(inductor_meta: _InductorMetaTy) None
            end_compile() None
            put(filename: str, data: JsonDataTy) None
            sync() None
          }
          class AutotuneHint {
            name
          }
          class AverageMeter {
            avg : int
            count : int
            fmt : str
            name
            sum : int
            val : int
            reset()
            update(val, n)
          }
          class AveragedModel {
            avg_fn : Optional[Callable[[Tensor, Tensor, Union[Tensor, int]], Tensor]]
            module
            multi_avg_fn : Optional[Callable[[PARAM_LIST, PARAM_LIST, Union[Tensor, int]], None]]
            n_averaged
            use_buffers : bool
            forward()
            update_parameters(model: Module)
          }
          class AvgPool1d {
            ceil_mode : bool
            count_include_pad : bool
            kernel_size : Union
            padding : Union
            stride : Union
            forward(input: Tensor) Tensor
          }
          class AvgPool2d {
            ceil_mode : bool
            count_include_pad : bool
            divisor_override : Optional[int]
            kernel_size : Union
            padding : Union
            stride : Union
            forward(input: Tensor) Tensor
          }
          class AvgPool3d {
            ceil_mode : bool
            count_include_pad : bool
            divisor_override : Optional[int]
            kernel_size : Union
            padding : Union
            stride : Union
            forward(input: Tensor) Tensor
          }
          class AxisError {
          }
          class BCELoss {
            forward(input: Tensor, target: Tensor) Tensor
          }
          class BCEWithLogitsLoss {
            pos_weight : Optional[Tensor]
            weight : Optional[Tensor]
            forward(input: Tensor, target: Tensor) Tensor
          }
          class BFloat16Storage {
            dtype()
          }
          class BFloat16Storage {
            dtype()
          }
          class BNReLU2d {
            forward(input)
            from_float(mod, use_precomputed_fake_quant)
            from_reference(bn_relu, output_scale, output_zero_point)
          }
          class BNReLU2d {
          }
          class BNReLU3d {
            forward(input)
            from_float(mod, use_precomputed_fake_quant)
            from_reference(bn_relu, output_scale, output_zero_point)
          }
          class BNReLU3d {
          }
          class BVar {
            c
          }
          class Backend {
            GLOO : str
            MPI : str
            NCCL : str
            UCC : str
            UNDEFINED : str
            XCCL : str
            backend_capability : Dict[str, List[str]]
            backend_list : list
            backend_type_map : Dict[str, ProcessGroup.BackendType]
            default_device_backend_map : Dict[str, str]
            register_backend(name, func, extended_api, devices: Optional[Union[str, List[str]]]) None
          }
          class BackendCompilerFailed {
            backend_name
            inner_exception
          }
          class BackendConfig {
            configs
            name : str
            from_dict(backend_config_dict: Dict[str, Any]) BackendConfig
            set_backend_pattern_config(config: BackendPatternConfig) BackendConfig
            set_backend_pattern_configs(configs: List[BackendPatternConfig]) BackendConfig
            set_name(name: str) BackendConfig
            to_dict() Dict[str, Any]
          }
          class BackendConfig {
            device_backend_map : Dict[str, Backend], dict
            get_device_backend_map() Dict[str, Backend]
          }
          class BackendFeature {
            name
          }
          class BackendPatternConfig {
            dtype_configs : List[DTypeConfig]
            fused_module : Optional[Type[torch.nn.Module]]
            fuser_method : Optional[Callable]
            observation_type : OUTPUT_USE_DIFFERENT_OBSERVER_AS_INPUT
            pattern : Optional[Pattern], Union
            qat_module : Optional[Type[torch.nn.Module]]
            reference_quantized_module : Optional[Type[torch.nn.Module]]
            root_module : Optional[Type[torch.nn.Module]]
            add_dtype_config(dtype_config: DTypeConfig) BackendPatternConfig
            from_dict(backend_pattern_config_dict: Dict[str, Any]) BackendPatternConfig
            set_dtype_configs(dtype_configs: List[DTypeConfig]) BackendPatternConfig
            set_fused_module(fused_module: Type[torch.nn.Module]) BackendPatternConfig
            set_fuser_method(fuser_method: Callable) BackendPatternConfig
            set_observation_type(observation_type: ObservationType) BackendPatternConfig
            set_pattern(pattern: Pattern) BackendPatternConfig
            set_qat_module(qat_module: Type[torch.nn.Module]) BackendPatternConfig
            set_reference_quantized_module(reference_quantized_module: Type[torch.nn.Module]) BackendPatternConfig
            set_root_module(root_module: Type[torch.nn.Module]) BackendPatternConfig
            to_dict() Dict[str, Any]
          }
          class BackwardBfsArgIter {
            add_children(node: Node) None
            create(origin: Node) 'BackwardBfsArgIter'
            next() Optional[Node]
            peek() Optional[Node]
          }
          class BackwardCFunction {
            apply()
            apply_jvp()
          }
          class BackwardHook {
            grad_outputs : NoneType, tuple
            input_tensors_index : NoneType, list
            module
            n_inputs : int
            n_outputs : int
            output_tensors_index : NoneType, list
            user_hooks
            user_pre_hooks
            setup_input_hook(args)
            setup_output_hook(args)
          }
          class BackwardHookFunction {
            backward(ctx)
            forward(ctx)
          }
          class BackwardHookVariable {
            module
            proxy
            user_hooks
            user_pre_hooks
            as_proxy()
            call_method(tx, name, args: List[VariableTracker], kwargs: Dict[str, VariableTracker]) VariableTracker
            create(tx, module: VariableTracker, user_hooks: VariableTracker, user_pre_hooks: VariableTracker)
          }
          class BackwardPrefetch {
            name
          }
          class BackwardSignature {
            gradients_to_parameters : Dict[str, str]
            gradients_to_user_inputs : Dict[str, str]
            loss_output : str
          }
          class BackwardState {
            proxy
          }
          class BackwardStateGraphArg {
            reconstruct(codegen)
          }
          class BackwardStateSource {
            guard_source()
            name()
          }
          class BackwardsNotSupported {
            backward(ctx)
            forward(ctx, args_spec)
          }
          class BadModule {
          }
          class Bar {
          }
          class Barrier {
            barrier_id : int
            init()
            sync(wait_for, timeout)
          }
          class BaseConstant {
            device
            dtype
            get_device() Optional[torch.device]
            get_origin_node() Optional[torch.fx.Node]
            get_reads() OrderedSet[Dep]
            get_size() Sequence[Expr]
          }
          class BaseDataScheduler {
            base_param
            data_sparsifier
            last_epoch : int
            schedule_param : str
            verbose : bool
            get_last_param()
            get_schedule_param()*
            load_state_dict(state_dict)
            state_dict()
            step()
          }
          class BaseDataSparsifier {
            data_groups : Dict[str, Dict]
            add_data(name: str, data, reuse_mask)
            get_data(name: str, return_original: bool)
            get_mask(name: str)
            load_state_dict(state_dict, strict)
            prepare(model, config)*
            squash_mask()
            state_dict()
            step()
            update_mask(name, data)*
          }
          class BaseFunctionalizeAPI {
            commit_update(tensor)* None
            functionalize(inner_f: Callable)* Callable
            mark_mutation_hidden_from_autograd(tensor)* None
            redispatch_to_next()* ContextManager
            replace(input_tensor, output_tensor)* None
            sync(tensor)* None
            unwrap_tensors(args: Union[torch.Tensor, Tuple[torch.Tensor, ...]])* Any
            wrap_tensors(args: Tuple[Any])* Tuple[Any]
          }
          class BaseListVariable {
            items : List[VariableTracker]
            value
            as_proxy()
            as_python_constant()
            call_method(tx, name, args: List['VariableTracker'], kwargs: Dict[str, 'VariableTracker']) 'VariableTracker'
            cls_for(obj)
            cls_for_instance(obj)
            debug_repr_helper(prefix, suffix)
            getitem_const(tx: 'InstructionTranslator', arg: VariableTracker)
            list_compare(tx: 'InstructionTranslator', op, left, right)
            modified(items)
            unpack_var_sequence(tx)
          }
          class BasePruningMethod {
            apply(module, name)
            apply_mask(module)
            compute_mask(t, default_mask)*
            prune(t, default_mask, importance_scores)
            remove(module)
          }
          class BaseScheduler {
            base_sl
            last_epoch : int
            sparsifier
            verbose : bool
            get_last_sl()
            get_sl()
            load_state_dict(state_dict)
            print_sl(is_verbose, group, sl, epoch)
            state_dict()
            step(epoch)
          }
          class BaseSchedulerNode {
            ancestors
            debug_device_str : Callable[[BaseSchedulerNode], List[str]]
            group : Tuple[torch.device, Tuple[Tuple[sympy.Expr, ...], ...]]
            last_usage
            max_order : int
            min_order : int
            mpi_node
            node : Optional[ir.Operation]
            outputs : List[SchedulerBuffer]
            outputs_by_name : Dict[str, SchedulerBuffer]
            read_writes
            scheduler
            unmet_dependencies : OrderedSet[Dep]
            written : bool
            add_fake_dep(dep: Dep) None
            can_inplace(read_dep: dependencies.Dep) bool
            codegen_originating_info(buffer: IndentedBuffer, only_once: bool) None
            debug_str() str
            debug_str_extra() str
            debug_str_short() str
            decide_inplace_update() None
            get_buffer_names() OrderedSet[str]
            get_device() Optional[torch.device]
            get_estimated_runtime() float
            get_first_name() str
            get_name() str
            get_nodes() Sequence[BaseSchedulerNode]
            get_operation_names() OrderedSet[str]
            get_output(buf_name: str) SchedulerBuffer
            get_outputs() Sequence[SchedulerBuffer]
            get_prologue_template_epilogue(nodes: List[BaseSchedulerNode]) Tuple[List[BaseSchedulerNode], BaseSchedulerNode, List[BaseSchedulerNode]]
            get_read_buffer_sizes() int
            get_read_write_buffers_sizes() int
            get_read_write_buffers_sizes_impl(include_reads: bool, include_writes: bool) int
            get_template_node() Optional[ir.TemplateBuffer]
            get_template_node_or_throw() ir.TemplateBuffer
            get_write_buffer_sizes() int
            has_aliasing_or_mutation() bool
            has_side_effects() bool
            is_cpu() bool
            is_extern() bool
            is_foreach() bool
            is_gpu() bool
            is_reduction() bool
            is_split_scan() bool
            is_template() bool
            log_details() None
            mark_run() None
            prune_deps() None
            prune_redundant_deps(name_to_fused_node: Dict[str, BaseSchedulerNode]) None
            prune_weak_deps() None
            reorder_loops_by_dep_pair(self_dep: MemoryDep, other_dep: MemoryDep) None
            set_last_usage(future_used_buffers: OrderedSet[str], mutation_real_name: Dict[str, str]) None
            set_read_writes(rw: dependencies.ReadWrites) None
            update_mutated_names(renames: Dict[str, str]) None
            used_buffer_names() OrderedSet[str]
            used_or_aliased_buffer_names() OrderedSet[str]
          }
          class BaseScheduling {
            benchmark_combo_kernel(node_list: Sequence[BaseSchedulerNode])* Tuple[float, float, str]
            benchmark_fused_nodes(nodes: Sequence[BaseSchedulerNode])* Tuple[float, str]
            can_fuse_horizontal(node1: BaseSchedulerNode, node2: BaseSchedulerNode)* bool
            can_fuse_vertical(node1: BaseSchedulerNode, node2: BaseSchedulerNode)* bool
            codegen_node(node: Union[FusedSchedulerNode, SchedulerNode])* None
            codegen_sync()* None
            codegen_template(template_node: BaseSchedulerNode, epilogue_nodes: Sequence[BaseSchedulerNode], prologue_nodes: Sequence[BaseSchedulerNode])* Optional[str]
            flush()* None
            fuse(node1: BaseSchedulerNode, node2: BaseSchedulerNode) FusedSchedulerNode
            get_backend_features(device: torch.device) Sequence[BackendFeature]
            get_fusion_pair_priority(node1: BaseSchedulerNode, node2: BaseSchedulerNode) int
            group_fn(sizes: Sequence[Sequence[sympy.Expr]])* Tuple[Tuple[sympy.Expr, ...], ...]
            ready_to_flush() bool
          }
          class BaseSparsifier {
            config : list
            defaults : Dict[str, Any]
            enable_mask_update : bool
            groups : List[Dict[str, Any]]
            model
            state : Dict[str, Dict]
            convert(module: nn.Module, mapping: Optional[Dict[Type[nn.Module], Type[nn.Module]]], inplace: bool, parameterization: Type[nn.Module])
            load_state_dict(state_dict: Dict[str, Any], strict: bool)
            make_config_from_model(model: nn.Module, SUPPORTED_MODULES: Set[Type]) None
            prepare(model, config)
            squash_mask(params_to_keep: Optional[Tuple[str, ...]], params_to_keep_per_layer: Optional[Dict[str, Tuple[str, ...]]])
            state_dict() Dict[str, Any]
            step(use_path: bool) None
            update_mask(module: nn.Module, tensor_name: str)*
          }
          class BaseStructuredSparsifier {
            patterns : NoneType, dict
            traced
            make_config_from_model(model: nn.Module, SUPPORTED_MODULES: Optional[Set[Type]]) None
            prune() None
          }
          class BaseTorchDispatchMode {
          }
          class BaseTorchFunctionMode {
          }
          class BaseTorchVariable {
            value
            as_proxy()
            as_python_constant()
            call_hasattr(tx: 'InstructionTranslator', name)
            can_constant_fold_through()
            create_with_source(value, source)
            reconstruct(codegen)
          }
          class BaseUserFunctionVariable {
            call_function(tx: 'InstructionTranslator', args: 'List[VariableTracker]', kwargs: 'Dict[str, VariableTracker]') 'VariableTracker'
            call_hasattr(tx: 'InstructionTranslator', name: str) VariableTracker
            closure_vars(tx)
            get_filename()
            get_name()
            inspect_parameter_names()
          }
          class BaseView {
            data
            dtype
            constant_to_device(device: torch.device) IRNode
            get_device() Optional[torch.device]
            get_layout() Layout
            get_name() str
            get_origin_node() Optional[torch.fx.Node]
            get_pointwise_size() Sequence[Expr]
            get_read_names() OrderedSet[str]
            get_reads() OrderedSet[Dep]
            get_storage_numel()
            get_unbacked_symbol_uses() OrderedSet[Symbol]
            has_exceeded_max_reads() bool
            is_extern() bool
            is_module_buffer() bool
            make_indexer() Callable[[Sequence[Expr]], Expr]
            make_loader() Callable[[Sequence[Expr]], OpsValue]
            make_reindexer()* Callable[[Sequence[Expr]], Sequence[Expr]]
            mark_reuse(users: int) None
            realize() Optional[str]
            realize_hint()
            unwrap_view()
          }
          class BasicEvaluation {
            cuda_events : List[_KinetoEvent]
            event_keys
            events
            metrics : Dict[EventKey, EventMetrics]
            profile
            queue_depth_list : list
            compute_idle_time()
            compute_queue_depth()
            compute_self_time()
            get_optimizable_events(length: int, print_enable: bool)
            rank_events(length)
          }
          class BatchAddPostGradFusion {
          }
          class BatchClampPreGradFusion {
          }
          class BatchDetachPreGradFusion {
          }
          class BatchDivPostGradFusion {
          }
          class BatchFusion {
          }
          class BatchLayernormFusion {
            fuse(graph: torch.fx.GraphModule, subset: List[torch.fx.Node])
            match(node: torch.fx.Node)
          }
          class BatchLinearLHSFusion {
            fuse(graph: torch.fx.GraphModule, subset: List[torch.fx.Node])
            match(node: torch.fx.Node) Optional[Tuple[str, bool, Any]]
          }
          class BatchMathOpsPreGradFusion {
            op
            fuse(graph: torch.fx.GraphModule, subset: List[torch.fx.Node])
            match(node: torch.fx.Node)
          }
          class BatchMulPostGradFusion {
          }
          class BatchNanToNumPreGradFusion {
          }
          class BatchNorm1d {
            bias
            num_batches_tracked
            running_mean
            running_var
            training : bool
            weight
          }
          class BatchNorm2d {
            forward(input: torch.Tensor) torch.Tensor
            from_float(mod, use_precomputed_fake_quant)
          }
          class BatchNorm2d {
            bias
            num_batches_tracked
            running_mean
            running_var
            training : bool
            weight
          }
          class BatchNorm3d {
            forward(input: torch.Tensor) torch.Tensor
            from_float(mod, use_precomputed_fake_quant)
          }
          class BatchNorm3d {
          }
          class BatchNormNet {
            bn
            fc1
            fc2
            forward(x)
          }
          class BatchNormQuantizeHandler {
          }
          class BatchPointwiseMathOpsPostGradFusion {
            op
            fuse(graph: torch.fx.GraphModule, subset: List[torch.fx.Node])
            match(node: torch.fx.Node)
          }
          class BatchPointwiseOpsFusionFactory {
            op
          }
          class BatchPointwiseOpsPostGradFusion {
            op
            fuse(graph: torch.fx.GraphModule, subset: List[torch.fx.Node])
            match(node: torch.fx.Node)
          }
          class BatchPointwiseOpsPreGradFusion {
            op
            fuse(graph: torch.fx.GraphModule, subset: List[torch.fx.Node])
            match(node: torch.fx.Node)
          }
          class BatchReLuPostGradFusion {
          }
          class BatchReLuPreGradFusion {
          }
          class BatchSampler {
            batch_size : int
            drop_last : bool
            sampler : Union[Sampler[int], Iterable[int]]
          }
          class BatchSigmoidPostGradFusion {
          }
          class BatchSigmoidPreGradFusion {
          }
          class BatchSubPostGradFusion {
          }
          class BatchTanhPostGradFusion {
          }
          class BatchTanhPreGradFusion {
          }
          class BatchUpdateParameterServer {
            batch_update_size
            curr_update_size : int
            future_model
            lock : lock
            model
            optimizer
            get_model()
            update_and_fetch_model(ps_rref, grads)
          }
          class BatcherIterDataPipe {
            batch_size : int
            datapipe
            drop_last : bool
            wrapper_class : Type[DataChunk]
          }
          class BatcherMapDataPipe {
            batch_size : int
            datapipe
            drop_last : bool
            wrapper_class : Type[DataChunk]
          }
          class BenchmarkRequest {
            extra_args : Iterable[Any]
            input_tensor_meta : Union[TensorMeta, List[TensorMeta]]
            kernel_name : str
            output_tensor_meta : Union[TensorMeta, List[TensorMeta]]
            benchmark() float
            cleanup_run_fn()* None
            do_bench(fn)* float
            make_run_fn()* Callable[[], None]
          }
          class BenchmarkTensors {
            input_tensors : List[torch.Tensor]
            output_tensor : Optional[torch.Tensor]
            unpack()
          }
          class Benchmarker {
            benchmark(fn: Callable[..., Any], fn_args: Tuple[Any, ...], fn_kwargs: Dict[str, Any]) float
            benchmark_cpu(_callable: Callable[[], Any], warmup: int, rep: int) float
            benchmark_gpu()* float
          }
          class Bernoulli {
            arg_constraints : dict
            has_enumerate_support : bool
            logits
            mean
            mode
            param_shape
            probs
            support
            variance
            entropy()
            enumerate_support(expand)
            expand(batch_shape, _instance)
            log_prob(value)
            logits()
            probs()
            sample(sample_shape)
          }
          class Beta {
            arg_constraints : dict
            concentration0
            concentration1
            has_rsample : bool
            mean
            mode
            support
            variance
            entropy()
            expand(batch_shape, _instance)
            log_prob(value)
            rsample(sample_shape: _size) torch.Tensor
          }
          class BiasHook {
            param
            prune_bias
          }
          class Bilinear {
            bias
            in1_features : int
            in2_features : int
            out_features : int
            weight
            extra_repr() str
            forward(input1: Tensor, input2: Tensor) Tensor
            reset_parameters() None
          }
          class BinConstraintD {
          }
          class BinConstraintT {
          }
          class BinaryConstraint {
            lhs
            op
            rhs
          }
          class BinaryOpFuzzer {
          }
          class BinaryOpQuantizeHandler {
          }
          class BinaryOpSparseFuzzer {
          }
          class BinarySubsystem {
          }
          class BinaryUfuncInfo {
            always_returns_bool : bool
            lhs_make_tensor_kwargs : NoneType, dict
            rhs_make_tensor_kwargs : NoneType, dict
            sample_kwargs
            supports_one_python_scalar : bool
            supports_rhs_python_scalar : bool
            supports_two_python_scalars : bool
          }
          class BinaryUnaryAttr {
            algorithm_attr : str
            alpha : float
            binary_op_name : str
            scalars_attr : list
            unary_op_name : str
          }
          class BindInputStep {
            apply(model_args: Sequence[Any], model_kwargs: Mapping[str, Any], model: torch.nn.Module | Callable | torch_export.ExportedProgram | None) tuple[Sequence[Any], Mapping[str, Any]]
          }
          class Binomial {
            arg_constraints : dict
            has_enumerate_support : bool
            logits
            mean
            mode
            param_shape
            probs
            total_count
            variance
            entropy()
            enumerate_support(expand)
            expand(batch_shape, _instance)
            log_prob(value)
            logits()
            probs()
            sample(sample_shape)
            support()
          }
          class BisectSubsystem {
          }
          class BisectValidationException {
            details : str
            msg : str
          }
          class BisectionResult {
            backend : str
            bisect_number : Optional[int]
            debug_info : Optional[str]
            subsystem : Optional[str]
          }
          class BitwiseFn {
            eval(a, b)
          }
          class BlockMask {
            BLOCK_SIZE : Tuple[int, int]
            full_kv_indices : Optional[Tensor]
            full_kv_num_blocks : Optional[Tensor]
            full_q_indices : Optional[Tensor]
            full_q_num_blocks : Optional[Tensor]
            kv_indices
            kv_num_blocks
            mask_mod : Callable
            q_indices : Optional[Tensor]
            q_num_blocks : Optional[Tensor]
            seq_lengths : Tuple[int, int]
            shape
            as_tuple(flatten: bool)
            from_kv_blocks(kv_num_blocks: Tensor, kv_indices: Tensor, full_kv_num_blocks: Optional[Tensor], full_kv_indices: Optional[Tensor], BLOCK_SIZE: Union[int, Tuple[int, int]], mask_mod: Optional[_mask_mod_signature], seq_lengths: Optional[Tuple[int, int]])
            numel()
            sparsity() float
            to(device: Union[torch.device, str]) 'BlockMask'
            to_dense() Tensor
            to_string(grid_size, limit)
          }
          class BlockParameters {
            block_shape : List[sympy.Expr]
            offsets : List[sympy.Expr]
            shape : List[sympy.Expr]
            strides : List[sympy.Expr]
          }
          class BlockPatternMatcher {
            get_slice_numels(dims: List[Expr]) List[Expr]
            get_subexpr_involving_symbol(expr: Expr, symbol: Symbol) Expr
            match_mod_div_block_expr(index: Expr, index_var: Symbol, numel: Expr, num_dims: int) Optional[Tuple[List[Expr], List[Expr], List[Expr]]]
          }
          class BlockPtrOptions {
            block_shape
            broadcast_shape : Sequence[sympy.Expr]
            broadcasting_dims : List[bool]
            constant_offset : Expr
            final_shape : Sequence[sympy.Expr]
            mask_vars : OrderedSet[str]
            offsets
            order : List[int]
            params
            shape
            strides
            advance_roffset()
            boundary_check()
            codegen_broadcast_and_reshape(value: str, initial_shape: Sequence[sympy.Expr], final_shape: Sequence[sympy.Expr], allow_implicit: bool) str
            compute_boundary_check(get_max_block: Callable[[str], int]) None
            create() BlockPtrOptions
            format(name: str, roffset) str
            has_indirect()
            has_mask()
            has_rindex() bool
            has_rmask()
            has_tmpmask()
            replace_roffset(expr: sympy.Expr, replacement: sympy.Expr) sympy.Expr
          }
          class BlockStackEntry {
            inst
            stack_index : Optional[int]
            target
            with_context : Optional[Union[ContextWrappingVariable, GenericContextWrappingVariable]]
            can_restore()
            exit(tx, is_graph_break)
            resume_fn()
          }
          class BlockingAsyncStager {
            cache_staged_state_dict : bool
            state_dict_cache : Optional[STATE_DICT_TYPE], tuple
            type_check : bool
            stage(state_dict: STATE_DICT_TYPE) STATE_DICT_TYPE
            synchronize_staging()* None
          }
          class BoolStorage {
            dtype()
          }
          class BoolStorage {
            dtype()
          }
          class Boolean {
          }
          class BooleanPair {
            compare() None
          }
          class BoundVars {
            loop_body
            replacement_vals
            unbounded_vars
            get_bounds() Dict[torch.fx.Node, ValueRanges[Expr]]
            get_index(name: str) ValueRanges[Expr]
            masked_subblock(subblock: LoopBodyBlock, env: Dict[torch.fx.Node, ValueRanges[Expr]], mask: Any, value: Any, submodules: Dict[str, Callable[..., Any]]) ValueRanges[Expr]
            set_indirect(old: Expr, new: ValueRanges[Expr]) ValueRanges[Expr]
            swap_submodules(submodules: Dict[str, Callable[..., Any]]) Dict[str, Callable[..., ValueRanges[Expr]]]
          }
          class BoxedBool {
            value : bool
            disable(obj)
          }
          class BoxedDeviceIndex {
            value : Optional[int]
            set(device_idx: Optional[int])
          }
          class BracesBuffer {
            is_vec : bool
            indent(offset)
          }
          class Broadcast {
            dim
            dim_size : int
            inputs() Iterable[DimSpec]
            new(dim: DimSpec, dim_size: int) DimSpec
          }
          class Broadcast {
            src
            work(data)
          }
          class Broadcast {
            backward(ctx)
            forward(ctx, target_gpus)
          }
          class BroadcastingListCls {
          }
          class BroadcastingTorchSaveReader {
            checkpoint_id : Optional[Union[str, os.PathLike, None]], Optional[Union[str, os.PathLike]]
            coordinator_rank : int
            is_coordinator : bool
            prepare_global_plan(global_plan: List[LoadPlan]) List[LoadPlan]
            prepare_local_plan(plan: LoadPlan) LoadPlan
            read_data(plan: LoadPlan, planner: LoadPlanner) Future[None]
            read_metadata() Metadata
            reset(checkpoint_id: Union[str, os.PathLike, None]) None
            set_up_storage_reader(metadata: Metadata, is_coordinator: bool) None
            validate_checkpoint_id(checkpoint_id: Union[str, os.PathLike]) bool
          }
          class BuckTargetWriter {
            cmd_line_path : str
            path : str
            py_file
            subdir
            target
            build()
            write(print_msg)
          }
          class Bucket {
            nodes : List[fx.Node]
            opcount_increased_to_capture_external_output : int
            param_ids : List
            params : List[str]
            paramsize_before_opcount_increase : int
            size : int
          }
          class Buffer {
            dtype
            layout
            name : Optional[str]
            codegen_reference(writer: Optional[IndentedBuffer]) str
            decide_layout()*
            freeze_layout()
            freeze_layout_with_exact_strides(exact_strides, allow_padding) None
            freeze_layout_with_fill_order(order) None
            freeze_layout_with_same_order(stride) None
            freeze_layout_with_stride_order(order, allow_padding) None
            get_defining_op() Optional[Operation]
            get_device() Optional[torch.device]
            get_inputs_that_alias_output() Sequence[str]
            get_layout() Layout
            get_mutation_names() Sequence[str]
            get_name() str
            get_offset() Expr
            get_output_spec() OutputSpec
            get_read_names() OrderedSet[str]
            get_size() Sequence[Expr]
            get_storage_numel()
            get_stride() List[Expr]
            get_unbacked_symbol_defs() OrderedSet[sympy.Symbol]
            get_unbacked_symbol_uses() OrderedSet[sympy.Symbol]
            is_zero_elements()
            make_indexer() Callable[[Sequence[Expr]], Expr]
            make_loader() Callable[[Sequence[Expr]], OpsValue]
            realize()* Optional[str]
            should_allocate() bool
          }
          class Buffer {
          }
          class BufferGroup {
            allocation : Optional[Allocation]
            is_output : bool
            live_range
            names : list
            node : Union
            make_allocation()
            sym_nbytes()
            update_usage(timestep: int)
          }
          class BufferInfo {
            buffer : Union[SchedulerBuffer, FreeableInputBuffer]
            end_step : int
            size_alloc : int
            size_free : int
            start_step : int
          }
          class BufferInfo {
            outdegree : int
          }
          class BufferMutationSpec {
            arg : Annotated[TensorArgument, 10]
            buffer_name : Annotated[str, 20]
          }
          class BuildExtension {
            cflags : NoneType
            force : bool
            no_python_abi_suffix
            use_ninja : bool
            build_extensions() None
            finalize_options() None
            get_ext_filename(ext_name)
            with_options()
          }
          class BuildOptionsBase {
            get_aot_mode() bool
            get_cflags() List[str]
            get_compile_only() bool
            get_compiler() str
            get_definations() List[str]
            get_include_dirs() List[str]
            get_ldflags() List[str]
            get_libraries() List[str]
            get_libraries_dirs() List[str]
            get_passthough_args() List[str]
            get_use_absolute_path() bool
            save_flags_to_file(file: str) None
          }
          class Builder {
          }
          class BuiltinVariable {
            call_float
            call_function_handler_cache : dict
            call_iand
            call_int
            call_ior
            call_list
            call_max
            call_min
            call_tuple
            fn
            as_proxy()
            as_python_constant()
            call_abs(tx: 'InstructionTranslator', arg: 'VariableTracker')
            call_and_(tx: 'InstructionTranslator', a, b)
            call_callable(tx: 'InstructionTranslator', arg)
            call_cast(_)
            call_contains(tx: 'InstructionTranslator', a: VariableTracker, b: VariableTracker)
            call_custom_dict(tx: 'InstructionTranslator', user_cls)
            call_custom_dict_fromkeys(tx: 'InstructionTranslator', user_cls)
            call_deepcopy(tx: 'InstructionTranslator', x)
            call_delattr(tx: 'InstructionTranslator', obj: VariableTracker, name_var: VariableTracker)
            call_dict(tx: 'InstructionTranslator')
            call_filter(tx: 'InstructionTranslator', fn, seq)
            call_format(tx: 'InstructionTranslator', _format_string)
            call_frozenset(tx: 'InstructionTranslator')
            call_function(tx: 'InstructionTranslator', args: 'List[VariableTracker]', kwargs: 'Dict[str, VariableTracker]') 'VariableTracker'
            call_getattr(tx: 'InstructionTranslator', obj: VariableTracker, name_var: VariableTracker, default)
            call_getitem(tx: 'InstructionTranslator')
            call_hasattr(tx: 'InstructionTranslator', obj, attr)
            call_id(tx: 'InstructionTranslator')
            call_index(tx: 'InstructionTranslator', arg: 'VariableTracker')
            call_isinstance(tx: 'InstructionTranslator', arg, isinstance_type)
            call_issubclass(tx: 'InstructionTranslator', left_ty, right_ty)
            call_iter(tx: 'InstructionTranslator', obj)
            call_len(tx: 'InstructionTranslator')
            call_map(tx: 'InstructionTranslator', fn)
            call_method(tx, name, args: 'List[VariableTracker]', kwargs: 'Dict[str, VariableTracker]') 'VariableTracker'
            call_neg(tx: 'InstructionTranslator', a)
            call_next(tx: 'InstructionTranslator', arg: VariableTracker)
            call_not_(tx: 'InstructionTranslator', a)
            call_or_(tx: 'InstructionTranslator', a, b)
            call_pos(tx: 'InstructionTranslator', arg: 'VariableTracker')
            call_range(tx: 'InstructionTranslator')
            call_reversed(tx: 'InstructionTranslator', obj: VariableTracker)
            call_round(tx: 'InstructionTranslator', arg)
            call_set(tx: 'InstructionTranslator')
            call_setattr(tx: 'InstructionTranslator', obj: VariableTracker, name_var: VariableTracker, val: VariableTracker)
            call_slice(tx: 'InstructionTranslator')
            call_sorted(tx: 'InstructionTranslator', obj: VariableTracker)
            call_str(tx: 'InstructionTranslator', arg)
            call_super(tx: 'InstructionTranslator', a, b)
            call_type(tx: 'InstructionTranslator', obj: VariableTracker)
            call_zip(tx: 'InstructionTranslator')
            can_constant_fold_through()
            can_insert_in_graph()
            constant_args()
            create_with_source(value, source)
            has_constant_handler(args, kwargs)
            python_and_tensor_constant_only()
            reconstruct(codegen)
            tensor_args()
            tensor_args_type(arg_types)
            unwrap_unspec_args_kwargs(args, kwargs)
          }
          class BypassAOTAutogradCache {
          }
          class BypassFxGraphCache {
          }
          class ByteStorage {
            dtype()
          }
          class ByteStorage {
            dtype()
          }
          class BytecodeDistpatchTableMeta {
          }
          class BytecodeHook {
          }
          class Bytes {
            value
          }
          class BytesIOContext {
          }
          class BytesStorageMetadata {
          }
          class C10dRendezvousBackend {
            name
            get_state() Optional[Tuple[bytes, Token]]
            set_state(state: bytes, token: Optional[Token]) Optional[Tuple[bytes, Token, bool]]
          }
          class CELU {
            alpha : float
            inplace : bool
            extra_repr() str
            forward(input: Tensor) Tensor
          }
          class CKGemmTemplate {
            alpha : float
            beta : float
            gemm_template : str
            is_batched
            output_node
            standalone_runner_template : str
            add_ck_gemm_choices(choices, layout, input_nodes, alpha, beta, input_reorder)
            emit_ck_instance(op: 'CKGemmOperation')
            filter_op(op: 'CKGemmOperation')
            gen_ops()
            globals() IndentedBuffer
            header() IndentedBuffer
            inline_utils()
            render(kernel: ROCmTemplateKernel, op: 'CKGemmOperation') str
            size_args()
          }
          class CKGroupedConvFwdTemplate {
            conv_template : str
            dilation
            groups
            n_spatial_dimensions
            output_node
            padding
            stride
            add_ck_conv_choices(choices, layout, input_nodes)
            emit_ck_instance(op: 'CKGroupedConvFwdOp') Tuple[str, str]
            filter_op(op: 'CKGroupedConvFwdOp')
            gen_ops()
            globals() IndentedBuffer
            header() IndentedBuffer
            render(kernel: ROCmTemplateKernel, op: 'CKGroupedConvFwdOp') str
            size_args()
          }
          class CKTemplate {
            globals() IndentedBuffer
            header() IndentedBuffer
            torch_type_to_ck(node: IRNode, ptr: str) str
          }
          class CPPTimer {
            timeit(number: int) float
          }
          class CPUDeviceBenchmarkMixin {
            do_bench(fn) float
          }
          class CPUOffload {
            offload_params : bool
          }
          class CPUOffloadPolicy {
            pin_memory : bool
          }
          class CPUTestBase {
            device_type : str
          }
          class CSE {
            invalidated_stores
            iter_buffer_ids : count
            name_prefix : str
            prefix : str
            reduction_cache : dict
            store_cache : dict
            suffix : str
            varname_map : dict
            augment_key(cache_key: object) object
            clone()
            contains(cache_key) bool
            generate(buffer: IndentedBuffer, expr: Union[str, CSEVariable, OpsValue, IndentedBuffer, DeferredLineBase]) CSEVariable
            get(cache_key: object) CSEVariable
            invalidate(keep_vars: Union[OrderedSet[str], OrderedSet[Never]])
            namedvar(name: str, bounds: ValueRanges[Any], dtype: Optional[torch.dtype]) CSEVariable
            newvar(bounds: ValueRanges[Any], dtype: Optional[torch.dtype]) CSEVariable
            put(cache_key: object, val: CSEVariable) None
            try_get(cache_key: object) Optional[CSEVariable]
          }
          class CSEPass {
            banned_ops : NoneType, set
            call(graph_module: GraphModule) PassResult
          }
          class CSEProxy {
            vr_analysis
            bucketize(values: CSEVariable, boundaries: Tuple[str, sympy.Expr, sympy.Expr, sympy.Expr], boundary_indices: CSEVariable, indexing_dtype: torch.dtype, right: bool, sorter: Optional[Tuple[str, sympy.Expr]], sorter_indices: Optional[CSEVariable]) CSEVariable
            check_bounds(expr: sympy.Expr, size: sympy.Expr, lower: bool, upper: bool)
            indirect_indexing(var: CSEVariable, size: Union[sympy.Expr, int], check: bool, wrap_neg)
            load(name: str, index: sympy.Expr) CSEVariable
            reduction(dtype: torch.dtype, src_dtype: torch.dtype, reduction_type: ReductionType, value: Union[CSEVariable, Tuple[CSEVariable, ...]]) Union[CSEVariable, Tuple[CSEVariable, ...]]
            scan(dtypes: Tuple[torch.dtype, ...], combine_fn: Callable[[Tuple[CSEVariable, ...], Tuple[CSEVariable, ...]], Tuple[CSEVariable, ...]], values: Tuple[CSEVariable, ...]) Tuple[CSEVariable, ...]
            sort(dtypes: Tuple[torch.dtype, ...], values: Tuple[CSEVariable, ...], stable: bool, descending: bool) Tuple[CSEVariable, ...]
            store(name: str, index: sympy.Expr, value: CSEVariable, mode: StoreMode) None
            store_reduction(name: str, index: sympy.Expr, value: CSEVariable)
          }
          class CSEProxy {
          }
          class CSEVariable {
            bounds : ValueRanges[Any]
            dtype : Optional[torch.dtype]
            name
            use_count : int
            update_on_args(name, args, kwargs)*
          }
          class CTCLoss {
            blank : int
            zero_infinity : bool
            forward(log_probs: Tensor, targets: Tensor, input_lengths: Tensor, target_lengths: Tensor) Tensor
          }
          class CUDABenchmarkRequest {
            DLL : Optional[DLLWrapper]
            hash_key : str
            source_code : str
            source_file : str
            workspace : NoneType, Optional[torch.Tensor]
            workspace_size : int
            cleanup_run_fn() None
            ensure_dll_loaded()
            make_run_fn() Callable[[], None]
            precompile()
            update_workspace_size() None
          }
          class CUDACPPScheduling {
            scheduler
            can_fuse_vertical(node1: BaseSchedulerNode, node2: BaseSchedulerNode) bool
            codegen_template(template_node: BaseSchedulerNode, epilogue_nodes: Sequence[BaseSchedulerNode], prologue_nodes: Sequence[BaseSchedulerNode])
            define_kernel(src_code: str, node_schedule) str
            get_backend_features(device)
            group_fn(sizes)
            is_cuda_cpp_template(node: BaseSchedulerNode) bool
          }
          class CUDACodeCache {
            cache : Dict[str, CacheEntry]
            cache_clear : staticmethod
            compile(source_code: str, dst_file_ext: str, extra_args: Optional[List[str]]) Tuple[str, str, str]
            load(source_code: str, dst_file_ext: str) Tuple[DLLWrapper, str, str]
            write(source_code: str, dst_file_ext: str) Tuple[str, str]
          }
          class CUDACombinedScheduling {
            benchmark_combo_kernel(node_list)
            benchmark_fused_nodes(nodes)
            can_fuse_horizontal(node1: BaseSchedulerNode, node2: BaseSchedulerNode)
            can_fuse_vertical(node1: BaseSchedulerNode, node2: BaseSchedulerNode)
            choose_node_backend(node: BaseSchedulerNode) BaseScheduling
            codegen_combo_kernel()
            codegen_node(node: Union[FusedSchedulerNode, SchedulerNode])
            codegen_sync()
            codegen_template(template_node: BaseSchedulerNode, epilogue_nodes: Sequence[BaseSchedulerNode], prologue_nodes: Sequence[BaseSchedulerNode])
            flush()
            generate_kernel_code_from_nodes(nodes, benchmark_kernel)
            get_backend_features(device)
            group_fn(sizes)
          }
          class CUDACompileError {
          }
          class CUDACompileSourceCapturingContext {
            sources : list
          }
          class CUDADeviceOpOverrides {
            abi_compatible_header()
            aoti_get_stream()
            cpp_aoti_device_guard()
            cpp_aoti_stream_guard()
            cpp_device_guard()
            cpp_device_ptr()
            cpp_getStreamFromExternal()
            cpp_kernel_type()
            cpp_stream_guard()
            cpp_stream_type()
            device_guard(device_idx)
            import_get_raw_stream_as(name)
            kernel_driver()
            kernel_header()
            set_device(device_idx)
            synchronize()
            tma_descriptor_helpers()
          }
          class CUDADeviceVariable {
            target_values
            create(tx: 'InstructionTranslator', device)
            enter(tx)
            exit(tx: 'InstructionTranslator')
            fn_name()
            module_name()
          }
          class CUDAGraph {
            capture_begin(pool, capture_error_mode)
            capture_end()
            debug_dump(debug_path)
            enable_debug_mode()
            pool()
            replay()
            reset()
          }
          class CUDAGraphNode {
            cached_tensor_outputs : OutputList[Optional[Tensor]]
            checkpointed_caching_state : Optional[AllocatorState]
            children : Dict[FunctionID, List[CUDAGraphNode]]
            cuda_graphs_pool : Tuple[int, int]
            cudagraph_managed_idxs : List[int]
            device : int
            expanded_dims : List[List[int]]
            expected_dead_indices_after_graph : List[PathOutputIndex], list
            expected_dead_indices_before_graph : List[PathOutputIndex], list
            graph : Optional[torch.cuda.CUDAGraph]
            id
            live_cudagraph_managed_path_refs : InputList[Optional[PathOutputIndex]]
            live_indices_after_graph : List[PathOutputIndex]
            non_managed_static_input_idxs : LevelList[int]
            non_static_input_idx : LevelList[int]
            output_storage_alias : OutputList[Optional[OutputAliasInfo]]
            outputs_metadata : OutputList[Union[Dict[str, Any], int, None]]
            outputs_weakrefs : OutputList[Optional[StorageWeakRefWrapper]]
            parent
            path_stacktraces : LevelList[Optional[StackTraces]]
            path_weakrefs : LevelList[OutputList[Optional[StorageWeakRefWrapper]]]
            preserved_aliased_inputs : InputList[bool]
            reconstructed_inputs : List[InputType]
            recorded_liveness_after_graph : LevelList[OutputList[bool]], list
            recorded_liveness_before_graph : LevelList[OutputList[bool]], list
            recording_outputs : NoneType, Optional[OutputType]
            rerecord_if_static_inputs_change : bool
            stack_traces : Optional[StackTraces]
            static_input_data_ptrs : InputList[Optional[int]]
            static_input_idxs : List[int]
            static_inputs_stable : bool
            static_output_tensors : OutputList[Optional[Tensor]]
            stream
            tensor_weakrefs : OutputList[Optional[TensorWeakRef]]
            unaliased_in_all_paths : OutputList[bool]
            wrapped_function
            add_child(function_id: FunctionID, node: CUDAGraphNode) None
            all_outputs_are_dead() bool
            check_invariants(inputs: List[InputType]) Tuple[CheckInvariantStatus, Callable[..., str]]
            check_static_inputs_are_stable(new_inputs: List[InputType]) None
            clear_path_state()* None
            create_storage(metadata: Dict[str, Any]) torch.types.Storage
            data_ptrs_dead_since_invocation() List[int]
            debug_assert_invariants(expected_liveness: List[List[bool]], newly_dead: List[PathOutputIndex]) None
            debug_check_invariants_after_invocation() None
            debug_check_invariants_before_invocation() None
            get_output_refcount(index: int) int
            num_descendants() int
            path_live_weakrefs() Iterator[StorageWeakRefWrapper]
            prepare_alias_info_for_tensor_construction(out_alias_info: Optional[OutputAliasInfo], metadata: Union[Dict[str, Any], int, None]) Union[UntypedStorage, None, int]
            prepare_storages_for_construction() List[Union[UntypedStorage, None, int]]
            reconstruct_outputs() OutputType
            remove_node_cached_tensors() None
            remove_path_cached_tensors() None
            run(new_inputs: List[InputType]) OutputType
            run_first_inputs(new_inputs: List[InputType]) OutputType
            run_graph() None
          }
          class CUDAGraphTreeManager {
            cuda_graphs_thread_pool
            current_gen : int
            current_node
            debug_checkpointing_counter : int
            debug_fail_counter : int
            device_index : int
            disable_invalidate_aliases : bool
            func_counter : count
            graph : NoneType, Optional[torch.cuda.CUDAGraph]
            graph_counter : count
            id_to_mode : Dict[FunctionID, CompilationMode]
            ids_to_funcs : Dict[FunctionID, WrappedFunction]
            ids_to_stack_traces : Dict[FunctionID, Optional[StackTraces]]
            in_recording
            in_warmup
            mode : BACKWARD, Optional[CompilationMode]
            non_cudagraph_managed_mutation_hint : Dict[Optional[GraphID], Dict[FunctionID, bool]]
            num_rerecord : Dict[Optional[GraphID], Dict[FunctionID, int]]
            path_state : EXECUTION, NONE, RECORDING, WARMUP
            roots : Dict[FunctionID, List[CUDAGraphNode]], NoneType
            running_forwards_with_pending_backwards : bool
            stream
            warmed_up_functions : OrderedSet[FunctionID]
            warmup_node_counter : count
            warned_functions : OrderedSet[FunctionID]
            warned_mutation : OrderedSet[FunctionID]
            add_function(model: ModelType, inputs: List[InputType], static_input_idxs: Sequence[int], stack_traces: Optional[StackTraces], mode: CompilationMode, constants: Tuple[torch.Tensor, ...], placeholders: Tuple[PlaceholderInfo, ...], mutated_input_idxs: Tuple[int, ...]) Tuple[ModelType, OutputType]
            apply_checkpoint_execution_state_in_allocator() None
            can_start_new_generation() bool
            check_warn_on_unable_to_start_executing(function_id: FunctionID) None
            clear_current_path_state_and_set_to_none() None
            dealloc_current_path_weakrefs() None
            exceed_rerecord_limit(node_id: Optional[GraphID], function_id: FunctionID) bool
            execute_node(node: CUDAGraphNode, new_inputs: List[InputType]) OutputType
            format_dealloc_msg(stack_trace: Optional[str]) str
            get_curr_generation() int
            get_roots() Iterator[CUDAGraphNode]
            in_new_torch_compile_invocation() bool
            live_cudagraph_pool_storages_in_curr_execution() List[StorageWeakRefPointer]
            new_func_id() FunctionID
            new_graph_id() GraphID
            new_warmup_node_id() GraphID
            record_function(new_inputs: List[InputType], function_id: FunctionID) OutputType
            run(new_inputs: List[InputType], function_id: FunctionID) OutputType
            run_eager(new_inputs: List[InputType], function_id: FunctionID) OutputType
            set_to_running_backward() None
            shutdown() None
            try_end_curr_execution() None
            try_end_curr_recording(function_id: FunctionID) None
            try_end_curr_warmup(function_id: FunctionID) None
            update_generation() None
            user_invoked_mark_step() bool
          }
          class CUDAKernel {
            layout_args : Dict[str, LayoutArg]
            named_nodes : Dict[str, IRNode]
            overrides
            add_layout_arg(symbol: ValidLayoutSymbols, node: IRNode, attr: ValidLayoutAttrs, dim: int)
            find_layout_arg(node: IRNode, attr: ValidLayoutAttrs, dim: int) Optional[LayoutArg]
            find_ld_idx(node: IRNode) int
            find_symbol(node: IRNode, attr: ValidLayoutAttrs, dim: int) Optional[str]
            get_layout_args() Tuple[Union[Expr, int], ...]
            init_layout_args() None
          }
          class CUDAPluggableAllocator {
          }
          class CUDARngStateHelper {
            get_torch_state_as_tuple(fake_mode)
            set_new_offset(relative_offset)
            set_torch_state_tensor(seed, offset)
          }
          class CUDASanitizer {
            dispatch
            enabled : bool
            disable()
            enable()
          }
          class CUDASanitizerDispatchMode {
            event_handler
          }
          class CUDASanitizerErrors {
            errors : List[SynchronizationError]
          }
          class CUDATemplate {
            index_counter : count
            input_nodes : List[Buffer]
            input_reorder : Optional[List[int]]
            layout
            output_node
            generate(description) CUDATemplateCaller
            globals() IndentedBuffer
            header() IndentedBuffer
            render()* str
          }
          class CUDATemplateBuffer {
            template : object
            workspace_size : int
            get_workspace_size()
          }
          class CUDATemplateCaller {
            bmreq
            category : str
            info_kwargs : Optional[Dict[str, Union[PrimitiveInfoType, List[PrimitiveInfoType]]]]
            make_kernel_render : Callable[[CUDATemplateBuffer, Optional[List[IRNode]]], str]
            template : str
            benchmark() float
            call_name() str
            hash_key() str
            info_dict() Dict[str, Union[PrimitiveInfoType, List[PrimitiveInfoType]]]
            output_node() TensorBox
            precompile() None
          }
          class CUDATemplateKernel {
            kernel_name
            signature : str
            arg_name(node: IRNode) Optional[str]
            call_kernel(name: str, node: 'CUDATemplateBuffer') None
            check_not_null(node: IRNode) str
            cutlass_dtype(node: IRNode, default_dtype) Optional[str]
            def_kernel(inputs: List[IRNode], outputs: List[IRNode], names_str: str, input_reorder: Optional[List[int]]) str
            dtype(node: IRNode) Optional[str]
            get_signature() str
            max_valid_index(node: IRNode, default)
            offset(node: IRNode) str
            ptr(node: IRNode) str
            row_or_column_stride(node: IRNode, default_value: int) str
            size(node: IRNode, start_index: int, end_index: Optional[int], default_value: int) str
            stride(node: IRNode, index: int, default_value: int) str
          }
          class CUDATestBase {
            cudnn_version : ClassVar[Any]
            device_type : str
            no_cudnn : ClassVar[bool]
            no_magma : ClassVar[bool]
            primary_device : ClassVar[str]
            get_all_devices()
            get_primary_device()
            has_cudnn()
            setUpClass()
          }
          class CUDAWarmupNode {
            already_warm : bool
            cuda_graphs_pool : Tuple[int, int]
            device_index : int
            existing_cuda_graph : Optional[torch.cuda.CUDAGraph]
            has_run : bool
            id
            outputs_weakrefs : List[Optional[StorageWeakRefWrapper]]
            parent : Optional[Union[CUDAGraphNode, CUDAWarmupNode]]
            stack_traces : Optional[StackTraces]
            stream
            tensor_weakrefs : List[Optional[TensorWeakRef]]
            wrapped_function
            all_outputs_are_dead() bool
            path_live_weakrefs() Iterator[StorageWeakRefWrapper]
            run(new_inputs: Any) OutputType
          }
          class CUTLASS2xGemmTemplate {
            add_cutlass_gemm_choices(choices: List[ChoiceCaller], layout: ir.Layout, input_nodes: List[Buffer], alpha: Union[float, int], beta: Union[float, int], input_reorder: Optional[List[int]]) None
            render_gemm_arguments(instance_type: str, argument_template: str, epilogue_template: str, should_swap_xw: bool, X: IRNode, W: IRNode, Bias: IRNode, Meta: IRNode, Y: IRNode, alpha: float, beta: float, kernel: CUDATemplateKernel, epilogue_args) str
          }
          class CUTLASS3xGemmTemplate {
            add_cutlass_gemm_choices(choices: List[ChoiceCaller], layout: ir.Layout, input_nodes: List[Buffer], alpha: Union[float, int], beta: Union[float, int], input_reorder: Optional[List[int]]) None
            render_gemm_arguments(argument_template: str, epilogue_template: str, should_swap_xw: bool, X: IRNode, W: IRNode, Bias: IRNode, Y: IRNode, alpha: float, beta: float, kernel: CUDATemplateKernel, epilogue_args) str
          }
          class CUTLASSArgs {
            architectures : Optional[str]
            build_dir : str
            cuda_version : Optional[str]
            curr_build_dir : str
            disable_full_archs_compilation : bool
            filter_by_cc : bool
            generator_target : str
            ignore_kernels : str
            interface_dir : NoneType
            kernel_filter_file : NoneType
            kernels : str
            operations : str
            selected_kernel_list : NoneType
          }
          class CUTLASSEVTOpNotImplementedError {
          }
          class CUTLASSGemmTemplate {
            alpha : float
            beta : float
            add_cutlass_gemm_choices(choices: List[ChoiceCaller], layout: ir.Layout, input_nodes: List[Buffer], alpha: Union[float, int], beta: Union[float, int], input_reorder: Optional[List[int]])* None
            cutlass_layout(torch_layout: ir.Layout) 'Optional[cutlass_lib.LayoutType]'
            filter_op(op: 'cutlass_library.gemm_op.GemmOperation') 'cutlass_library.gemm_op.GemmOperation'
            fix_op_layout(op: 'cutlass_library.gemm_op.GemmOperation', X: Buffer, W: Buffer, Bias: Optional[Buffer], Y: Union[Buffer, ReinterpretView]) 'cutlass_library.gemm_op.GemmOperation'
            flip_cutlass_layout(cutlass_layout: 'cutlass_lib.LayoutType') 'cutlass_lib.LayoutType'
            gemm_mode() str
            gen_ops() 'List[Tuple[str, cutlass_gemm_op.GemmOperation]]'
            header() IndentedBuffer
            layout_match(torch_layout: ir.Layout, cutlass_layout: 'cutlass_lib.LayoutType') bool
            render(kernel: CUDATemplateKernel, op: 'cutlass_gemm_op.GemmOperation', template_buffer_node: Optional[CUDATemplateBuffer]) str
            set_alignment(torch_layout, op_element) bool
            should_swap_XW(bias: IRNode) bool
            swap_XW(op: 'cutlass_library.gemm_op.GemmOperation') 'cutlass_library.gemm_op.GemmOperation'
            test_call_statement(kernel, input_nodes, names_str: str) str
          }
          class CUTLASSTemplate {
            cute_int(int_str: str, var_name: str) str
            cutlass_sparse_meta_type_cast(node: IRNode, ptr: str) str
            cutlass_type_cast(node: IRNode, ptr: str) str
            globals() IndentedBuffer
            header() IndentedBuffer
          }
          class CacheBase {
            system : dict
            get_global_cache_path() Optional[Path]
            get_local_cache() Dict[str, Any]
            get_local_cache_path() Path
            get_system() Dict[str, Any]
            update_local_cache(local_cache: Dict[str, Any]) None
          }
          class CacheEntry {
            input_path : str
            output_path : str
          }
          class CacheEntry {
            input_path : str
            output_path : str
          }
          class CacheSizeRelevantForFrame {
            num_cache_entries : int
            num_cache_entries_with_same_id_matched_objs : int
            will_compilation_exceed(limit: int) bool
            will_compilation_exceed_accumulated_limit() bool
            will_compilation_exceed_specific_limit(limit: int) bool
          }
          class CachedMethod {
            clear_cache(self) None
          }
          class CachedMetricsDeltas {
            cpp_to_dtype_count : int
            generated_cpp_vec_kernel_count : int
            generated_kernel_count : int
            ir_nodes_pre_fusion : int
            num_bytes_accessed : int
            num_matches_for_scatter_upon_const_tensor : int
          }
          class CachedMetricsHelper {
            cached_metrics : dict
            apply_deltas(delta: CachedMetricsDeltas)
            get_deltas() CachedMetricsDeltas
          }
          class CachingAutotuner {
            autotune_time_taken_ns : int
            configs : NoneType
            coordesc_tuner
            cuda_kernel_saved : bool
            custom_kernel : bool
            device_props : DeviceProperties
            dump_launch_params
            filename : Optional[str]
            fn
            heuristic_type
            inductor_meta : NoneType, dict
            kernel_hash : str
            launchers : list
            lock : lock
            mutated_arg_names : List[str]
            optimize_mem
            precompile_time_taken_ns : int
            reset_to_zero_arg_names : NoneType, list
            save_cache_hook
            size_hints : NoneType
            triton_interpret
            triton_meta
            autotune_to_one_config()
            bench(launcher)
            benchmark_all_configs()
            clone_args() Tuple[List[Any], Dict[str, Any]]
            coordinate_descent_tuning(launcher)
            copy_args_to_cpu_if_needed()
            get_device_interface()
            maybe_clone_args(exclude: Container[str]) Tuple[List[Any], Dict[str, Any]]
            precompile(warm_cache_only)
            reset_to_zero_args()
            restore_args_from_cpu(cpu_copies)
            run()
            save_gpu_kernel(grid, stream, launcher)
          }
          class CalcConv {
            c_out
            conv_result
            dilation
            input_var
            kernel
            matching_constraint
            padding
            stride
          }
          class CalcMaxPool {
            dilation
            input_var
            kernel
            matching_constraint
            maxpool_result
            padding
            stride
          }
          class CalcProduct {
            dims_to_flatten
            end
            flattened
            start
          }
          class CallFunction {
            op : str
          }
          class CallFunctionNoArgsSource {
          }
          class CallFunctionVarArgs {
            op : str
          }
          class CallMethod {
            op : str
          }
          class CallMethodItemSource {
            guard_source()
            name() str
          }
          class CallMethodKey {
            name : str
            get(o: Any) Any
          }
          class CallMethodVarArgs {
            op : str
          }
          class CallModule {
            op : str
          }
          class CallModuleVarArgs {
            op : str
          }
          class CallTorchBind {
          }
          class CallTorchbindHigherOrderVariable {
            method_name
            script_obj_var
            call_function(tx: 'InstructionTranslator', args: List[VariableTracker], kwargs: Dict[str, VariableTracker]) VariableTracker
          }
          class CallbackRegistry {
            callback_list : List[Callable[P, None]]
            name : str
            add_callback(cb: Callable[P, None]) None
            fire_callbacks() None
          }
          class CallgrindModuleType {
          }
          class CallgrindStats {
            baseline_exclusive_stats
            baseline_inclusive_stats
            built_with_debug_symbols : bool
            number_per_run : int
            stmt_callgrind_out : Optional[str]
            stmt_exclusive_stats
            stmt_inclusive_stats
            task_spec
            as_standardized() 'CallgrindStats'
            counts() int
            delta(other: 'CallgrindStats', inclusive: bool) FunctionCounts
            stats(inclusive: bool) FunctionCounts
          }
          class CanReshape {
            src
            target
          }
          class CandidateTiling {
            name : Optional[str]
            score : int
            tiling : Tuple[sympy.Expr, sympy.Expr]
            is_good_size(s)
          }
          class CantSplit {
          }
          class CapabilityBasedPartitioner {
            allowed_single_node_partition_ops : NoneType, list
            allows_single_node_partition : bool
            dependency_viewer
            graph_module
            non_compute_ops : NoneType, list
            operator_support
            fuse_partitions(partitions: List[Partition], prefix: str) GraphModule
            partition_and_fuse(prefix: str) GraphModule
            propose_partitions() List[Partition]
            remove_bookend_non_compute_ops(partitions: List[Partition])
          }
          class Capture {
            columns
            ctx : dict
            apply_ops_2(dataframe)
          }
          class CaptureA {
            execute()
          }
          class CaptureAdd {
            ctx
            left
            right
            execute()
          }
          class CaptureCall {
            callable
            ctx : dict
            kwargs : dict
            execute()
          }
          class CaptureControl {
            disabled : bool
          }
          class CaptureDataFrame {
          }
          class CaptureDataFrameWithDataPipeOps {
            as_datapipe()
            batch(batch_size, drop_last: bool, wrapper_class)
            collate()
            filter()
            groupby(group_key_fn)
            raw_iterator()
            shuffle()
          }
          class CaptureF {
            ctx : dict
            kwargs : dict
          }
          class CaptureGetAttr {
            ctx
            name
            src
            execute()
          }
          class CaptureGetItem {
            ctx
            key
            left
            execute()
          }
          class CaptureIndexing {
            bucketize(values: T, boundaries: Tuple[str, sympy.Expr, sympy.Expr, sympy.Expr], boundary_indices: T, indexing_dtype: torch.dtype, right: bool, sorter: Optional[Tuple[str, sympy.Expr]], sorter_indices: Optional[T]) T
            check_bounds(index, size, lower, upper)
            frexp(value_proxy)
            index_expr(index, dtype)
            indirect_indexing(index_proxy, size, check, wrap_neg)
            load(name: str, index: sympy.Expr)
            load_seed(name: str, index: int)
            masked(mask_proxy, masked_body: Callable[..., Any], other_proxy)
            output(result)
            reduction(dtype, src_dtype, reduction_type, value)
            scan(dtype_proxy, combine_fn: Callable[[Tuple[Any, ...], Tuple[Any, ...]], Tuple[Any, ...]], value_proxy)
            sort(dtypes, values, stable, descending)
            store(name, index, value, mode)
            store_reduction(name, index, value)
          }
          class CaptureInitial {
            name : str
          }
          class CaptureLikeMock {
            attribute
            get_target
            name
            save
          }
          class CaptureLogs {
            logs : NoneType, list
            tbs : NoneType, list
            get_context_manager()
          }
          class CaptureMul {
            ctx
            left
            right
            execute()
          }
          class CaptureSetItem {
            ctx
            key
            left
            value
            execute()
          }
          class CaptureStrategy {
          }
          class CaptureStructuredTrace {
            logger : NoneType, RootLogger
            logs : List[Tuple[str, Dict[str, Any]]], list
            prev_get_dtrace : bool
            specific_log_keys : List[str]
            emit(record: Any) None
          }
          class CaptureSub {
            ctx
            left
            right
            execute()
          }
          class CaptureVariable {
            ctx
            name : str
            names_idx : int
            value
            apply_ops(dataframe)
            execute()
          }
          class CaptureVariableAssign {
            execute()
          }
          class CapturedTraceback {
            skip : int
            tb : NoneType
            cleanup()
            extract()
            format()
            format_all(tbs)
            summary()
          }
          class CatQuantizeHandler {
          }
          class CatTransform {
            bijective
            dim : int
            lengths : list
            transforms : List[Transform]
            codomain()
            domain()
            event_dim()
            length()
            log_abs_det_jacobian(x, y)
            with_cache(cache_size)
          }
          class CatchErrorsWrapper {
            hooks
          }
          class CatchWarningsCtxManagerVariable {
            catch_warnings_args
            create(tx: 'InstructionTranslator', catch_warnings_args)
            enter(tx)
            reconstruct(cg)
          }
          class Categorical {
            arg_constraints : dict
            has_enumerate_support : bool
            logits
            mean
            mode
            param_shape
            probs
            variance
            entropy()
            enumerate_support(expand)
            expand(batch_shape, _instance)
            log_prob(value)
            logits()
            probs()
            sample(sample_shape)
            support()
          }
          class Category {
            name
          }
          class CategoryDict {
            get(key: Key, version: int) Optional[Category]
            set_by_id(key: TensorKey, category: Category) None
            set_by_key(key: TensorKey, category: Category) None
            set_by_version(key: TensorKey, version: int, category: Category) None
            setdefault_by_version(key: TensorKey, version: int, category: Category) None
          }
          class CategoryElement {
            by_id : Optional[Category]
            by_key : Dict[TensorKey, Category]
            by_version : Dict[TensorAndID, Category]
          }
          class Cauchy {
            arg_constraints : dict
            has_rsample : bool
            loc
            mean
            mode
            scale
            support
            variance
            cdf(value)
            entropy()
            expand(batch_shape, _instance)
            icdf(value)
            log_prob(value)
            rsample(sample_shape: _size) torch.Tensor
          }
          class CausalBias {
            seq_len_kv : int
            seq_len_q : int
            variant
          }
          class CausalVariant {
            name
          }
          class CeilDiv {
            is_integer : bool
          }
          class CeilToInt {
            is_integer : bool
            eval(number)
          }
          class CellVariable {
            local_name : Optional[str]
            pre_existing_contents : Optional[VariableTracker]
          }
          class ChainDataset {
            datasets : Iterable[Dataset]
          }
          class ChainedScheduler {
            optimizer : Optional[Optimizer]
            load_state_dict(state_dict)
            state_dict()
            step()
          }
          class ChainedSource {
            base
            is_dict_key()
            is_ephemeral()
          }
          class ChannelShuffle {
            groups : int
            extra_repr() str
            forward(input: Tensor) Tensor
          }
          class CharStorage {
            dtype()
          }
          class CharStorage {
            dtype()
          }
          class CheckFunctionManager {
            guard_manager
            output_graph : NoneType
            torch_function_mode_stack : NoneType
            compile_check_fn(builder, guards_out, guard_fail_fn)
            id_ref(obj, obj_str)
            invalidate(obj_str)
            lookup_weakrefs(obj)
          }
          class CheckInvariantStatus {
            name
          }
          class CheckpointError {
          }
          class CheckpointException {
            failures
          }
          class CheckpointFunction {
            backward(ctx)
            forward(ctx, run_function, preserve_rng_state)
          }
          class CheckpointHigherOrderVariable {
            call_function(tx: 'InstructionTranslator', args: List[VariableTracker], kwargs: Dict[str, VariableTracker]) VariableTracker
          }
          class CheckpointImpl {
            name
          }
          class CheckpointPolicy {
            name
          }
          class CheckpointWrapper {
            checkpoint_fn : partial
            checkpoint_impl
            forward()
          }
          class Checkpointable {
            copy_graphstate()* T
            restore_graphstate(state: T)*
          }
          class Chi2 {
            arg_constraints : dict
            df
            expand(batch_shape, _instance)
          }
          class ChildFailedError {
            failures : Dict[GlobalRank, ProcessFailure]
            name : str
            format_msg(boarder_delim, section_delim)
            get_first_failure() Tuple[GlobalRank, ProcessFailure]
          }
          class ChoiceCaller {
            description : str
            input_nodes : List[Buffer]
            layout
            name : str
            autoheuristic_id() str
            benchmark() float
            call_name()* str
            hash_key()* str
            info_dict() Dict[str, Union[PrimitiveInfoType, List[PrimitiveInfoType]]]
            output_node()* TensorBox
            to_callable()*
          }
          class ChromiumEventLogger {
            id_ : str
            tls : _local
            add_event_data(event_name: str) None
            get_event_data() Dict[str, Any]
            get_pt2_compile_substack()
            get_stack() List[str]
            get_top() Optional[str]
            log_event_end(event_name: str, time_ns: int, metadata: Dict[str, Any], start_time_ns: int, log_pt2_compile_event: bool) None
            log_event_start(event_name: str, time_ns: int, metadata: Dict[str, Any], log_pt2_compile_event: bool) None
            log_instant_event(event_name: str, time_ns: int, metadata: Optional[Dict[str, Any]], log_pt2_compile_event: bool) None
            reset() None
            try_add_event_data(event_name: str) None
          }
          class ChunkShardingSpec {
            ShardingDim : Union
            dim : Union
            placements : List[Union[torch.distributed._remote_device, str]]
            build_metadata(tensor_sizes: torch.Size, tensor_properties: sharded_tensor_meta.TensorProperties) sharded_tensor_meta.ShardedTensorMetadata
            shard(tensor: torch.Tensor, src_rank: int, process_group) 'ShardedTensor'
          }
          class ChunkStorageMetadata {
            offsets
            sizes
          }
          class CircularPad1d {
            padding : Tuple[int, int]
          }
          class CircularPad2d {
            padding : Tuple[int, int, int, int]
          }
          class CircularPad3d {
            padding : Tuple[int, int, int, int, int, int]
          }
          class ClassMethod {
            linear
            forward(x)
            method(x)
          }
          class CleanDiv {
          }
          class CleanupHook {
            name : str
            scope : Dict[str, Any]
            create(scope, name, val)
          }
          class CleanupManager {
            count : int
            instance : ClassVar[CleanupManager]
          }
          class ClearCacheOnAllocateMixin {
            allocate(block: Allocation, is_last: bool)
            clear_cache()
          }
          class ClosureHandler {
            run(closure)
          }
          class CodeCacheFuture {
            result()* None
          }
          class CodeContextDict {
            code_context
            clear() None
            get_context(code: types.CodeType) Dict[str, Any]
            has_context(code: types.CodeType) bool
            pop_context(code: types.CodeType) Dict[str, Any]
          }
          class CodeFlow {
            message : Optional[_message.Message]
            properties : Optional[_property_bag.PropertyBag]
            thread_flows : List[_thread_flow.ThreadFlow]
          }
          class CodeGen {
            additional_globals() List[Tuple[str, Any]]
            gen_fn_def(free_vars: List[str], maybe_return_annotation: str) str
            generate_output(output_args: Argument) str
            process_inputs() Any
            process_outputs(outputs: Any) Any
          }
          class CodeGen {
            exit_stack : ExitStack
          }
          class CodeId {
            filename : str
            firstlineno : int
            name : str
            make(code: types.CodeType) CodeId
          }
          class CodeState {
            automatic_dynamic : DefaultDict[str, FrameStateSizeEntry]
          }
          class CollatorIterDataPipe {
          }
          class CollectTracepointsPass {
            sig
            specs : Dict[str, ModuleCallSignature]
            call(gm: torch.fx.GraphModule) Optional[PassResult]
          }
          class Collective {
            join(rank, data)
          }
          class CollectiveFunctionRewriteVariable {
            replacement_var
            call_function(tx: 'InstructionTranslator', args: 'List[VariableTracker]', kwargs: 'Dict[str, VariableTracker]') 'VariableTracker'
            can_rewrite(variable)
            create(tx: 'InstructionTranslator', old_fn, source)
            rewrite(tx: 'InstructionTranslator', fn)
          }
          class Colorize {
            name
          }
          class ColwiseParallel {
            desired_input_layouts : tuple
            input_layouts : tuple
            output_layouts : tuple
            use_local_output : bool
          }
          class Combine {
            backward(ctx, grad_output)
            forward(ctx, input, mask)
          }
          class ComboKernel {
            MAX_NUM_ARGS : int
            block_args : List[str], list
            block_size_1d : int
            block_size_2d : int
            block_size_reduce : int
            dispatch_class : Optional[Union[Type[ComboKernel.SequentialDispatch], Type[ComboKernel.RoundRobinDispatch]]]
            dynamic_shape_args : List[str]
            enable_autotune : bool
            grids : List[List[int]]
            iter_vars_count : count
            min_x_blocks_list : List[Union[int, str]]
            mixed_sizes : bool
            num_warps : int
            sub_kernels : List[TritonKernel]
            x_numels_list : List[Union[int, str]]
            add_blockd_to_args(argdefs: List[str]) List[str]
            add_numel_to_args(argdefs: List[str], signature: List[Any]) List[str]
            add_numel_to_call_args_and_grid(name: str, call_args: List[Any], arg_types: List[Any], grid: List[Any]) None
            add_numel_to_call_args_and_grid_benchmark(extra_args: List[Any], grid: Union[List[Any], Tuple[Any, ...]]) None
            call_kernel(code: IndentedBuffer, name: str) None
            codegen_blocks(code: IndentedBuffer) None
            codegen_kernel(name: Optional[str]) str
            codegen_kernel_benchmark(num_gb: float, grid: Optional[List[Any]]) IndentedBuffer
            codegen_static_numels_sub_kernel(code: IndentedBuffer, sub_kernel: TritonKernel, num: int) List[str]
            create_sub_kernel(triton_kernel: TritonKernel) TritonKernel
            create_triton_kernel(tiling: Dict[str, sympy.Expr], features: SIMDKernelFeatures, optimize_mask: bool) TritonKernel
            get_default_meta() Dict[str, int]
            get_mutated_args_sub_kernels() List[str]
            grid_no_autotune(grid: Union[Tuple[Any], List[Any]], num_kernels: int, min_blocks: int, is_sequential: bool) List[int]
            horizontal_partition(nodes: List[BaseSchedulerNode], triton_scheduling: SIMDScheduling, kernel_map: Dict[BaseSchedulerNode, TritonKernel], node_info_map: Dict[BaseSchedulerNode, Tuple[Any, Any, Any, Any]], custom_algorithm: bool) List[List[BaseSchedulerNode]]
            imports_for_benchmark_kernel() str
            jit_line(heuristics: str, size_hints: Dict[str, int], selected_kernel: TritonKernel, signature: List[Any], argdefs: List[str], pointwise_with_reduce: bool) str
            min_x_blocks_sub_kernel(sub_kernel: TritonKernel, num: int) None
            select_combo_heuristics(heuristics_list: List[str], size_hints_list: List[Dict[str, int]]) Tuple[str, Dict[str, int], TritonKernel]
            select_dispatch_strategy() None
            select_heuristics(sub_kernel: TritonKernel) Tuple[str, Dict[str, int]]
            uniquify_block_sizes(code: IndentedBuffer, num_kernel: int, uniquify: List[str]) IndentedBuffer
          }
          class CommBlock {
            comm_node
            inputs : List[fx.Node]
            node_list : List[fx.Node]
            outputs : OrderedSet[fx.Node]
            shape : Union[torch.Size, List[torch.Size]]
            wait_nodes : List[fx.Node]
          }
          class CommBufferAllocateLine {
            codegen(code: IndentedBuffer) None
            make_allocation_line(comm_buffer_type, group_name, wrapper, name, device, dtype, shape, stride)
          }
          class CommBufferFreeLine {
            codegen(code: IndentedBuffer) None
          }
          class CommBufferLayout {
            comm_buffer_type
            group_name : str
          }
          class CommBufferLine {
            comm_buffer_type
            group_name
            node
            size
            wrapper
          }
          class CommBufferType {
            name
          }
          class CommDebugMode {
            advanced_module_tracker
            comm_counts : Dict[Any, int]
            comm_module_counts : dict
            comm_module_operation_counts : dict
            comm_registry : set
            generate_comm_debug_tracing_table(noise_level)
            generate_json_dump(file_name, noise_level)
            get_comm_counts() Dict[Any, int]
            get_parameter_info() Dict[str, Dict[str, Any]]
            get_sharding_info() Dict[str, Dict[str, Any]]
            get_total_counts() int
            log_comm_debug_tracing_table_to_file(file_name, noise_level)
          }
          class CommonDdpComparisonTest {
            world_size
            get_remote_grads(rref, context_id)
            trainer_name(rank)
          }
          class CommonDistAutogradTest {
            dst_rank
            context_cleanup_test_helper(rpc_args, func, nested)
          }
          class CommonListMethodsVariable {
            call_method(tx, name, args: List['VariableTracker'], kwargs: Dict[str, 'VariableTracker']) 'VariableTracker'
          }
          class CommonRemoteModuleTest {
            world_size
          }
          class CompanionMismatch {
          }
          class Compare {
            colorize(rowwise)
            extend_results(results)
            highlight_warnings()
            print()
            trim_significant_figures()
          }
          class CompilationCallbackHandler {
            end_callbacks : List[Callable[[], None]]
            start_callbacks : List[Callable[[], None]]
            clear() None
            install_callbacks() Generator[None, Any, Any]
            register_end_callback(callback: Callable[[], None]) Callable[[], None]
            register_start_callback(callback: Callable[[], None]) Callable[[], None]
            remove_end_callback(callback: Callable[[], None]) None
            remove_start_callback(callback: Callable[[], None]) None
            run_end_callbacks() None
            run_start_callbacks() None
          }
          class CompilationMetrics {
            accumulated_cache_size : Optional[int]
            aot_autograd_cumulative_compile_time_us : Optional[int]
            backend_compile_time_s : Optional[float]
            backward_cumulative_compile_time_us : Optional[int]
            cache_size : Optional[int]
            co_filename : Optional[str]
            co_firstlineno : Optional[int]
            co_name : Optional[str]
            code_gen_time_s : Optional[float]
            compile_id : Optional[str]
            compile_time_autotune_time_us : Optional[int]
            compliant_custom_ops : Optional[Set[str]]
            config_inline_inbuilt_nn_modules : Optional[bool]
            config_suppress_errors : Optional[bool]
            cuda_synchronize_time_us : Optional[int]
            cuda_version : Optional[str]
            distributed_ephemeral_timeout_us : Optional[int]
            duration_us : Optional[int]
            dynamo_compile_time_before_restart_us : Optional[int]
            dynamo_config : Optional[str]
            dynamo_cumulative_compile_time_us : Optional[int]
            dynamo_time_before_restart_s : Optional[float]
            end_time_us : Optional[int]
            entire_frame_compile_time_s : Optional[float]
            fail_reason : Optional[str]
            fail_type : Optional[str]
            fail_user_frame_filename : Optional[str]
            fail_user_frame_lineno : Optional[int]
            feature_usage : Optional[dict[str, bool]]
            frame_key : Optional[str]
            graph_input_count : Optional[int]
            graph_node_count : Optional[int]
            graph_op_count : Optional[int]
            guard_count : Optional[int]
            has_guarded_code : Optional[bool]
            inductor_code_gen_cumulative_compile_time_us : Optional[int]
            inductor_compile_time_s : Optional[float]
            inductor_config : Optional[str]
            inductor_cumulative_compile_time_us : Optional[int]
            inductor_fx_remote_cache_backend_type : Optional[str]
            inductor_fx_remote_cache_hit_count : Optional[int]
            inductor_fx_remote_cache_hit_keys : Optional[str]
            inductor_fx_remote_cache_miss_count : Optional[int]
            inductor_fx_remote_cache_miss_keys : Optional[str]
            is_forward : Optional[bool]
            joint_graph_pass_time_us : Optional[int]
            log_format_version : int
            non_compliant_ops : Optional[Set[str]]
            num_triton_bundles : Optional[int]
            post_grad_pass_time_us : Optional[int]
            pre_grad_pass_time_us : Optional[int]
            remote_cache_time_saved_s : Optional[float]
            remote_cache_version : Optional[int]
            remote_fx_graph_cache_get_time_ms : Optional[int]
            remote_fx_graph_cache_get_time_us : Optional[int]
            remote_fx_graph_cache_put_time_ms : Optional[int]
            remote_fx_graph_cache_put_time_us : Optional[int]
            restart_reasons : Optional[Set[str]]
            runtime_cudagraphify_time_us : Optional[int]
            runtime_triton_autotune_time_us : Optional[int]
            shape_env_guard_count : Optional[int]
            specialize_float : Optional[bool]
            start_time : Optional[float]
            start_time_us : Optional[int]
            structured_logging_overhead_s : Optional[float]
            structured_logging_overhead_us : Optional[int]
            triton_compile_time_us : Optional[int]
            triton_version : Optional[str]
          }
          class CompilationMode {
            name
          }
          class CompileCollectiveRestartAnalysis {
          }
          class CompileContext {
            attempt : int
            compile_id : Optional[CompileId]
            shape_env_guards : List[str]
            current_compile_id()
            current_trace_id()
            get() CompileContext
            try_get() Optional[CompileContext]
          }
          class CompileCounter {
            frame_count : int
            op_count : int
            clear() None
          }
          class CompileCounterWithBackend {
            backend : str
            frame_count : int
            graphs : List[torch.fx.GraphModule]
            op_count : int
          }
          class CompileId {
            frame_compile_id : int
            frame_id : int
          }
          class CompileTimeInstructionCounter {
            clear() None
            end() None
            record()
            start() None
            value() int
          }
          class CompiledAOTI {
            filename : Union[str, List[str]]
            post_compile(example_inputs: Sequence[InputType], cudagraphs: BoxedBool, constants: CompiledFxGraphConstants)* None
            set_triton_bundle(triton_bundle: Any)* None
          }
          class CompiledBackward {
            backward_state_indices : List[int]
            num_symints_saved_for_bw_ : int
            is_backward()
          }
          class CompiledFn {
          }
          class CompiledForward {
            is_backward()
          }
          class CompiledFunction {
            compiled_bw
            compiled_fw
            maybe_subclass_metadata : Optional[SubclassMeta]
            metadata
            num_symints_saved_for_bw
            backward(ctx)
            forward(ctx)
          }
          class CompiledFunctionBackward {
            backward(double_ctx)
            forward(double_ctx)
          }
          class CompiledFxGraph {
            allocated_constant_name : Optional[Dict[str, str]]
            boxed_forward_device_index : Optional[BoxedDeviceIndex]
            cache_key : str
            cache_linemap : Optional[List[Tuple[int, str]]]
            constants : Optional[Dict[str, torch.Tensor]]
            counter_deltas : Counter[str]
            cudagraph_info : Optional[CudagraphCachedInfo]
            current_callable : Optional[Callable[..., Any]]
            device_idxs : OrderedSet[int]
            device_types : OrderedSet[str]
            disabled_cudagraphs_reason : Optional[str]
            fx_kwargs
            guards_expr : Optional[str]
            inputs_to_check : Sequence[int]
            metrics_deltas
            mutated_input_idxs : OrderedSet[int]
            mutated_inputs : OrderedSet[str]
            output_strides : Optional[List[Optional[Tuple[_StrideExprStr, ...]]]]
            source_code : str
            torchbind_constants : Dict[str, torch._C.ScriptObject]
            after_deserialization(constants: CompiledFxGraphConstants) str
            post_compile(example_inputs: Sequence[InputType], cudagraphs: BoxedBool, constants: CompiledFxGraphConstants) None
            prepare_for_serialization() None
            set_triton_bundle(triton_bundle: Any) None
          }
          class CompiledFxGraphConstants {
            unwrap(g: CompiledFxGraph) Dict[str, torch.Tensor]
          }
          class CompiledFxGraphConstantsWithGm {
            gm
            unwrap(g: CompiledFxGraph) Dict[str, torch.Tensor]
          }
          class CompilerBisector {
            bisection_enabled : bool
            advance_backend(curr_backend: str) Optional[str]
            advance_subsystem(curr_backend: str, curr_subsystem: Subsystem) Optional[Subsystem]
            delete_bisect_status() None
            disable_subsystem(backend: str, subsystem: str, debug_info: Optional[Callable[[], str]]) bool
            do_bisect(fn: Callable[[], bool], cli_interface: bool) Optional[BisectionResult]
            get_backend() Optional[str]
            get_bisect_range(backend_name: str, subsystem_name: str) Tuple[int, int]
            get_config_change(config_name: str) Optional[Dict[str, object]]
            get_dir() str
            get_run_state(backend_name: str, subsystem_name: str) Optional[str]
            get_subsystem() Optional[str]
            get_subsystem_object(backend_name: str, subsystem_name: str) Subsystem
            get_system_counter(name: str, increment: bool) int
            initialize_system() None
            process_subsystem(curr_backend: str, curr_subsystem: Subsystem, fn: Callable[[], bool], cli_interface: bool) bool
            read_lines_from_file(file_path: str) List[str]
            set_config_values(backend: str, subsystem: str, config_data: Dict[str, object]) None
            update_bisect_range(backend_name: str, subsystem_name: str, low: int, high: int) None
            update_bisect_status(backend_name: str, subsystem_name: str) None
            update_config_change(backend: str, subsystem: ConfigChange) None
            update_run_state(backend_name: str, subsystem: Subsystem, run_state: str) None
            write_lines_to_file(file_path: str, lines: List[str]) None
          }
          class CompilerWrapper {
            post_compile(compiled_fn, aot_config) Callable
            pre_compile(flat_fn, flat_args: List[Tensor], aot_config: AOTConfig) Tuple[Callable, List[Tensor], ViewAndMutationMeta]
          }
          class ComplexDoubleStorage {
            dtype()
          }
          class ComplexDoubleStorage {
            dtype()
          }
          class ComplexFloatStorage {
            dtype()
          }
          class ComplexFloatStorage {
            dtype()
          }
          class ComplexView {
            get_inputs_that_alias_output() Sequence[str]
            should_allocate() bool
          }
          class Component {
            constructor_args : List[str]
            getattr_maps : Dict[torch.fx.Node, torch.fx.Node]
            gm : Optional[torch.fx.GraphModule]
            graph
            input_placeholders : List
            name : str
            order : int
            orig_inputs : List
            orig_outputs : List
          }
          class ComposableQuantizer {
            quantizers : List[Quantizer]
            annotate(model: torch.fx.GraphModule) torch.fx.GraphModule
            transform_for_annotation(model: torch.fx.GraphModule) torch.fx.GraphModule
            validate(model: torch.fx.GraphModule)* None
          }
          class ComposeTransform {
            inv
            parts : List[Transform]
            bijective()
            codomain()
            domain()
            forward_shape(shape)
            inverse_shape(shape)
            log_abs_det_jacobian(x, y)
            sign()
            with_cache(cache_size)
          }
          class CompositeCompliantTensor {
            elem
          }
          class CompositeCompliantTensorMode {
          }
          class CompositeModel {
            l1
            l2
            u1
            u2
            forward(x)
          }
          class CompositeParamModel {
            l
            p
            u1
            u2
            forward(x)
          }
          class ComptimeContext {
            assert_static(val)
            get_local(name: str) ComptimeVar
            graph()
            graph_break(msg)
            parent()
            print(val)
            print_bt()
            print_disas()
            print_graph()
            print_guards()
            print_locals()
            print_value_stack()
            sleep(sec)
          }
          class ComptimeVar {
            as_fake()
            as_proxy()
            as_python_constant()
            force_static()
            is_dynamic()
            is_proxy()
            is_python_constant()
            python_type()
            size(dim: Optional[int]) Union[int, torch.SymInt]
          }
          class ComptimeVariable {
            call_function(tx: 'InstructionTranslator', args: 'List[VariableTracker]', kwargs: 'Dict[str, VariableTracker]') 'VariableTracker'
            reconstruct(codegen)*
            var_getattr(tx: 'InstructionTranslator', name: str) 'VariableTracker'
          }
          class ComputedBuffer {
            data
            name
            origin_node : Field
            origins : Field
            traceback : Field
            constant_to_device(device: torch.device) IRNode
            decide_layout() None
            get_computed_buffer_name() Optional[str]
            get_default_sizes_body() Tuple[Tuple[List[sympy.Expr], List[sympy.Expr]], LoopBody, Tuple[List[sympy.Expr], List[sympy.Expr]]]
            get_fill_order() Optional[List[int]]
            get_read_names() OrderedSet[str]
            get_read_writes() dependencies.ReadWrites
            get_reads() OrderedSet[Dep]
            get_reduction_size() Sequence[sympy.Expr]
            get_reduction_type() Optional[str]
            get_store_function() Callable[..., OpsValue]
            get_unbacked_symbol_uses() OrderedSet[sympy.Symbol]
            is_no_op() bool
            make_loader() Callable[[Sequence[Expr]], OpsValue]
            num_reads() int
            should_allocate() bool
            simplify_and_reorder(extra_indexing_constraints: Optional[Tuple[Dict[Any, Any], List[Any]]], recompute_sizes_body_func: Optional[Callable[..., Any]]) Tuple[Tuple[List[sympy.Expr], List[sympy.Expr]], LoopBody]
          }
          class ConcatDataFramesPipe {
            n_batch : int
            source_datapipe
          }
          class ConcatDataset {
            cummulative_sizes
            cumulative_sizes : List[int]
            datasets : List[Dataset[_T_co]]
            cumsum(sequence)
          }
          class ConcatKernel {
            inputs : list
            name
            can_realize_into_without_copy(src, dst)
            create(inputs, dim)
            realize_into(src, dst)
            should_allocate() bool
          }
          class ConcaterIterDataPipe {
            datapipes : Tuple[IterDataPipe]
          }
          class ConcaterMapDataPipe {
            datapipes : Tuple[MapDataPipe]
          }
          class ConcreteProp {
            pbar : tqdm
            seen_storages : set
            skip_offload : bool
            writer : NoneType
            propagate()
            run_node(n)
          }
          class ConcreteTypeStore {
            methods_compiled : Set[torch._C.ConcreteModuleType]
            type_store : Dict[Type[Module], List[torch._C.ConcreteModuleType]]
            get_or_create_concrete_type(nn_module)
          }
          class CondAutogradOp {
            backward(ctx)
            forward(ctx, pred, fw_true_graph, fw_false_graph, joint_true_graph, joint_false_graph)
          }
          class CondBranchClassMethod {
            subm
            bar(x)
            forward(x)
          }
          class CondBranchNestedFunction {
            forward(x)
          }
          class CondBranchNonlocalVariables {
            forward(x)
          }
          class CondClosedOverVariable {
            forward(pred, x)
          }
          class CondHigherOrderVariable {
            call_function(tx: 'InstructionTranslator', args: 'List[VariableTracker]', kwargs: 'Dict[str, VariableTracker]') 'VariableTracker'
          }
          class CondOp {
          }
          class CondOpArgsMismatchError {
          }
          class CondOperands {
            forward(x, y)
          }
          class CondPredicate {
            forward(x)
          }
          class Conditional {
            false_subgraph : Optional[Subgraph]
            name
            operands : Optional[List[TensorBox]]
            outputs : Optional[List[MultiOutput]]
            predicate : Optional[IRNode]
            true_subgraph : Optional[Subgraph]
            codegen(wrapper) None
            create(predicate: TensorBox, true_fn: Subgraph, false_fn: Subgraph, operands: List[TensorBox])
          }
          class Config {
            expr_count : Dict[str, int]
            expr_to_name : Dict[str, str]
          }
          class Config {
            default : Any
            env_name_default : Optional[str]
            env_name_force : Optional[str]
            justknob : Optional[str]
            value_type : Optional[type]
          }
          class ConfigChange {
            config_field : str
            config_name : str
            config_value : object
            name : str
          }
          class ConfigModule {
            codegen_config() str
            get_config_copy() Dict[str, Any]
            get_hash() bytes
            get_type(config_name: str) type
            load_config(maybe_pickled_config: Union[bytes, Dict[str, Any]]) None
            patch(arg1: Optional[Union[str, Dict[str, Any]]], arg2: Any) 'ContextDecorator'
            save_config() bytes
            save_config_portable() Dict[str, Any]
            shallow_copy_dict() Dict[str, Any]
            to_dict() Dict[str, Any]
          }
          class ConfigModuleInstance {
          }
          class ConfigPatch {
          }
          class ConfigurableQuantizeHandler {
            dtype_configs
            observation_type
            is_general_tensor_value_op() bool
          }
          class ConfigurationOverride {
            configuration
            descriptor
            properties : Optional[_property_bag.PropertyBag]
          }
          class Conj {
            conjucts
          }
          class ConnectionWrapper {
            conn
            recv()
            send(obj)
          }
          class ConsoleMetricHandler {
            emit(metric_data: MetricData)
          }
          class ConstDictKeySource {
            is_dict_key()
            name()
            reconstruct(codegen)
          }
          class ConstDictVariable {
            items
            original_items
            should_reconstruct_all : bool
            user_cls : dict
            as_proxy()
            as_python_constant()
            call_hasattr(tx, name)
            call_method(tx, name, args: 'List[VariableTracker]', kwargs: 'Dict[str, VariableTracker]') 'VariableTracker'
            debug_repr()
            getitem_const(tx: 'InstructionTranslator', arg: VariableTracker)
            getitem_const_raise_exception_if_absent(tx: 'InstructionTranslator', arg: VariableTracker)
            has_new_items()
            is_new_item(value, other)
            keys_as_python_constant()
            len()
            maybe_getitem_const(arg: VariableTracker)
            python_type()
            reconstruct(codegen)
            unpack_var_sequence(tx)
          }
          class ConstMap {
            const_mapping
          }
          class Constant {
            device
            dtype
            value : Any
            constant_to_device(device: torch.device) IRNode
            make_loader() Callable[[Sequence[Expr]], OpsValue]
            realize()* Optional[str]
          }
          class ConstantArgument {
            name : str
            value : Union[int, float, bool, str, None]
          }
          class ConstantAttrMap {
            add(key: Union[torch.Tensor, torch.ScriptObject, FakeScriptObject], value: Any) None
          }
          class ConstantBuffer {
            override_device : Optional[torch.device]
            constant_to_device(device: torch.device) IRNode
            make_loader() Callable[[Sequence[Expr]], OpsValue]
          }
          class ConstantExtraMetadataTensor {
            constant_attribute : int
            elem
            add_constant(a)
          }
          class ConstantFolder {
            deferred_value : object
            lifted_constant_names : Optional[List[str]]
            node_replacements : Dict[torch.fx.Node, Any]
            replaced_uses : Dict[torch.fx.Node, int]
            skip_constructors : bool
            unknown_value : object
            user_to_last_uses : defaultdict
            add_node_replacement(node: torch.fx.Node, tensor: torch.Tensor) None
            insert_placerholder_values(env: Dict[torch.fx.Node, Any]) None
            insertable_tensor_check(tensor: torch.Tensor) bool
            is_impure(node: torch.fx.node.Node) bool
            node_to_last_non_output_use() Dict[torch.fx.Node, List[torch.fx.Node]]
            run() Any
            run_node(node: torch.fx.Node) Any
          }
          class ConstantFolder {
            node_replacements : Dict[torch.fx.Node, Any]
            replaced_uses : Dict[torch.fx.Node, int]
            skip_constructors : bool
            unknown_value : object
            user_to_last_uses : defaultdict
            add_node_replacement(node: torch.fx.Node, tensor: torch.Tensor) None
            insertable_tensor_check(tensor: torch.Tensor) bool
            is_impure(node: torch.fx.Node) bool
            node_to_last_non_output_use()
            run()
            run_node(node)
          }
          class ConstantIntNode {
            val : int
            clone() 'ConstantIntNode'
            constant_int() int
            eq(other: Any) Any
            ge(other: Any) Any
            gt(other: Any) Any
            is_bool() bool
            is_constant() bool
            is_float() bool
            is_int() bool
            is_nested_int() bool
            is_symbolic() bool
            le(other: Any) Any
            lt(other: Any) Any
            maybe_as_int() int
            mul(other: Any) Any
            ne(other: Any) Any
          }
          class ConstantLR {
            factor : float
            total_iters : int
            get_lr()
          }
          class ConstantLikeVariable {
            value
            as_python_constant()
            call_method(tx, name, args: List[VariableTracker], kwargs: Dict[str, VariableTracker]) VariableTracker
            var_getattr(tx: 'InstructionTranslator', name: str) VariableTracker
          }
          class ConstantPad1d {
            padding : Tuple[int, int]
          }
          class ConstantPad2d {
            padding : Tuple[int, int, int, int]
          }
          class ConstantPad3d {
            padding : Tuple[int, int, int, int, int, int]
          }
          class ConstantRegexMatchVariable {
          }
          class ConstantSource {
            source_name : str
            guard_source()
            make_guard(fn)*
            name()
            reconstruct(codegen)
          }
          class ConstantValue {
            as_bool : Annotated[bool, 50]
            as_float : Annotated[float, 30]
            as_int : Annotated[int, 20]
            as_none : Annotated[bool, 10]
            as_string : Annotated[str, 40]
          }
          class ConstantVariable {
            items
            value
            as_proxy()
            as_python_constant()
            call_hasattr(tx: 'InstructionTranslator', name: str) 'VariableTracker'
            call_method(tx, name, args: 'List[VariableTracker]', kwargs: 'Dict[str, VariableTracker]') 'VariableTracker'
            const_getattr(tx: 'InstructionTranslator', name)
            create(value) VariableTracker
            getitem_const(tx: 'InstructionTranslator', arg: VariableTracker)
            is_base_literal(obj)
            is_literal(obj)
            is_python_constant()
            unpack_var_sequence(tx)
          }
          class ConstrainAsSizeExample {
            forward(x)
          }
          class ConstrainAsValueExample {
            forward(x, y)
          }
          class Constraint {
            warn_only : bool
          }
          class Constraint {
          }
          class Constraint {
            event_dim : int
            is_discrete : bool
            check(value)*
          }
          class ConstraintGenerator {
            constraints : list
            graph : NoneType
            symbol_dict : dict
            traced
            traced_params : dict
            generate_constraints(counter)
            generate_constraints_node(n: Node, counter)
          }
          class ConstraintRegistry {
            register(constraint, factory)
          }
          class ConstraintViolationError {
            args : tuple
          }
          class Constructor {
            backward(ctx, grad_output)
            forward(ctx, data, mask)
          }
          class ConstructorMoverPass {
            allow_outputs : bool
            target : str
            allow_cpu_device(node: fx.Node) bool
            cannot_be_moved(node: fx.Node) bool
            find_movable_constructors(graph: fx.Graph, constructors: List[fx.Node]) OrderedSet[fx.Node]
            get_cpu_indeg_count(graph: fx.Graph) Dict[fx.Node, int]
            get_node_device(node: fx.Node) Optional[torch.device]
          }
          class Container {
          }
          class ContentStoreReader {
            loc : str
            storage_cache : Optional[Dict[Optional[torch.device], Dict[str, StorageWeakRef]]], defaultdict
            read_storage(h: str) torch.UntypedStorage
            read_tensor(name: str) torch.Tensor
            read_tensor_metadata(name: str)
          }
          class ContentStoreWriter {
            loc : str
            seen_storage_hashes : Set[str]
            stable_hash : bool
            compute_tensor_metadata(t: torch.Tensor, h)
            write_storage(storage: torch.UntypedStorage) str
            write_tensor(name: str, t: torch.Tensor) None
          }
          class ContextDecorator {
          }
          class ContextMangerState {
            cleanup_fn : Optional[Callable]
            proxy : Optional[torch.fx.Proxy]
            cleanup()
            cleanup_assert()
          }
          class ContextProp {
            getter
            setter
          }
          class ContextWrappingVariable {
            initial_values : NoneType
            state : NoneType
            target_values
            call_function(tx: 'InstructionTranslator', args: 'List[VariableTracker]', kwargs: 'Dict[str, VariableTracker]') 'VariableTracker'
            enter(tx)
            exit(tx: 'InstructionTranslator')
            exit_on_graph_break()
            fn_name()*
            module_name()*
            reconstruct(codegen)
            reconstruct_type(codegen)
            set_cleanup_hook(tx: 'InstructionTranslator', fn)
            supports_graph_breaks()
          }
          class ContinueExecutionCache {
            cache
            generated_code_metadata
            generate(code, lineno, offset: int, setup_fn_target_offsets: Tuple[int, ...], nstack: int, argnames: Tuple[str, ...], argnames_null: Tuple[str, ...], setup_fns: Tuple[ReenterWith, ...], stack_ctx_vars: Tuple[Tuple[int, Tuple[Any]], ...], argnames_ctx_vars: Tuple[Tuple[str, Tuple[Any]], ...], null_idxes: Tuple[int, ...]) types.CodeType
            generate_based_on_original_code_object(code, lineno, offset: int, setup_fn_target_offsets: Tuple[int, ...])
            lookup(code, lineno)
            unreachable_codes(code_options) List[Instruction]
          }
          class ContinuousBernoulli {
            arg_constraints : dict
            has_rsample : bool
            logits
            mean
            param_shape
            probs
            stddev
            support
            variance
            cdf(value)
            entropy()
            expand(batch_shape, _instance)
            icdf(value)
            log_prob(value)
            logits()
            probs()
            rsample(sample_shape: _size) torch.Tensor
            sample(sample_shape)
          }
          class ControlFlow {
            example_inputs()
            forward(xs: torch.Tensor, pred1: torch.Tensor, pred2: torch.Tensor, y: torch.Tensor) torch.Tensor
          }
          class ControlFlowToyModel {
            lin1
            lin2
            forward(x)
          }
          class Conv1d {
            forward(input: Tensor, reduce_range: bool) Tensor
          }
          class Conv1d {
            forward(x: torch.Tensor) torch.Tensor
            from_float(float_conv, weight_qparams)
          }
          class Conv1d {
            bias()
            forward(input)
            from_float(mod, use_precomputed_fake_quant)
            set_weight_bias(w: torch.Tensor, b: Optional[torch.Tensor]) None
            weight()
          }
          class Conv1d {
            from_float(mod, use_precomputed_fake_quant)
          }
          class Conv1d {
            forward(input: Tensor) Tensor
          }
          class Conv2d {
            forward(input: Tensor, reduce_range: bool) Tensor
          }
          class Conv2d {
            forward(x: torch.Tensor) torch.Tensor
            from_float(float_conv, weight_qparams)
          }
          class Conv2d {
            bias()
            forward(input)
            from_float(mod, use_precomputed_fake_quant)
            set_weight_bias(w: torch.Tensor, b: Optional[torch.Tensor]) None
            weight()
          }
          class Conv2d {
            forward(input)
            from_float(mod, use_precomputed_fake_quant)
          }
          class Conv2d {
            forward(input: Tensor) Tensor
          }
          class Conv2dActivation {
            conv2d1
            conv2d2
            seq
            forward(x: torch.Tensor) torch.Tensor
          }
          class Conv2dBias {
            conv2d1
            conv2d2
            seq
            forward(x: torch.Tensor) torch.Tensor
          }
          class Conv2dBiasFollowedByBatchNorm2dPattern {
            description : str
            name : str
            skip
            url : str
            match(event: _ProfilerEvent)
          }
          class Conv2dPadBias {
            act1
            act2
            conv2d1
            conv2d2
            seq
            forward(x: torch.Tensor) torch.Tensor
          }
          class Conv2dPool {
            af1
            conv2d1
            conv2d2
            conv2d3
            maxpool
            seq
            forward(x: torch.Tensor) torch.Tensor
          }
          class Conv2dPoolFlatten {
            af1
            avg_pool
            conv2d1
            conv2d2
            fc
            flatten
            seq
            forward(x: torch.Tensor) torch.Tensor
          }
          class Conv2dPoolFlattenFunctional {
            af1
            avg_pool
            conv2d1
            conv2d2
            fc
            seq
            forward(x: torch.Tensor) torch.Tensor
          }
          class Conv2dPropAnnotaton {
            conv
            linear
            forward(x)
          }
          class Conv2dThenConv1d {
            conv1d
            conv2d
            example_inputs()
            forward(x)
          }
          class Conv2dWithCat {
            conv1
            conv2
            forward(x, y)
          }
          class Conv2dWithObsSharingOps {
            adaptive_avg_pool2d
            conv
            hardtanh
            forward(x)
          }
          class Conv2dWithTwoCat {
            conv1
            conv2
            forward(x1, x2, x3, x4)
          }
          class Conv2dWithTwoLinear {
            conv
            linear1
            linear2
            forward(x)
          }
          class Conv2dWithTwoLinearPermute {
            conv
            linear1
            linear2
            forward(x)
          }
          class Conv3d {
            forward(input: Tensor, reduce_range: bool) Tensor
          }
          class Conv3d {
            forward(x: torch.Tensor) torch.Tensor
            from_float(float_conv, weight_qparams)
          }
          class Conv3d {
            bias()
            forward(input)
            from_float(mod, use_precomputed_fake_quant)
            set_weight_bias(w: torch.Tensor, b: Optional[torch.Tensor]) None
            weight()
          }
          class Conv3d {
            forward(input)
            from_float(mod, use_precomputed_fake_quant)
          }
          class Conv3d {
            forward(input: Tensor) Tensor
          }
          class ConvAdd2d {
            forward(input, extra_input)
            from_float(mod, use_precomputed_fake_quant)
            from_reference(ref_qconv, output_scale, output_zero_point)
          }
          class ConvAdd2d {
            add
            forward(x1, x2)
          }
          class ConvAddReLU2d {
            forward(input, extra_input)
            from_float(mod, use_precomputed_fake_quant)
            from_reference(ref_qconv, output_scale, output_zero_point)
          }
          class ConvAddReLU2d {
            add
            relu
            forward(x1, x2)
          }
          class ConvBNFusion {
            bn_bias : NoneType
            bn_eps : NoneType
            bn_module : NoneType
            bn_nodes : list
            bn_running_mean : NoneType
            bn_running_var : NoneType
            bn_weight : NoneType
            conv_module
            fusion_enabled : bool
            add_bn_node(bn_node)
            disable_fusion()
            is_fusion_enabled()
          }
          class ConvBNReLU {
          }
          class ConvBn1d {
          }
          class ConvBn1d {
          }
          class ConvBn2d {
          }
          class ConvBn2d {
          }
          class ConvBn3d {
          }
          class ConvBn3d {
          }
          class ConvBnAddReluModel {
            bn
            conv
            conv2
            left_conv : bool
            relu
            two_conv : bool
            use_torch_add : bool
            with_bn : bool
            with_relu : bool
            forward(x1, x2)
            get_example_inputs() Tuple[Any, ...]
          }
          class ConvBnModel {
            bn
            conv
            forward(x)
            get_example_inputs() Tuple[Any, ...]
          }
          class ConvBnReLU1d {
          }
          class ConvBnReLU1d {
            forward(input)
            from_float(mod, use_precomputed_fake_quant)
          }
          class ConvBnReLU2d {
          }
          class ConvBnReLU2d {
            forward(input)
            from_float(mod, use_precomputed_fake_quant)
          }
          class ConvBnReLU2dAndLinearReLU {
            conv_bn_relu
            linear
            relu
            forward(x)
          }
          class ConvBnReLU3d {
          }
          class ConvBnReLU3d {
            forward(input)
            from_float(mod, use_precomputed_fake_quant)
          }
          class ConvBnReLUModel {
            bn
            conv
            relu
            forward(x)
            get_example_inputs() Tuple[Any, ...]
          }
          class ConvLayoutParams {
            dilation : tuple[int, ...]
            groups : int
            output_padding : tuple[int, ...]
            padding : tuple[int, ...]
            stride : tuple[int, ...]
            transposed : bool
          }
          class ConvLinearWPermute {
            conv
            linear1
            forward(x)
          }
          class ConvMaxPool2d {
            conv
            pool
            forward(x)
          }
          class ConvModel {
            conv
            forward(x)
            get_example_inputs() Tuple[Any, ...]
          }
          class ConvPerSampleGrad {
            backward(ctx, grad_output)
            forward(ctx, kwarg_names, conv_fn)
          }
          class ConvPoolArgs2d {
            dilation_h : int
            dilation_w : int
            group : int
            kernel_h : int
            kernel_w : int
            pad_b : int
            pad_l : int
            pad_r : int
            pad_t : int
            stride_h : int
            stride_w : int
          }
          class ConvReLU1d {
            forward(input)
            from_float(mod, use_precomputed_fake_quant)
            from_reference(ref_qconv, output_scale, output_zero_point)
          }
          class ConvReLU1d {
          }
          class ConvReLU1d {
            qconfig : NoneType
            weight_fake_quant
            forward(input)
            from_float(mod, use_precomputed_fake_quant)
          }
          class ConvReLU2d {
            forward(input)
            from_float(mod, use_precomputed_fake_quant)
            from_reference(ref_qconv, output_scale, output_zero_point)
          }
          class ConvReLU2d {
          }
          class ConvReLU2d {
            qconfig : NoneType
            weight_fake_quant
            forward(input)
            from_float(mod, use_precomputed_fake_quant)
          }
          class ConvReLU3d {
            forward(input)
            from_float(mod, use_precomputed_fake_quant)
            from_reference(ref_qconv, output_scale, output_zero_point)
          }
          class ConvReLU3d {
          }
          class ConvReLU3d {
            qconfig : NoneType
            weight_fake_quant
            forward(input)
            from_float(mod, use_precomputed_fake_quant)
          }
          class ConvReluAddModel {
            fc1
            fc2
            relu
            forward(x)
            get_example_inputs() Tuple[Any, ...]
          }
          class ConvReluConvModel {
            fc1
            fc2
            relu
            forward(x)
            get_example_inputs() Tuple[Any, ...]
          }
          class ConvReluModel {
            fc
            relu
            forward(x)
            get_example_inputs() Tuple[Any, ...]
          }
          class ConvReluQuantizeHandler {
          }
          class ConvTWithBNRelu {
            bn
            convt
            relu
            forward(x)
          }
          class ConvTranspose1d {
            forward(input: Tensor, reduce_range: bool) Tensor
          }
          class ConvTranspose1d {
            forward(x: torch.Tensor, output_size: Optional[List[int]]) torch.Tensor
            from_float(float_conv, weight_qparams)
          }
          class ConvTranspose1d {
            bias()
            forward(input)
            from_reference(ref_qconvt, output_scale, output_zero_point)
            set_weight_bias(w: torch.Tensor, b: Optional[torch.Tensor]) None
            weight()
          }
          class ConvTranspose1d {
            forward(input: Tensor, output_size: Optional[List[int]]) Tensor
          }
          class ConvTranspose2d {
            forward(input: Tensor, reduce_range: bool) Tensor
          }
          class ConvTranspose2d {
            forward(x: torch.Tensor, output_size: Optional[List[int]]) torch.Tensor
            from_float(float_conv, weight_qparams)
          }
          class ConvTranspose2d {
            bias()
            forward(input)
            from_reference(ref_qconvt, output_scale, output_zero_point)
            set_weight_bias(w: torch.Tensor, b: Optional[torch.Tensor]) None
            weight()
          }
          class ConvTranspose2d {
            forward(input: Tensor, output_size: Optional[List[int]]) Tensor
          }
          class ConvTranspose3d {
            forward(input: Tensor, reduce_range: bool) Tensor
          }
          class ConvTranspose3d {
            forward(x: torch.Tensor, output_size: Optional[List[int]]) torch.Tensor
            from_float(float_conv, weight_qparams)
          }
          class ConvTranspose3d {
            bias()
            forward(input)
            from_reference(ref_qconvt, output_scale, output_zero_point)
            set_weight_bias(w: torch.Tensor, b: Optional[torch.Tensor]) None
            weight()
          }
          class ConvTranspose3d {
            forward(input: Tensor, output_size: Optional[List[int]]) Tensor
          }
          class ConvTransposeModel {
            conv
            forward(x)
            get_example_inputs() Tuple[Any, ...]
          }
          class ConvWithAdaptiveAvgPool2d {
            adaptive_avg_pool2d
            conv
            forward(x)
          }
          class ConvWithBNRelu {
            bn
            conv
            relu
            forward(x)
          }
          class Conversion {
            analysis_tool_log_files : Optional[List[_artifact_location.ArtifactLocation]]
            invocation : Optional[_invocation.Invocation]
            properties : Optional[_property_bag.PropertyBag]
            tool
          }
          class ConversionError {
          }
          class ConvertComplexToRealRepresentationInputStep {
            apply(model_args: Sequence[Any], model_kwargs: Mapping[str, Any], model: torch.nn.Module | Callable | torch_export.ExportedProgram | None) tuple[Sequence[Any], Mapping[str, Any]]
          }
          class ConvertComplexToRealRepresentationOutputStep {
            apply(model_outputs: Any, model: torch.nn.Module | Callable | torch_export.ExportedProgram | None) Any
          }
          class ConvertCustomConfig {
            observed_to_quantized_mapping : Dict[QuantType, Dict[Type, Type]]
            preserved_attributes : List[str]
            from_dict(convert_custom_config_dict: Dict[str, Any]) ConvertCustomConfig
            set_observed_to_quantized_mapping(observed_class: Type, quantized_class: Type, quant_type: QuantType) ConvertCustomConfig
            set_preserved_attributes(attributes: List[str]) ConvertCustomConfig
            to_dict() Dict[str, Any]
          }
          class ConvertFrame {
          }
          class ConvertFrameAssert {
          }
          class ConvertFrameProtocol {
          }
          class ConvertIntKey {
            get(b: bool) Union[int, SymInt]
          }
          class ConvertIntSource {
            guard_source()
            name()
            reconstruct(codegen)
          }
          class Converter {
            call_method
            call_module
            get_attr
            call_function(target: str, args: Sequence[Any], kwargs: Mapping[str, Any]) PatternExpr
            placeholder(target: str, args: Sequence[Any], kwargs: Mapping[str, Any]) Union[ExclusiveKeywordArg, KeywordArg]
            run_node(n: torch.fx.Node) Any
          }
          class ConvolutionBinary {
            cpp_constant_args : tuple
            codegen(wrapper)
            create(x: 'TensorBox', other: 'TensorBox', weight: 'TensorBox', bias: 'TensorBox', padding_: List[int], stride_: List[int], dilation_: List[int], groups: int, binary_attr: str, binary_alpha: Optional[float], unary_attr: Optional[str], unary_scalars: Optional[List[Any]], unary_algorithm: Optional[str])
          }
          class ConvolutionBinaryInplace {
            mutation_outputs : list
            codegen(wrapper)
            create(x: 'TensorBox', other: 'TensorBox', weight: 'TensorBox', bias: 'TensorBox', padding_: List[int], stride_: List[int], dilation_: List[int], groups: int, binary_attr: str, binary_alpha: Optional[float], unary_attr: Optional[str], unary_scalars: Optional[List[Any]], unary_algorithm: Optional[str])
            get_unbacked_symbol_defs() OrderedSet[sympy.Symbol]
          }
          class ConvolutionTransposeUnary {
            codegen(wrapper)
            create(x: 'TensorBox', weight: 'TensorBox', bias: 'TensorBox', padding_: List[int], output_padding_: List[int], stride_: List[int], dilation_: List[int], groups_: int, attr, scalars: Optional[List[Any]], algorithm)
          }
          class ConvolutionUnary {
            codegen(wrapper)
            create(x: 'TensorBox', weight: 'TensorBox', bias: 'TensorBox', padding_: List[int], stride_: List[int], dilation_: List[int], groups: int, attr, scalars: Optional[List[Any]], algorithm)
          }
          class CooperativeReductionWorkspaceCache {
            args
            current_loop : list
            loop_count : int
            prior_loop : list
            ready_for_reuse : defaultdict
            store_count : int
            allocate(nbytes: sympy.Expr)
            increment_store_count()
            on_loop_end()
          }
          class CoordescTuner {
            cached_benchmark_results : dict
            inductor_meta : dict
            is_mm : bool
            name : str
            size_hints : NoneType
            tunable_fields
            autotune(func: Callable[['triton.Config'], float], baseline_config: 'triton.Config', baseline_timing: Optional[float]) 'triton.Config'
            cache_benchmark_result(config, timing)
            call_func(func, config)
            check_all_tuning_directions(func: Callable[['triton.Config'], float], best_config, best_timing)
            compare_config(func, candidate_config, best_config, best_timing)
            get_config_max(prefix: str) int
            get_neighbour_values(name, orig_val, radius, include_self)
            get_warpsmax()
            has_improvement(baseline, test)
            lookup_in_cache(config)
            value_too_large(name: str, val: int) bool
          }
          class CopyGraph {
            run_node(old_node: torch.fx.Node) torch.fx.Node
          }
          class CopyIfCallgrind {
            serialization
            setup
            value
            unwrap_all(globals: Dict[str, Any]) Dict[str, Any]
          }
          class CopyNodeQuantizeHandler {
          }
          class CoreMLComputeUnit {
            ALL : str
            CPU : str
            CPUAndGPU : str
          }
          class CoreMLQuantizationMode {
            LINEAR : str
            LINEAR_SYMMETRIC : str
            NONE : str
          }
          class CorrCholeskyTransform {
            bijective : bool
            codomain
            domain
            forward_shape(shape)
            inverse_shape(shape)
            log_abs_det_jacobian(x, y, intermediates)
          }
          class CosineAnnealingLR {
            T_max : int
            eta_min : float
            get_lr()
          }
          class CosineAnnealingWarmRestarts {
            T_0 : int
            T_cur : int
            T_i : int
            T_mult : int
            eta_min : float
            last_epoch
            get_lr()
            step(epoch)
          }
          class CosineEmbeddingLoss {
            margin : float
            forward(input1: Tensor, input2: Tensor, target: Tensor) Tensor
          }
          class CosineSimilarity {
            dim : int
            eps : float
            forward(x1: Tensor, x2: Tensor) Tensor
          }
          class CountIteratorVariable {
            item : NoneType, int
            step : int
            next_variable(tx)
            reconstruct(codegen)
          }
          class CountOps {
          }
          class CppBenchmarkRequest {
            DLL : Optional[Union[CDLL, ModuleType]]
            hash_key
            source_code : str
            cleanup_run_fn() None
            make_run_fn() Callable[[], None]
            precompile()
          }
          class CppBmmTemplate {
            b_index : Symbol
            render_options : dict
            check_if_block_weight(W, micro_gemm)
            codegen_gemm_stub_def()
            codegen_multi_thread_gemm()
            codegen_single_thread_gemm()
            get_default_reindexers(epilogue_nodes)
            get_gemm_function_call(kernel: CppTemplateKernel, function_name: str, placeholder: str, b_index: int) str
            get_options(kernel: CppTemplateKernel, template_buffer_node: Optional[ir.CppTemplateBuffer], flag_template_buffer_has_other_users: Optional[bool], epilogue_nodes: Optional[List[ir.IRNode]]) Dict[str, Any]
            get_padded_size(n, block_n, k, should_block_weight)
            render(kernel: CppTemplateKernel, template_buffer_node: Optional[ir.CppTemplateBuffer], flag_template_buffer_has_other_users: Optional[bool], epilogue_nodes: Optional[List[ir.IRNode]]) str
          }
          class CppBuilder {
            build() Tuple[bytes, str]
            get_command_line() str
            get_target_file_path() str
          }
          class CppCSEVariable {
            dependent_itervars
            dtype : NoneType
            is_vec : bool
            depends_on(itervar: sympy.Symbol)
            update_on_args(name, args, kwargs)
          }
          class CppCodeCache {
            cache : Dict[str, Callable[[], Union[CDLL, ModuleType]]]
            cache_clear : staticmethod
            cpp_compile_command_flags : Dict[str, Any]
            load(source_code: str, device_type: str) Any
            load_async(source_code: str, device_type: str, submit_fn: Any, extra_flags: Sequence[str]) Any
          }
          class CppCompileError {
          }
          class CppFlexAttentionTemplate {
            extra_sizevars : list
            fake_buffers
            has_other_buffer
            kernel_input_name_to_buffer
            kv_block_size
            len_mask_other
            len_score_other
            mask_buf_idx : NoneType
            mask_buf_name : NoneType
            mask_mod
            mask_mod_other_buffers : NoneType
            no_full_kv_block
            other_buf_start_idx : int
            other_buffer_input_offset : int
            other_ptr_data : dict
            scale
            score_buf_idx : NoneType
            score_buf_name : NoneType
            score_mod
            score_mod_other_buffers : NoneType
            add_choices(choices, input_nodes, layout, scale, score_mod, mask_mod, kv_block_size, has_other_buffer, no_full_kv_block, fake_buffers, len_score_other, len_mask_other, kernel_input_name_to_buffer)
            apply_score_mod(score, b, h, q_idx, kv_idx)
            codegen_allocate_buffer(buffer_name: str, buffer_dtype, buffer_size)
            codegen_brgemm_pack_function(kernel_name: str)
            codegen_softmax_fusion(kernel_name: str)
            generate_other_buffer(buf_list, start_offset, len_attr, kernel_args)
            modification(subgraph_buffer, output_name, output_idx)
            render(kernel, template_buffer_node: Optional[ir.CppTemplateBuffer], epilogue_nodes: Optional[List[ir.IRNode]]) str
            update_kernel_args(kernel_args)
          }
          class CppFunctionalizeAPI {
            commit_update(tensor) None
            functionalize(inner_f: Callable) Callable
            mark_mutation_hidden_from_autograd(tensor) None
            redispatch_to_next() ContextManager
            replace(input_tensor, output_tensor) None
            sync(tensor) None
            unwrap_tensors(args: Union[torch.Tensor, Tuple[torch.Tensor, ...]]) Union[torch.Tensor, Tuple[torch.Tensor, ...]]
            wrap_tensors(args: Tuple[Any]) Tuple[Any]
          }
          class CppGemmTemplate {
            alpha : int
            beta : int
            cache_blocking
            has_bias : bool
            is_dynamic_M
            k
            m
            n
            padded_n
            register_blocking : GemmBlocking
            render_options : dict
            should_block_weights : bool
            thread_blocking
            add_choices(choices, layout, input_nodes, beta, alpha, has_bias, trans_w, input_indices, epilogue_creator: Optional[Callable[[ir.Buffer], ir.Pointwise]])
            block_weight(W, new_size, padding)
            check_if_block_weight(W, micro_gemm)
            codegen_blocks(num_threads, N, K, micro_gemm, is_dynamic_M, kernel, GemmOut, config, L1_cache_size, L2_cache_size, X, W)
            codegen_gemm_stub_def()
            codegen_m_loop_params()
            codegen_microkernel_def()
            codegen_multi_threads_params()
            codegen_n_loop_params()
            codegen_single_thread_params(is_dynamic_M)
            get_default_reindexers(epilogue_nodes)
            get_options(kernel: CppTemplateKernel, template_buffer_node: Optional[ir.CppTemplateBuffer], flag_template_buffer_has_other_users: Optional[bool], epilogue_nodes: Optional[List[ir.IRNode]]) Dict[str, Any]
            get_padded_size(n, block_n, k, should_block_weight)
            log_blockings()
            make_cache_blocking_cache()
            make_thread_blocking_cache()
            maybe_k_slicing()
            pack_vnni_weight(W, micro_gemm, new_size)
            prep_weight(inputs, layout: ir.Layout, micro_gemm: CppMicroGemm, should_block_weight: bool)
            render(kernel: CppTemplateKernel, template_buffer_node: Optional[ir.CppTemplateBuffer], flag_template_buffer_has_other_users: Optional[bool], epilogue_nodes: Optional[List[ir.IRNode]]) str
          }
          class CppKernel {
            active_ranges : dict[sympy.Expr, Tuple[sympy.Expr, ...]]
            assert_function
            call_ranges : Optional[Tuple[sympy.Expr, ...]]
            compute : NoneType
            cse
            inner_itervars : List[sympy.Symbol]
            is_reduction : bool
            itervars : List[sympy.Symbol]
            loads : NoneType
            local_reduction_init
            local_reduction_stores
            newvar_prefix : str
            non_parallel_reduction_prefix
            num_threads
            overrides
            parallel_reduction_prefix
            parallel_reduction_suffix
            poststores
            preloads
            ranges : List[sympy.Expr]
            reduction_cse
            reduction_depth : NoneType
            reduction_omp_dec : Dict[Tuple[str, str], str]
            reduction_prefix
            reduction_prefix_generators : List[Callable]
            reduction_suffix
            reduction_var_names : List[str]
            sexpr
            stores : NoneType
            suffix : str
            weight_recps_cse
            cache_dtype_convert(dst, dst_dtype, src, src_dtype)
            check_bounds(expr: sympy.Expr, size: sympy.Expr, lower: bool, upper: bool)
            codegen_conditions(code: BracesBuffer, prefix: Optional[str], var: Optional[sympy.Symbol])
            codegen_loops(code, worksharing)
            codegen_loops_impl(loop_nest, code, worksharing)
            create_cse_var()
            decide_parallel_depth(max_parallel_depth, threads)
            finalize_reduction_prefix(size: Optional[int])
            gen_body(code: Optional[BracesBuffer])
            get_to_dtype_expr(src, dtype, src_dtype)
            index_depends_on(index: sympy.Expr, itervar: sympy.Symbol)
            index_indirect_depends_on(index: sympy.Expr, itervar: sympy.Symbol)
            index_to_str(index: sympy.Expr) str
            load(name: str, index: sympy.Expr)
            masked(mask)
            reduction(dtype, src_dtype, reduction_type, value)
            scale_index_with_offset(index: sympy.Expr, scale, itervar_idx, offset)
            set_ranges(lengths, reduction_lengths)
            size_hint()
            store(name, index, value, mode)
            store_reduction(name, index, value)
            update_stores_with_parallel_reduction()
            var_ranges()
            write_to_suffix()
          }
          class CppKernelProxy {
            call_ranges : NoneType
            kernel_group
            kernels : List[CppKernel], list
            loop_nest : NoneType
            picked_vec_isa
            reduction_suffix
            aggregate_reduction_buffers(inner_loop_reduction_outer_not: bool, outer_loop: Optional['LoopLevel'])
            codegen_functions(fn_list, var_sizes_list)
            codegen_loop_bodies(loop_bodies, var_sizes_list)
            codegen_loops(code, worksharing)
            codegen_nodes(nodes: List[SchedulerNode])
            data_type_propagation(nodes)
            gen_body(code: Optional[BracesBuffer])
            is_lowp_fp_scheduler(scheduler_node: SchedulerNode)
            legalize_lowp_fp_dtype(nodes)
            legalize_lowp_fp_dtype_loopbody(loop_body: LoopBody)
            update_stores_with_parallel_reduction()
          }
          class CppMicroBrgemm {
            TEMPLATE_ENTRY : str
            codegen_define(kernel: CppTemplateKernel) str
            codegen_finalize(kernel: CppTemplateKernel) str
            get_b_layout()
          }
          class CppMicroGemm {
            DECLARE_KERNEL : str
            alpha : int
            compute_dtype
            input2_dtype
            input_dtype
            name
            output_dtype
            register_blocking
            codegen_call(kernel: CppTemplateKernel, A: ir.Buffer, B: ir.Buffer, C: ir.Buffer, accum: bool) str
            codegen_define(kernel: CppTemplateKernel)* str
            codegen_finalize(kernel: CppTemplateKernel) str
            codegen_init(kernel: CppTemplateKernel) str
            get_b_layout() LayoutType
            get_common_options()
            get_kernel_declaration()
            get_kernel_extra_args() str
            get_kernel_extra_args_declare() str
          }
          class CppMicroGemmAMX {
            TEMPLATE_ENTRY : str
            TEMPLATE_KERNEL : str
            codegen_define(kernel: CppTemplateKernel) str
            codegen_finalize(kernel: CppTemplateKernel) str
            codegen_init(kernel: CppTemplateKernel) str
            get_b_layout()
            get_kernel_extra_args() str
            get_kernel_extra_args_declare() str
          }
          class CppMicroGemmConfig {
            compute_dtype
            extra_check : Optional[Callable[..., bool]]
            input2_dtype
            input_dtype
            output_dtype
            register_blocking : GemmBlocking
            vec_isa_cls : Type[VecISA]
          }
          class CppMicroGemmFP32Vec {
            TEMPLATE_ENTRY : str
            TEMPLATE_KERNEL : str
            codegen_define(kernel: CppTemplateKernel) str
          }
          class CppMicroGemmRef {
            TEMPLATE_ENTRY : str
            codegen_define(kernel: CppTemplateKernel) str
          }
          class CppOptions {
          }
          class CppOuterLoopFusedCount {
            inner_kernel_number : int
            local_buffer_number : int
          }
          class CppOverrides {
            abs(x)
            acos(x)
            acosh(x)
            add(a, b)
            asin(x)
            asinh(x)
            atan(x)
            atan2(x, y)
            atanh(x)
            bitwise_and(a, b)
            bitwise_left_shift(a, b)
            bitwise_not(a)
            bitwise_or(a, b)
            bitwise_right_shift(a, b)
            bitwise_xor(a, b)
            ceil(x)
            constant(val, dtype)
            copysign(x, y)
            cos(x)
            cosh(x)
            erf(x)
            erfc(x)
            erfinv(x)
            exp(x)
            exp2(x)
            expm1(x)
            floor(x)
            floordiv(a, b)
            fmod(a, b)
            frexp(x)
            hypot(x, y)
            index_expr(expr, dtype)
            isinf(x)
            isnan(x)
            lgamma(x)
            log(x)
            log10(x)
            log1p(x)
            log2(x)
            logical_and(a, b)
            logical_not(a)
            logical_or(a, b)
            logical_xor(a, b)
            masked(mask, body, other)
            maximum(a, b)
            minimum(a, b)
            mod(a, b)
            mul(a, b)
            neg(x)
            nextafter(x, y)
            pow(a, b)
            rand(seed: sympy.Expr, offset: sympy.Expr)
            randint64(seed: sympy.Expr, offset: sympy.Expr, low, high)
            randn(seed: sympy.Expr, offset: sympy.Expr)
            relu(x)
            round(x)
            rsqrt(x)
            sigmoid(x)
            sign(x)
            signbit(x)
            sin(x)
            sinh(x)
            sqrt(x)
            sub(a, b)
            tan(x)
            tanh(x)
            to_dtype(x, dtype, src_dtype, use_compute_types)
            to_dtype_bitcast(x, dtype, src_dtype)
            trunc(x)
            truncdiv(a, b)
            where(a, b, c)
          }
          class CppPrinter {
          }
          class CppPrinter {
            doprint(expr)
          }
          class CppPythonBindingsCodeCache {
            cache : Dict[str, Callable[[], Union[CDLL, ModuleType]]]
            cache_clear : staticmethod
            call_entry_function : str
            cpp_compile_command_flags : dict
            entry_function : str
            extra_parse_arg : str
            suffix_template
            load_pybinding() Any
            load_pybinding_async(argtypes: List[str], source_code: str, device_type: str, num_outputs: int, submit_fn: Any, extra_flags: Sequence[str]) Any
          }
          class CppScheduling {
            MAX_FUSED_KERNEL_ARGS_NUM : int
            backend_features : dict
            kernel_group : Union[CppWrapperKernelGroup, KernelGroup]
            scheduler
            can_fuse_horizontal(node1, node2)
            can_fuse_vertical(node1, node2)
            can_fuse_vertical_outer_loop(node1, node2)
            codegen_node(node: Union[OuterLoopFusedSchedulerNode, FusedSchedulerNode, SchedulerNode])
            codegen_outer_loop_node(node: OuterLoopFusedSchedulerNode)
            codegen_sync()*
            codegen_template(template_node: BaseSchedulerNode, epilogue_nodes: Sequence[BaseSchedulerNode], prologue_nodes: Sequence[BaseSchedulerNode])
            define_kernel(src_code, nodes, kernel_args)
            flush()
            fuse(node1, node2)
            get_backend_features(device: torch.device)
            get_fusion_pair_priority(node1, node2)
            group_fn(sizes)
            is_cpp_template(node: BaseSchedulerNode) bool
            ready_to_flush()
            reset_kernel_group()
            try_loop_split(nodes: List[SchedulerNode])
          }
          class CppTemplate {
            epilogue_creator : Optional[Callable[[ir.Buffer], ir.Pointwise]]
            index_counter : count
            input_nodes
            layout
            num_threads : int
            output_node
            generate()
            header() IndentedBuffer
            render()* str
          }
          class CppTemplateBuffer {
            choice
            template
          }
          class CppTemplateCaller {
            bmreq
            category : str
            info_kwargs : Optional[Dict[str, Union[ir.PrimitiveInfoType, List[ir.PrimitiveInfoType]]]]
            make_kernel_render : Callable[[ir.CppTemplateBuffer, bool, Optional[List[ir.IRNode]]], str]
            template : str
            benchmark() float
            hash_key() str
            info_dict() Dict[str, Union[ir.PrimitiveInfoType, List[ir.PrimitiveInfoType]]]
            output_node() ir.TensorBox
            precompile() None
          }
          class CppTemplateKernel {
            args
            kernel_name
            local_buffers : dict
            render_hooks : dict
            acc_dtype(node: ir.Buffer) str
            call_kernel(name: str, node: ir.CppTemplateBuffer)
            def_kernel(inputs: Dict[str, ir.Buffer], outputs: Dict[str, ir.Buffer], aliases: Optional[Dict[str, str]], function_name: str, extra_sizevars: Optional[List[sympy.Expr]], placeholder: str) str
            define_buffer(name, sizes: List[Any], dtype) str
            dtype(node: ir.Buffer) str
            index(node: ir.Buffer, indices: List[Any]) str
            maybe_codegen_profile() str
            permute(node, dims)
            reinit_buffer_if_null(name)
            release_buffer(name)
            render(template)
            select(node, dim: int, idx: int) ir.ReinterpretView
            size(node: ir.Buffer, dim: int) str
            slice_nd(node, ranges: List[Tuple[Any, Any]]) ir.ReinterpretView
            store_output(dst: ir.Buffer, src: ir.Buffer, orig_src: Optional[ir.Buffer], epilogue_nodes: Optional[List[ir.IRNode]], offsets: Optional[List[Any]], reindexers: Optional[List[Optional[Callable[[List[Any]], List[Any]]]]])
            store_pointwise_nodes(dst: ir.Buffer, nodes: List[ir.IRNode], offsets: Optional[List[sympy.Expr]], reindexers: Optional[List[Optional[Callable[[List[Any]], List[Any]]]]]) str
            stride(node: ir.Buffer, dim: int) str
            unroll_pragma(unroll)
            view(node, sizes: List[Any]) ir.View
          }
          class CppTile2DKernel {
            inner_is_tiling_idx : bool
            inner_num_elems
            inner_tail_size : NoneType
            num_elems
            outer_idx
            outer_num_elems
            outer_tail_size : NoneType
            overrides
            tail_size : NoneType
            tiling_idx
            tiling_indices
            codegen_inner_loops(code)
            gen_transposed_tile_load_store(name, var, index, is_store)
            inner_itervar()
            load(name: str, index: sympy.Expr)
            need_vec_transpose(index)
            set_ranges(group, reduction_group)
            store(name, index, value, mode)
            transform_indexing(index: sympy.Expr) sympy.Expr
          }
          class CppTile2DOverrides {
            index_expr(expr, dtype)
          }
          class CppTorchDeviceOptions {
          }
          class CppTorchOptions {
          }
          class CppVecKernel {
            is_reduction : bool
            num_elems
            overrides
            reduction_var_names
            tail_size : NoneType
            tiling_factor
            tiling_idx
            vec_isa
            weight_recp_vec_range
            weight_recps_val
            arange(index: CppCSEVariable, stride: sympy.Symbol) CppCSEVariable
            broadcast(scalar_var: CppCSEVariable) CppCSEVariable
            get_to_dtype_expr(src, dtype, src_dtype)
            indirect_assert(var, lower, upper, mask)
            load(name: str, index: sympy.Expr)
            reduction(dtype, src_dtype, reduction_type, value)
            reduction_acc_type_vec(reduction_type, dtype)
            reduction_combine_vec(reduction_type, var, next_value, use_weight_recps, index: Optional[sympy.Symbol], horizontal_reduction: Optional[bool], src_dtype: Optional[torch.dtype])
            reduction_init_vec(reduction_type, dtype)
            store(name, index, value, mode)
            store_reduction(name, index, value)
            welford_weight_reciprocal_vec(dtype, num_threads)
          }
          class CppVecOverrides {
            abs(x)
            acos(x)
            acosh(x)
            add(a, b)
            and_(x, y)
            asin(x)
            asinh(x)
            atan(x)
            atan2(a, b)
            atanh(x)
            bitwise_and(a, b)
            bitwise_left_shift(a, b)
            bitwise_not(a)
            bitwise_or(a, b)
            bitwise_right_shift(a, b)
            bitwise_xor(a, b)
            ceil(x)
            copysign(a, b)
            cos(x)
            cosh(x)
            eq(x, y)
            erf(x)
            erfc(x)
            erfinv(x)
            exp(x)
            exp2(x)
            expm1(x)
            floor(x)
            floordiv(a, b)
            fmod(a, b)
            frexp(x)
            ge(x, y)
            gt(x, y)
            hypot(a, b)
            index_expr(expr, dtype)
            le(x, y)
            lgamma(x)
            load_seed(name, offset)
            log(x)
            log10(x)
            log1p(x)
            log2(x)
            logical_and(a, b)
            logical_not(a)
            logical_or(a, b)
            logical_xor(a, b)
            lt(x, y)
            masked(mask, body, other)
            maximum(a, b)
            minimum(a, b)
            mul(a, b)
            ne(x, y)
            neg(x)
            nextafter(x, y)
            pow(a, b)
            rand(seed, offset)
            randint64(seed, offset, low, high)
            randn(seed, offset)
            reciprocal(a)
            relu(x)
            remainder(a, b)
            round(x)
            rsqrt(x)
            scalarize(scalar_func)
            sigmoid(x)
            sign(x)
            sin(x)
            sinh(x)
            sqrt(x)
            square(a)
            sub(a, b)
            tan(a)
            tanh(a)
            to_dtype(x, dtype, src_dtype, use_compute_dtypes)
            truediv(a, b)
            trunc(x)
            truncdiv(a, b)
            where(a, b, c)
          }
          class CppWrapperCodeCache {
            cache : Dict[str, Callable[[], Union[CDLL, ModuleType]]]
            cache_clear : staticmethod
            call_entry_function : str
            cpp_compile_command_flags : dict
            entry_function : str
            extra_parse_arg
          }
          class CppWrapperCodegenError {
          }
          class CppWrapperCpu {
            arg_var_id : count
            cached_output_id : count
            comment : str
            custom_op_wrapper_loaded : bool
            declare : str
            declare_maybe_reference : str
            declared_int_array_vars
            device : str
            device_codegen
            ending : str
            initialized_kernels : Dict[str, Kernel]
            int_array_id : count
            kernel_callsite_id : count
            none_str : str
            output_is_tensor : dict
            prefix
            scalar_to_tensor_id : count
            supports_intermediate_hooks : bool
            tmp_tensor_id : count
            used_cached_devices
            used_cached_dtypes
            used_cached_layouts
            used_cached_memory_formats
            used_cond_predicate
            var_array_id : count
            add_benchmark_harness(output)
            c_type_for_prim_type(val, type_) str
            codegen_alloc_from_pool(name, offset, dtype, shape, stride) str
            codegen_conditional(conditional)
            codegen_const_run_driver()
            codegen_cpp_sizevar(x: Expr) str
            codegen_device(device)
            codegen_device_copy(src, dst, non_blocking: bool)
            codegen_dtype(dtype)
            codegen_dynamic_scalar(node)
            codegen_exact_buffer_reuse(old_name: str, new_name: str, del_line: str)
            codegen_input_size_var_decl(code: IndentedBuffer, name)
            codegen_input_stride_var_decl(code: IndentedBuffer, name)
            codegen_input_symbol_assignment(name: str, value: ir.TensorBox, bound_vars: OrderedSet[sympy.Symbol])
            codegen_int_array_var(int_array: str, writeline: Callable[..., None], known_statically, graph)
            codegen_invoke_subgraph(invoke_subgraph)*
            codegen_layout(layout)
            codegen_memory_format(memory_format)
            codegen_model_constructor()
            codegen_model_kernels()
            codegen_multi_output(name, value)*
            codegen_reinterpret_view(data, size, stride, offset, writeline: Callable[..., None], dtype) str
            codegen_scalar_to_tensor(output: str)
            codegen_shape_tuple(shape: Sequence[Expr]) str
            codegen_sizevar(x: Expr) str
            codegen_subgraph(subgraph, outer_inputs, outer_outputs)
            codegen_subgraph_prefix(subgraph, outer_inputs, outer_outputs)
            codegen_subgraph_suffix(subgraph, outer_inputs, outer_outputs)
            codegen_tensor_dtype_var_decl(code: IndentedBuffer, name)
            codegen_tensor_item(dtype: torch.dtype, tensor: str, scalar: str, indented_buffer)
            codegen_tuple_access(basename: str, name: str, index: str) str
            codegen_while_loop(while_loop)
            create(is_subgraph: bool, subgraph_name: str, parent_wrapper: PythonWrapperCodegen)
            create_tmp_raii_handle_var(base_handle)
            define_kernel(kernel_name: str, kernel_body: str, metadata: Optional[str], gpu)
            ensure_size_computed(sym: sympy.Symbol)
            finalize_prefix()
            generate(is_inference)
            generate_before_suffix(result)
            generate_c_shim_extern_kernel_alloc(extern_kernel, args)
            generate_c_shim_extern_kernel_call(kernel, args)
            generate_c_shim_fallback_kernel(fallback_kernel, args)
            generate_end(result)
            generate_end_graph()*
            generate_extern_kernel_alloc(extern_kernel, args)
            generate_extern_kernel_args_decl_if_needed(op_overload, raw_args, output_args: Optional[List[str]], raw_outputs: Optional[List[ir.Buffer]])
            generate_extern_kernel_out(kernel: str, out: str, out_view: Optional[str], args: List[str])
            generate_fallback_kernel(fallback_kernel, args)
            generate_fallback_kernel_with_runtime_lookup(buf_name: str, python_kernel_name: str, cpp_kernel_name: str, codegen_args: List[str], op_overload: Optional[torch._ops.OpOverload], raw_args, outputs)
            generate_fallback_kernel_with_runtime_lookup_aot(op_overload, raw_args, output_args: Optional[List[str]], raw_outputs: Optional[List[ir.Buffer]])
            generate_fallback_kernel_with_runtime_lookup_jit(buf_name: str, python_kernel_name: str, cpp_kernel_name: str, codegen_args: List[str], op_overload: Optional[torch._ops.OpOverload], raw_args, output_args: Optional[List[str]], raw_outputs: Optional[List[ir.Buffer]])
            generate_float_value(val)
            generate_index_put_fallback(kernel, x, indices, values, accumulate)
            generate_inf_and_nan_checker(nodes)
            generate_input_output_runtime_checks()
            generate_kernel_call(kernel_name: str, call_args, grid, device_index, gpu, triton, arg_types, raw_args, grid_fn: str, triton_meta, autotune_configs, grid_extra_kwargs)
            generate_numel_expr(kernel_name: str, tree, suffix: Optional[str])
            generate_profiler_mark_wrapper_call(stack)
            generate_py_arg(py_args_var, idx, raw_arg, arg_type)
            generate_reset_kernel_saved_flags()*
            generate_return(output_refs: List[str])
            generate_save_uncompiled_kernels()*
            generate_scatter_fallback(output, inputs, cpp_kernel_name, python_kernel_name, src_is_tensor, reduce, kwargs)
            generate_scoped_gil_acquire(declarations_before_scope, lines_in_scope)
            generate_start_graph()*
            get_c_shim_func_name(kernel)
            get_output_refs()
            include_extra_header(header: str)
            load_custom_op_wrapper()
            make_allocation(name, device, dtype, shape, stride)
            make_buffer_allocation(buffer)
            make_buffer_free(buffer)
            make_free_by_names(names_to_del: List[str])
            mark_output_type()
            prepare_triton_kernel_call(device_index, call_args)
            val_to_arg_str(val, type_) str
            val_to_arg_str_for_prim_type(val, type_) str
            write_constant(name, hashed)
            write_header()
            write_input_output_info(info_kind: str, idx: int, name: str)
            write_prefix()
            write_wrapper_decl()
          }
          class CppWrapperCpuArrayRef {
            allow_stack_allocation : Optional[bool]
            arg_var_id : count
            cached_output_id : count
            custom_op_wrapper_loaded : bool
            device : str
            int_array_id : count
            kernel_callsite_id : count
            lines
            scalar_to_tensor_id : count
            stack_allocated_buffers : Dict[BufferName, BufferLike]
            supports_intermediate_hooks : bool
            tmp_tensor_id : count
            var_array_id : count
            can_stack_allocate_buffer(buffer)
            codegen_device_copy(src, dst, non_blocking: bool)
            codegen_input_numel_asserts()
            codegen_reinterpret_view(data, size, stride, offset, writeline: Callable[..., None], dtype) str
            codegen_tensor_item(dtype: torch.dtype, tensor: str, scalar: str, indented_buffer)
            create(is_subgraph: bool, subgraph_name: str, parent_wrapper: PythonWrapperCodegen)
            create_tmp_raii_handle_var(base_handle)
            generate_c_shim_extern_kernel_call(kernel, args)
            generate_fallback_kernel_with_runtime_lookup(buf_name: str, python_kernel_name: str, cpp_kernel_name: str, codegen_args: List[str], op_overload: Optional[torch._ops.OpOverload], raw_args, outputs)
            generate_index_put_fallback(kernel, x, indices, values, accumulate)
            generate_kernel_call(kernel_name: str, call_args, grid, device_index, gpu, triton, arg_types, raw_args, grid_fn: str, triton_meta, autotune_configs, grid_extra_kwargs)
            generate_return(output_refs: List[str])
            generate_scatter_fallback(output, inputs, cpp_kernel_name, python_kernel_name, src_is_tensor, reduce, kwargs)
            get_input_cpp_type(input)
            is_safe_to_use_borrow_arrayref_tensor_as_tensor()
            make_allocation(name, device, dtype, shape, stride, buffer_if_can_stack_allocate)
            make_buffer_allocation(buffer)
            make_buffer_free(buffer)
            make_buffer_reuse(old: BufferLike, new: BufferLike, delete_old: bool)
            memory_plan()
            memory_plan_reuse()
            val_to_arg_str(val, type_) str
            write_header()
            write_wrapper_decl()
          }
          class CppWrapperGpu {
            device : str
            device_codegen
            grid_id : count
            codegen_inputs()
            create(is_subgraph: bool, subgraph_name: str, parent_wrapper: PythonWrapperCodegen)
            define_kernel(kernel_name: str, kernel_body: str, metadata: Optional[str], gpu)
            generate(is_inference)
            generate_args_decl(call_args, arg_types, arg_signatures)
            generate_default_grid(kernel_name: str, grid_args: List[Any], gpu: bool, grid_callable: Optional[Callable[..., Any]])
            generate_kernel_call(kernel_name: str, call_args, grid, device_index, gpu, triton, arg_types, raw_args, grid_fn: str, triton_meta, autotune_configs, grid_extra_kwargs)
            generate_load_kernel_once(kernel_name: str, graph: 'GraphLowering')
            generate_tma_descriptor(desc)
            generate_user_defined_triton_kernel(kernel_name: str, raw_args: List[Any], grid: List[Any], configs, triton_meta, constexprs)
            make_zero_buffer(name)
            write_get_raw_stream(device_idx: int, graph) str
            write_header()
            write_tma_descriptor_helpers_once()
          }
          class CppWrapperKernelArgs {
            wrap_size_arg(size)
          }
          class CppWrapperKernelGroup {
            args
          }
          class CpuDeviceOpOverrides {
            device_guard(device_idx)
            import_get_raw_stream_as(name)
            set_device(device_idx)
            synchronize()
          }
          class CpuDeviceProperties {
            multi_processor_count : int
          }
          class CpuInterface {
            current_device()
            get_compute_capability(device: _device_t) str
            get_raw_stream(device_idx) int
            is_available() bool
            synchronize(device: _device_t)*
          }
          class CreateTMADescriptorVariable {
            rank : int
            call_function(tx: 'InstructionTranslator', args: 'List[VariableTracker]', kwargs: 'Dict[str, VariableTracker]') 'VariableTracker'
          }
          class CriterionTest {
            check_batched_grad
            check_bfloat16
            check_complex
            check_forward_only
            check_gradgrad
            check_half
            constructor_args
            default_dtype
            extra_args
            should_test_cuda
            test_cpu
            tf32_precision
            with_tf32
            test_cuda(test_case, dtype, extra_args)
          }
          class CrossEntropyLoss {
            ignore_index : int
            label_smoothing : float
            forward(input: Tensor, target: Tensor) Tensor
          }
          class CrossMapLRN2d {
            backward(ctx, grad_output)
            forward(ctx, input, size, alpha, beta, k)
          }
          class CrossMapLRN2d {
            alpha : float
            beta : float
            k : float
            size : int
            extra_repr() str
            forward(input: Tensor) Tensor
          }
          class CrossRefFakeMode {
            check_aliasing : bool
            check_strides : bool
            ignore_op_fn : NoneType
            only_check_ops_with_meta : bool
          }
          class CrossRefMode {
          }
          class CtxCustomSave {
            save_for_backward()
            save_for_forward()
          }
          class CtxWithSavedTensors {
            saved_tensors
          }
          class CuBLASConfigGuard {
            cublas_config_restore : NoneType
            cublas_var_name : str
            is_cuda10_2_or_higher
          }
          class CubeGenVmap {
            generate_vmap_rule : bool
            backward(ctx, grad_output, grad_saved)
            forward(x)
            jvp(ctx, input_tangent)
            setup_context(ctx, inputs, outputs)
          }
          class CubicSL {
            delta_t : list
            init_sl : list
            init_t : list
            initially_zero : list
            sparsifier
            total_t : list
            get_sl()
            sparsity_compute_fn(s_0, s_f, t, t_0, dt, n, initially_zero)
          }
          class CudaDdpComparisonTest {
            test_ddp_dist_autograd_local_vs_remote_gpu()
          }
          class CudaDistAutogradTest {
            test_gpu_simple()
            test_gpu_to_cpu_continuation()
            test_gpu_to_cpu_continuation_gpu_root()
          }
          class CudaError {
          }
          class CudaGraphsSupport {
            is_node_supported(submodules, node: torch.fx.Node) bool
          }
          class CudaInterface {
            Event
            Stream
            current_device : staticmethod
            current_stream : staticmethod
            device
            device_count : staticmethod
            exchange_device : staticmethod
            get_device_properties : staticmethod
            get_raw_stream : staticmethod
            is_bf16_supported : staticmethod
            maybe_exchange_device : staticmethod
            memory_allocated : staticmethod
            set_device : staticmethod
            set_stream : staticmethod
            stream : staticmethod
            synchronize : staticmethod
            get_compute_capability(device: _device_t)
            is_available() bool
          }
          class CudaKernelParamCache {
            cache : Dict[str, Dict[str, str]]
            cache_clear : staticmethod
            get(key: str) Optional[Dict[str, str]]
            get_keys() KeysView[str]
            set(key: str, params: Dict[str, str], cubin: str, bin_type: str) None
          }
          class CudaMemoryLeakCheck {
            caching_allocator_befores : list
            driver_befores : list
            name : NoneType
            testcase
          }
          class CudaNonDefaultStream {
            beforeStreams : list
          }
          class CudaRemoteModuleTest {
            test_input_moved_to_cuda_device()
            test_input_moved_to_cuda_device_script()
            test_invalid_devices()
            test_valid_device()
          }
          class CudaRpcTest {
            test_profiler_remote_cuda()
          }
          class CudaSyncGuard {
            debug_mode_restore
            mode
          }
          class CudagraphCachedInfo {
            cudagraph_fail_reasons : List[str]
            placeholders : Sequence[PlaceholderInfo]
            stack_traces : List[Optional[str]]
          }
          class CudagraphsBackend {
            compiler_name : str
            reset()
          }
          class CudnnModule {
            allow_tf32
            benchmark
            benchmark_limit : NoneType
            deterministic
            enabled
          }
          class CumulativeDistributionTransform {
            bijective : bool
            codomain
            distribution
            domain
            sign : int
            log_abs_det_jacobian(x, y)
            with_cache(cache_size)
          }
          class CurrentState {
            name
          }
          class CustomDecompTable {
            decomp_table : dict
            deleted_custom_ops : set
            has_materialized : bool
            copy() 'CustomDecompTable'
            items()
            keys()
            materialize() Dict[torch._ops.OperatorBase, Callable]
            pop()
            update(other_dict)
          }
          class CustomDrawer {
          }
          class CustomException {
            bool
          }
          class CustomFromMask {
            PRUNING_TYPE : str
            mask
            apply(module, name, mask)
            compute_mask(t, default_mask)
          }
          class CustomFunctionHigherOrderOperator {
          }
          class CustomGraphPass {
            uuid()* Optional[Any]
          }
          class CustomModuleQuantizeHandler {
          }
          class CustomObjArgument {
            class_fqn : Annotated[str, 20]
            name : Annotated[str, 10]
          }
          class CustomObjArgument {
            class_fqn : str
            fake_val : Optional[FakeScriptObject]
            name : str
          }
          class CustomOp {
            impl(device_types: typing.Union[str, typing.Iterable[str]], _stacklevel) typing.Callable
            impl_abstract(_stacklevel) typing.Callable
            impl_backward(output_differentiability, _stacklevel)
            impl_factory() typing.Callable
            impl_save_for_backward(_stacklevel)
          }
          class CustomOpDef {
            register_autograd() None
            register_fake() Callable
            register_kernel() Callable
            register_torch_dispatch() Callable
            register_vmap(func: Optional[Callable])
            set_kernel_enabled(device_type: str, enabled: bool)
          }
          class CustomPolicy {
          }
          class CustomizedDictVariable {
            call_hasattr
            as_proxy()*
            call_method(tx, name, args: 'List[VariableTracker]', kwargs: 'Dict[str, VariableTracker]') 'VariableTracker'
            create(user_cls, args, kwargs, options)
            is_matching_cls(cls)
            is_matching_cls_hf(cls)
            is_matching_object(obj)
            reconstruct(codegen)
            var_getattr(tx: 'InstructionTranslator', name: str) 'VariableTracker'
            wrap(builder, obj)
          }
          class CutlassEVTEpilogueArgumentFormatter {
            accumulator_node_name : str
            aliases : Dict[str, str]
            output
            var_counter : int
            getvalue(result) str
            ir_to_evt_argument_string(template_output_node_name: str, epilogue_nodes: List[IRNode]) str
            reduction(dtype, src_dtype, reduction_type, value)
          }
          class CutlassEVTEpilogueTypeFormatter {
            accumulator_node_name
            aliases : dict
            evt_type_name
            output
            var_counter : int
            getvalue(result) str
            ir_to_evt_string(template_output_node_name: str, evt_type_name: str, epilogue_nodes: List[IRNode])
            reduction(dtype, src_dtype, reduction_type, value)
          }
          class CycleIteratorVariable {
            item : Optional[VariableTracker]
            iterator : NoneType
            saved : Optional[List[VariableTracker]]
            saved_index : int
            next_variable(tx)
          }
          class CyclicLR {
            base_lrs : list
            base_momentums : list
            cycle_momentum : bool
            gamma : float
            max_lrs : list
            max_momentums : list
            mode : Literal['triangular', 'triangular2', 'exp_range']
            optimizer
            scale_mode : Literal['cycle', 'iterations'], str
            step_ratio
            total_size
            use_beta1
            get_lr()
            load_state_dict(state_dict)
            scale_fn(x) float
            state_dict()
          }
          class CyclingIterator {
          }
          class DAG {
            nodes : List[DAGNode]
            create_node(submodule_node: Node, input_nodes: List[Node], output_nodes: List[Node], logical_devices: List[int], size_bytes: int) None
          }
          class DAGNode {
            input_nodes : List[Node]
            logical_device_ids : List[int]
            output_nodes : List[Node]
            size_bytes : int
            submodule_node
          }
          class DDP {
            register_comm_hook() None
            set_requires_gradient_sync(requires_gradient_sync: bool) None
          }
          class DDPCommHookType {
            name
          }
          class DDPMeshInfo {
            replicate_mesh_rank : int
            replicate_mesh_size : int
            replicate_process_group
          }
          class DDPOptimizer {
            backend_compile_fn
            bucket_bytes_cap : int
            buckets : list
            first_bucket_cap
            add_module_params_to_bucket(mod, bucket, processed_modules, prefix)
            add_param(bucket, param, name)
            add_param_args(bucket, node)
            compile_fn(gm: fx.GraphModule, example_inputs: List[torch.Tensor])
          }
          class DDPUnevenTestInput {
            hook : Optional[Callable]
            inp : Union[torch.tensor, tuple]
            model
            name : str
            state : Optional[Any]
            sync_interval : int
            throw_on_early_termination : bool
          }
          class DEVICEInitMode {
            name
          }
          class DFIterDataPipe {
          }
          class DGreatestUpperBound {
            res
            rhs1
            rhs2
          }
          class DLDeviceType {
            name
          }
          class DLLWrapper {
            DLL
            is_open : bool
            lib_path : str
            close() None
          }
          class DQuantType {
            name
          }
          class DTensor {
            device_mesh
            placements
            from_local(local_tensor: torch.Tensor, device_mesh: Optional[DeviceMesh], placements: Optional[Sequence[Placement]]) 'DTensor'
            full_tensor() torch.Tensor
            redistribute(device_mesh: Optional[DeviceMesh], placements: Optional[Sequence[Placement]]) 'DTensor'
            to_local() torch.Tensor
          }
          class DTensorConverter {
            args : Tuple[object, ...]
            flatten_args : List[object]
            flatten_args_spec
            flatten_kwargs : List[object]
            flatten_kwargs_spec
            hit : int
            kwargs : Dict[str, object]
            mesh
            miss : int
            sharding_combs : Iterator[Sequence[Placement]]
            gen_sharding_choices_for_arg(arg: torch.Tensor) Sequence[Placement]
            is_supported_tensor(t: torch.Tensor) bool
            successful() bool
            to_dist_tensor(t: torch.Tensor, mesh: DeviceMesh, placements: List[Placement]) torch.Tensor
          }
          class DTensorExtensions {
            compute_stream : NoneType
            device_handle
            post_unflatten_transform : NoneType
            all_gather_dtensor(tensor: DTensor, parent_mesh: Optional[DeviceMesh]) torch.Tensor
            chunk_dtensor(tensor: torch.Tensor, rank: int, device_mesh: DeviceMesh) torch.Tensor
            chunk_tensor(tensor: torch.Tensor, rank: int, world_size: int, num_devices_per_node: int, pg: dist.ProcessGroup, device: Optional[torch.device]) torch.Tensor
            post_unflatten_transform(tensor: torch.Tensor, param_extension: Any) torch.Tensor
            pre_flatten_transform(tensor: torch.Tensor) Tuple[torch.Tensor, Optional[Any]]
            pre_load_state_dict_transform(tensor: torch.Tensor) Tuple[torch.Tensor, List[Shard]]
          }
          class DTensorOpTestBase {
            device_type
            world_size
            build_device_mesh()
            setUp() None
          }
          class DTensorSpec {
            device_mesh
            dim_map
            mesh
            ndim
            num_shards
            num_shards_map
            placements : Tuple[Placement, ...]
            shape
            stride
            sums
            tensor_meta : Optional[TensorMeta]
            from_dim_map(mesh: DeviceMesh, dim_map: List[int], sums: List[int], tensor_meta: Optional[TensorMeta]) 'DTensorSpec'
            is_replicated() bool
            is_sharded() bool
            shallow_copy_with_tensor_meta(tensor_meta: Optional[TensorMeta]) 'DTensorSpec'
          }
          class DTensorTestBase {
            backend
            world_size
            build_device_mesh() DeviceMesh
            destroy_pg() None
            init_pg(eager_init) None
            run_subtests()
            setUp() None
          }
          class DType {
            itemsize
            kind
            name
            torch_dtype
            type
            typecode
          }
          class DTypeConfig {
            bias_dtype : Optional[torch.dtype]
            input_dtype
            input_dtype_with_constraints
            is_dynamic : Optional[bool]
            output_dtype
            output_dtype_with_constraints
            weight_dtype
            weight_dtype_with_constraints
            from_dict(dtype_config_dict: Dict[str, Any]) DTypeConfig
            to_dict() Dict[str, Any]
          }
          class DTypeVar {
            dtype
          }
          class DTypeWithConstraints {
            dtype : Optional[torch.dtype]
            quant_max_upper_bound : Optional[Union[int, float, None]]
            quant_min_lower_bound : Optional[Union[int, float, None]]
            scale_exact_match : Optional[float]
            scale_max_upper_bound : Optional[Union[int, float, None]]
            scale_min_lower_bound : Optional[Union[int, float, None]]
            zero_point_exact_match : Optional[int]
          }
          class DVar {
            c
          }
          class DataChunk {
            items : Iterable[_T]
            as_str(indent: str) str
            raw_iterator() Iterator[_T]
          }
          class DataChunkDF {
          }
          class DataDependentOutputException {
            func
          }
          class DataFlowEdge {
            input_version : Optional[int]
            is_allocation
            is_deletion
            mutated : Optional[bool]
          }
          class DataFlowGraph {
            flow_nodes
            leaf_events
            bump(key: TensorKey) None
            delete(key: TensorKey) None
            lookup(key: TensorKey) int
            validate()
          }
          class DataFlowNode {
            inputs
            intermediates
            outputs
            start_time
          }
          class DataFrameTracedOps {
            output_var
            source_datapipe
          }
          class DataFrameTracer {
            source_datapipe : Optional[Any]
            is_shardable()
            set_shuffle_settings()*
          }
          class DataFramesAsTuplesPipe {
            source_datapipe
          }
          class DataLoader {
            batch_sampler : Optional[Union[Sampler[List], Iterable[List], None]]
            batch_size : Optional[int]
            collate_fn : Optional[_collate_fn_t]
            dataset : Dataset[_T_co]
            drop_last : bool
            generator : NoneType
            in_order : bool
            multiprocessing_context
            num_workers : int
            persistent_workers : bool
            pin_memory : bool
            pin_memory_device : str
            prefetch_factor : Optional[int]
            sampler : Union[Sampler, Iterable]
            timeout : float
            worker_init_fn : Optional[_worker_init_fn_t]
            check_worker_number_rationality()
          }
          class DataNormSparsifier {
            norm : str
            update_mask(name, data, sparsity_level, sparse_block_shape, zeros_per_block)
          }
          class DataParallel {
            device_ids : list
            dim : int
            module : T
            output_device : NoneType, int
            src_device_obj
            forward() Any
            gather(outputs: Any, output_device: Union[int, torch.device]) Any
            parallel_apply(replicas: Sequence[T], inputs: Sequence[Any], kwargs: Any) List[Any]
            replicate(module: T, device_ids: Sequence[Union[int, torch.device]]) List[T]
            scatter(inputs: Tuple[Any, ...], kwargs: Optional[Dict[str, Any]], device_ids: Sequence[Union[int, torch.device]]) Any
          }
          class DataParallelMeshInfo {
            mesh
            replicate_mesh_dim : Optional[int]
            shard_mesh_dim : Optional[int]
          }
          class DataProcessorChoiceCallerWrapper {
            benchmark() float
            output_node() ir.TensorBox
          }
          class DataProcessorTemplateWrapper {
            generate()
            maybe_append_choice(choices)
          }
          class DataPtrVariable {
            from_tensor
            reconstruct(codegen)
          }
          class DataTypePropagation {
            body
            graphs : Dict[Union[Callable[..., Any], str], Any]
            deduce_node_dtype(node: torch.fx.Node)
            deduce_node_dtype_by_inputs(node: torch.fx.Node)
            deduce_node_dtype_by_subgraph(node: torch.fx.Node)
            propagate()
            propagate_graph(graph: torch.fx.Graph)
            propagate_loopbody(body)
            propagate_scheduler_node(node)
          }
          class Dataset {
          }
          class DdpComparisonTest {
            test_ddp_comparison()
            test_ddp_comparison_uneven_inputs()
            test_ddp_dist_autograd_local_vs_remote()
            test_ddp_dist_autograd_sparse_grads()
          }
          class DdpMode {
            name
          }
          class DdpUnderDistAutogradTest {
            world_size
            do_test_on_master(ddp_mode: DdpMode, simulate_uneven_inputs: bool, remote_em_rref: rpc.RRef, remote_net_rref: rpc.RRef)
            remote_worker_name() str
            test_backward_ddp_inside()
            test_backward_ddp_outside()
            test_backward_ddp_outside_uneven_inputs()
            test_backward_no_ddp()
            trainer_name(rank)
          }
          class DeFusedEmbeddingBagLinear {
            bagging_op
            dequant
            emb
            linear
            qconfig
            quant
            forward(input: torch.Tensor) torch.Tensor
          }
          class DeQuantStub {
            qconfig : NoneType
            forward(x)
          }
          class DeQuantize {
            forward(Xq)
            from_float(mod, use_precomputed_fake_quant)
          }
          class DeallocFromPoolLine {
            is_last_pool_usage : bool
            codegen(code: IndentedBuffer)
          }
          class DebugAssertWrapper {
            flat_requires_grad : List[Optional[bool]]
            post_compile(compiled_fn, aot_config: AOTConfig)
          }
          class DebugAutotuner {
            cached : NoneType, tuple
            precompile_time_taken_ns
            regex_filter : str
            with_bandwidth_info : bool
            with_profiler : bool
            run()
          }
          class DebugContext {
            copy(new_path: str) None
            create_debug_dir(folder_name: str) Optional[str]
            filename(suffix: str) str
            fopen(filename: str, write_mode: str) IO[Any]
            fopen_context(filename: str, write_mode: str) Iterator[IO[Any]]
            upload_tar() None
          }
          class DebugDirManager {
            counter : count
            id
            new_name : str
            prev_debug_name : str
          }
          class DebugFormatter {
            filename
            fopen
            fopen_context
            handler
            draw_orig_fx_graph(gm: torch.fx.GraphModule, nodes: SchedulerNodeList) None
            fx_graph(gm: torch.fx.GraphModule, inputs: List[torch.Tensor]) None
            fx_graph_transformed(gm: torch.fx.GraphModule, inputs: List[torch.Tensor]) None
            graph_diagram(nodes: SchedulerNodeList) None
            ir_post_fusion(nodes: SchedulerNodeList) None
            ir_pre_fusion(nodes: SchedulerNodeList) None
            log_autotuning_results(name: str, input_nodes: List[ir.IRNode], timings: Dict['ChoiceCaller', float], elapse: float, precompile_elapse: float) None
            output_code(filename: str) None
          }
          class DebugInterpreter {
            symbol_mapping
            run()
            run_node(n)
          }
          class DebugPrinterManager {
            arg_signatures : Optional[List[type]]
            args_to_print_or_save : Optional[List[str]]
            debug_printer_level : OFF
            filtered_kernel_names_to_print : list
            kernel : NoneType
            kernel_name : str
            kernel_type : NoneType
            codegen_intermediate_tensor_value_print(args_to_print, kernel_name, before_launch, arg_signatures: Optional[List[type]]) None
            codegen_intermediate_tensor_value_save(args_to_save, kernel_name, before_launch, arg_signatures: Optional[List[type]]) None
            codegen_model_inputs_value_print(input_args_to_print: List[str]) None
            set_printer_args(args_to_print_or_save: List[str], kernel_name: str, arg_signatures: Optional[List[type]], kernel, kernel_type)
          }
          class DebuggingVariable {
            value
            call_function(tx: 'InstructionTranslator', args, kwargs)
            can_reorder_logs(fn, args, kwargs) True
            is_reorderable_logging_function(obj)
            reconstruct(codegen)
          }
          class Decoder {
            handlers : list
            key_fn
            add_handler()
            decode(data)
            decode1(key, data)
          }
          class DecompSkip {
            new_op_name : str
            new_op_schema : str
            onnxscript_function : Callable
            op_callable : Callable
            abstract()*
            register(export_options: torch.onnx.ExportOptions)*
            register_custom_op()
            replacement()
            unregister()*
          }
          class Decompose {
            allow_fake_constant : bool | None
            decomposition_table : Mapping[torch._ops.OpOverload, Callable]
            enable_dynamic_axes : bool
          }
          class DecompositionInterpreter {
            decomposition_table : dict
            mode
            new_graph
            tracer
            get_attr(target: str, args: Tuple[object, ...], kwargs: Dict[str, object]) object
            output(target: str, args: Tuple[object, ...], kwargs: Dict[str, object]) object
            placeholder(target: str, args: Tuple[object, ...], kwargs: Dict[str, object]) object
            run() object
          }
          class DecorateInfo {
            active_if : bool
            cls_name : NoneType
            decorators : list
            device_type : NoneType
            dtypes : NoneType
            test_name : NoneType
            is_active(cls_name, test_name, device_type, dtype, param_kwargs)
          }
          class Decorator {
            forward(x, y)
          }
          class DedupList {
            items : list
            membership
            append(node_user: T) None
          }
          class DefaultDeviceType {
            get_device_type() str
            set_device_type(device: str)
          }
          class DefaultDictVariable {
            default_factory : NoneType
            call_method(tx, name, args: 'List[VariableTracker]', kwargs: 'Dict[str, VariableTracker]') 'VariableTracker'
            debug_repr()
            is_python_constant()
            is_supported_arg(arg)
          }
          class DefaultFuseHandler {
            fuse(load_arg: Callable, named_modules: Dict[str, torch.nn.Module], fused_graph: Graph, root_node: Node, extra_inputs: List[Any], matched_node_pattern: NodePattern, fuse_custom_config: FuseCustomConfig, fuser_method_mapping: Dict[Pattern, Union[torch.nn.Sequential, Callable]], is_qat: bool) Node
          }
          class DefaultLoadPlanner {
            allow_partial_load : bool
            flatten_sharded_tensors : bool
            flatten_state_dict : bool
            is_coordinator : bool
            mappings : Dict
            metadata : Optional[Metadata]
            original_state_dict : Dict
            state_dict : Dict, dict
            commit_tensor(read_item: ReadItem, tensor: torch.Tensor)* None
            create_global_plan(global_plan: List[LoadPlan]) List[LoadPlan]
            create_local_plan() LoadPlan
            finish_plan(new_plan: LoadPlan) LoadPlan
            load_bytes(read_item: ReadItem, value: io.BytesIO) None
            lookup_tensor(index: MetadataIndex) torch.Tensor
            resolve_tensor(read_item: ReadItem)
            set_up_planner(state_dict: STATE_DICT_TYPE, metadata: Optional[Metadata], is_coordinator: bool) None
            transform_tensor(read_item: ReadItem, tensor: torch.Tensor)
          }
          class DefaultLogsSpecs {
            root_log_dir
            reify(envs: Dict[int, Dict[str, str]]) LogsDest
          }
          class DefaultNodeQuantizeHandler {
          }
          class DefaultSavePlanner {
            dedup_save_to_lowest_rank : bool
            flatten_sharded_tensors : bool
            flatten_state_dict : bool
            global_plan : list
            is_coordinator : bool
            mappings : Dict
            metadata
            plan
            state_dict : Dict
            create_global_plan(all_plans: List[SavePlan]) Tuple[List[SavePlan], Metadata]
            create_local_plan() SavePlan
            finish_plan(new_plan: SavePlan) SavePlan
            lookup_object(index: MetadataIndex) Any
            resolve_data(write_item: WriteItem) Union[torch.Tensor, io.BytesIO]
            set_up_planner(state_dict: STATE_DICT_TYPE, storage_meta: Optional[StorageMeta], is_coordinator: bool) None
            transform_object(write_item: WriteItem, object: Any)
          }
          class DefaultState {
            gradient_postdivide_factor
            gradient_predivide_factor : float
            process_group
            world_size : int
          }
          class DefaultsSource {
            field : str
            idx_key : Union[int, str]
            is_kw : bool
            guard_source()
            name()
            reconstruct(codegen)
          }
          class DeferredCudaCallError {
          }
          class DeferredGpuDefaultGrid {
            grid
            grid_callable : Optional[Callable[..., Any]]
            grid_extra_kwargs : dict
            kernel_name : str
          }
          class DeferredGpuGridLine {
            autotune_configs
            grid
            grid_var : str
            kernel_name : str
          }
          class DeferredGpuKernelLine {
            additional_files : List[str]
            kernel_name : str
            keys : Tuple[str, ...]
            line_template : str
          }
          class DeferredLine {
            name
          }
          class DeferredLineBase {
            line : str
            lstrip()
            with_prefix(prefix)
          }
          class DeferredMtiaCallError {
          }
          class DelayGraphBreakVariable {
          }
          class DelayReplaceLine {
            key : str
            value_fn : Callable[[], str]
          }
          class DeletedGuardManagerWrapper {
            diff_guard_root : NoneType
            invalidation_reason
            populate_diff_guard_manager()
          }
          class DeletedVariable {
          }
          class DemultiplexerIterDataPipe {
          }
          class DenseTopMLP {
            dense_mlp
            top_mlp
            forward(sparse_feature: torch.Tensor, dense: torch.Tensor) torch.Tensor
          }
          class Dep {
            index : Expr
            name : str
            get_numel()* sympy.Expr
            has_unbacked_symbols()* bool
            is_contiguous()* bool
            normalize_with_stride_order(prefix)
            numbytes_hint()*
            rename(renames: Dict[str, str])* 'Dep'
          }
          class DequeVariable {
            maxlen : NoneType
            as_python_constant()
            call_method(tx, name, args: List['VariableTracker'], kwargs: Dict[str, 'VariableTracker']) 'VariableTracker'
            debug_repr()
            python_type()
            reconstruct(codegen: 'PyCodegen') None
            var_getattr(tx: 'InstructionTranslator', name)
          }
          class DerivedQuantizationSpec {
            ch_axis : Optional[int]
            derive_qparams_fn : Callable[[List[ObserverOrFakeQuantize]], Tuple[Tensor, Tensor]]
            derived_from : List[EdgeOrNode]
            dtype
            is_dynamic : bool
            qscheme : Optional[torch.qscheme]
            quant_max : Optional[int]
            quant_min : Optional[int]
          }
          class DetachExecutor {
            value_remap : dict
            call_function(target, args, kwargs)
            call_module(target, args, kwargs)
            run()
          }
          class DetailedTemplateSyntaxError {
            original_error
          }
          class DetectorBase {
            detector_config_info : NoneType
            determine_observer_insert_points(model)* Dict
            generate_detector_report(model)* Tuple[str, Dict[str, Any]]
            get_detector_name()* str
            get_qconfig_info(model)* Dict[str, DetectorQConfigInfo]
          }
          class DetectorQConfigInfo {
            is_activation_dynamic : bool
            is_equalization_recommended : bool
            is_weight_per_channel : bool
            module_fqn : str
            generate_equalization_qconfig() EqualizationQConfig
            generate_quantization_qconfig(module: torch.nn.Module) QConfig
          }
          class DeterministicAlgorithmsVariable {
            create(tx: 'InstructionTranslator', target_value)
            enter(tx)
            fn_name()
            module_name()
          }
          class DeterministicGuard {
            deterministic
            deterministic_restore
            fill_uninitialized_memory : bool
            fill_uninitialized_memory_restore : bool
            warn_only : bool
            warn_only_restore
          }
          class Device {
            available_mem_bytes : int
            logical_id : int
            name : str
          }
          class Device {
            index : Optional[Annotated[Optional[int], 20]]
            type : Annotated[str, 10]
          }
          class DeviceCodegen {
            cpp_wrapper_codegen : type
            scheduling : Any
            wrapper_codegen : type
          }
          class DeviceContext {
            async_closure_handler
            async_step_closures : list
            closure_handler
            device
            step_closures : list
          }
          class DeviceContext {
            device
            old_device
          }
          class DeviceCopy {
            codegen(wrapper) None
            create(x, device, non_blocking)
          }
          class DeviceGuard {
            device_interface : Type[DeviceInterface]
            idx : Optional[int]
            prev_idx : int
          }
          class DeviceInterface {
            current_device()*
            current_stream()*
            device_count()*
            exchange_device(device: int)* int
            get_compute_capability(device: _device_t)*
            get_device_properties(device: _device_t)
            get_raw_stream(device_idx: int)* int
            is_available()* bool
            is_bf16_supported(including_emulation: bool)*
            maybe_exchange_device(device: int)* int
            memory_allocated(device: _device_t)* int
            set_device(device: _device_t)*
            set_stream(stream: torch.Stream)*
            stream(stream: torch.Stream)*
            synchronize(device: _device_t)*
          }
          class DeviceMesh {
            device_type : str
            mesh
            mesh_dim_names : Optional[Tuple[str, ...]]
            ndim
            shape
            from_group(group: Union[ProcessGroup, List[ProcessGroup]], device_type: str, mesh: Optional[Union[torch.Tensor, 'ArrayLike']]) 'DeviceMesh'
            get_all_groups() List[ProcessGroup]
            get_coordinate() Optional[List[int]]
            get_group(mesh_dim: Optional[Union[int, str]]) ProcessGroup
            get_local_rank(mesh_dim: Optional[Union[int, str]]) int
            get_rank() int
            size(mesh_dim: Optional[int]) int
          }
          class DeviceMeshVariable {
            as_python_constant()
            call_method(tx, name, args: 'List[VariableTracker]', kwargs: 'Dict[str, VariableTracker]') 'VariableTracker'
            is_device_mesh(value)
            var_getattr(tx: 'InstructionTranslator', name: str) VariableTracker
          }
          class DeviceOpOverrides {
            abi_compatible_header()*
            aoti_get_stream()*
            cpp_aoti_device_guard()*
            cpp_aoti_stream_guard()*
            cpp_device_guard()*
            cpp_device_ptr()*
            cpp_getStreamFromExternal()*
            cpp_kernel_type()*
            cpp_stream_guard()*
            cpp_stream_type()*
            device_guard(device_idx)*
            import_get_raw_stream_as(name)*
            kernel_driver()*
            kernel_header()*
            set_device(device_idx)*
            synchronize()*
            tma_descriptor_helpers()*
          }
          class DevicePlacementSpec {
            device
          }
          class DeviceProperties {
            cc : int
            index : int
            major : Optional[int]
            max_threads_per_multi_processor : Optional[int]
            multi_processor_count : int
            regs_per_multiprocessor : Optional[int]
            type : str
            warp_size : Optional[int]
            create(device) DeviceProperties
          }
          class DeviceTypeTestBase {
            device_type : str
            precision
            rel_tol
            get_all_devices()
            get_primary_device()
            instantiate_test(name, test)
            run(result)
          }
          class DiGraph {
            edges
            nodes
            add_edge(u, v)
            add_node(n)
            all_paths(src: str, dst: str)
            backward_transitive_closure(src: str) Set[str]
            first_path(dst: str) List[str]
            forward_transitive_closure(src: str) Set[str]
            predecessors(n)
            successors(n)
            to_dot() str
          }
          class DiagTensorBelow {
            diag
            handled_ops : dict
            get_wrapper_properties(diag, requires_grad)
          }
          class Diagnostic {
            logger : Logger
            log(level: int, message: str) None
          }
          class Diagnostic {
            additional_messages : list[str]
            graphs : list[infra.Graph]
            level
            locations : list[infra.Location]
            logger : Logger
            message : str | None
            rule
            source_exception : Exception | None
            stacks : list[infra.Stack]
            tags : list[infra.Tag]
            thread_flow_locations : list[infra.ThreadFlowLocation]
            debug(message: str) None
            error(message: str) None
            info(message: str) None
            log(level: int, message: str) None
            log_section(level: int, message: str) Generator[None, None, None]
            log_source_exception(level: int, exception: Exception) None
            record_python_call(fn: Callable, state: Mapping[str, str], message: str | None, frames_to_skip: int) infra.ThreadFlowLocation
            record_python_call_stack(frames_to_skip: int) infra.Stack
            sarif() sarif.Result
            warning(message: str) None
            with_graph(graph: infra.Graph) Self
            with_location(location: infra.Location) Self
            with_stack(stack: infra.Stack) Self
            with_thread_flow_location(location: infra.ThreadFlowLocation) Self
          }
          class DiagnosticContext {
            logger : Logger
          }
          class DiagnosticContext {
            diagnostics : list[_Diagnostic]
            logger : Logger
            name : str
            options
            version : str
            add_inflight_diagnostic(diagnostic: _Diagnostic) Generator[_Diagnostic, None, None]
            dump(file_path: str, compress: bool) None
            inflight_diagnostic(rule: infra.Rule | None) _Diagnostic
            log(diagnostic: _Diagnostic) None
            log_and_raise_if_error(diagnostic: _Diagnostic) None
            pop_inflight_diagnostic() _Diagnostic
            push_inflight_diagnostic(diagnostic: _Diagnostic) None
            sarif() sarif.Run
            sarif_log() sarif.SarifLog
            to_json() str
          }
          class DiagnosticOptions {
            verbosity_level : int
            warnings_as_errors : bool
          }
          class DictKeys {
            kv : str
            set_items
            view_items_vt
            call_method(tx, name, args: List['VariableTracker'], kwargs: Dict[str, 'VariableTracker']) 'VariableTracker'
            python_type()
          }
          class DictOutputModule {
            module
            forward(x)
          }
          class DictValues {
            kv : str
            view_items_vt
            python_type()
          }
          class DictView {
            dv_dict
            kv : Optional[str]
            view_items
            view_items_vt
            call_method(tx, name, args: List['VariableTracker'], kwargs: Dict[str, 'VariableTracker']) 'VariableTracker'
            reconstruct(codegen)
            unpack_var_sequence(tx)
          }
          class Dictionary {
            forward(x, y)
          }
          class DimConstraints {
            add(expr: SympyBoolean) bool
            add_equality(source: Source, expr: sympy.Expr) None
            forced_specializations() Dict[str, sympy.Expr]
            prettify_results(original_signature: inspect.Signature, dynamic_shapes: Union[Dict[str, Any], Tuple[Any], List[Any]], constraint_violation_error: object, forced_specializations: Dict[str, str]) str
            rewrite_with_congruences(s: sympy.Symbol, expr: _SympyT) _SympyT
            solve() None
          }
          class DimDynamic {
            name
          }
          class DimOrder {
            name
          }
          class DimSpec {
            inputs() Iterable['DimSpec']
          }
          class DimensionInfo {
            expr : Optional[sympy.Expr]
            size : Expr
            stride : Expr
            index_str(replacements, zero_vars)
          }
          class Directory {
            children : Dict[str, Directory]
            is_dir : bool
            name : str
            has_file(filename: str) bool
          }
          class DirectoryReader {
            directory
            get_all_records()
            get_record(name)
            get_storage_from_record(name, numel, dtype)
            has_record(path)
            serialization_id()
          }
          class Dirichlet {
            arg_constraints : dict
            concentration
            has_rsample : bool
            mean
            mode
            support
            variance
            entropy()
            expand(batch_shape, _instance)
            log_prob(value)
            rsample(sample_shape: _size) torch.Tensor
          }
          class DisableBisect {
          }
          class DisableContext {
          }
          class DisableReduction {
          }
          class DisabledSavedTensorsHooksVariable {
            create(tx: 'InstructionTranslator', target_value)
            enter(tx)
            fn_name()
            module_name()
          }
          class Disj {
            disjuncts
          }
          class DispatchCacheInfo {
            bypasses : Dict[str, int]
            hits : int
            misses : int
            size : int
          }
          class DispatchError {
          }
          class Dispatcher {
            funcs : dict
            name
            ordering : list
            add(signature, func)
            register()
            resolve(args)
          }
          class Dispatcher {
            doc : NoneType
            funcs : dict
            name
            ordering
            add(signature, func)
            dispatch()
            dispatch_iter()
            get_func_annotations(func)
            get_func_params(func)
            help()
            register()
            reorder(on_ambiguity)
            resolve(types)
            source()
          }
          class DistAutogradTest {
            hook_called_times : int
            test_async_dist_autograd()
            test_autograd_context()
            test_backward_accumulate_grads()
            test_backward_autograd_engine_error()
            test_backward_complex_python_udf()
            test_backward_different_dtypes()
            test_backward_different_tensor_dims()
            test_backward_invalid_args()
            test_backward_multiple_output_tensors()
            test_backward_multiple_roots()
            test_backward_multiple_round_trips()
            test_backward_no_grad_on_tensor()
            test_backward_node_failure()
            test_backward_node_failure_python_udf()
            test_backward_python_udf_error()
            test_backward_rref()
            test_backward_rref_multi()
            test_backward_rref_nested()
            test_backward_simple()
            test_backward_simple_python_udf()
            test_backward_simple_script_call()
            test_backward_simple_self()
            test_backward_unused_send_function()
            test_backward_unused_tensors()
            test_backward_verify_hooks()
            test_backward_without_context()
            test_backward_without_rpc()
            test_backwards_nested_python_udf()
            test_clean_context_during_backward()
            test_context_cleanup_nested_rpc()
            test_context_cleanup_no_tensors()
            test_context_cleanup_tensor_no_grad()
            test_context_cleanup_tensor_with_grad()
            test_debug_info()
            test_dist_autograd_profiling()
            test_error_in_context()
            test_grad_copy_sparse_indices_extra_ref()
            test_grad_only_on_return_value()
            test_grad_only_on_return_value_remote()
            test_graph_for_builtin_call()
            test_graph_for_builtin_remote_call()
            test_graph_for_py_nested_call()
            test_graph_for_py_nested_call_itself()
            test_graph_for_py_nested_remote_call()
            test_graph_for_py_nested_remote_call_itself()
            test_graph_for_python_call()
            test_graph_for_python_remote_call()
            test_mixed_requires_grad()
            test_multiple_backward()
            test_multiple_backward_with_errors()
            test_nested_backward_accumulate_grads()
            test_nested_context()
            test_no_grad_copy()
            test_no_grad_copy_sparse()
            test_no_graph_with_tensors_not_require_grad()
            test_no_graph_with_tensors_not_require_grad_remote()
            test_post_hooks()
            test_remote_complex_args()
            test_rpc_complex_args()
            test_thread_local_context_id()
            test_trainer_ps()
            test_trainer_ps_torchscript_functions()
            test_worker_ids_recorded()
          }
          class DistOptimizerTest {
            test_dist_optim()
            test_dist_optim_exception()
            test_dist_optim_exception_on_constructor()
            test_dist_optim_none_grads()
          }
          class DistTestCases {
            backend_feature : dict
            skip_collective : dict
          }
          class DistributedDataParallel {
            broadcast_bucket_size : int
            broadcast_buffers : bool
            bucket_bytes_cap : int
            bucket_bytes_cap_default : bool
            buffer_hook
            device
            device_ids : NoneType
            device_mesh
            device_type
            dim : int
            find_unused_parameters : bool
            gradient_as_bucket_view : bool
            is_multi_device_module
            join_device
            join_process_group
            logger : Optional[dist.Logger]
            mixed_precision : Optional[_MixedPrecision]
            module
            modules_buffers
            named_module_buffers
            output_device : NoneType, int
            parameters_to_ignore : set
            process_group : NoneType
            reducer
            require_backward_grad_sync : bool
            require_forward_param_sync : bool
            static_graph : bool
            use_side_stream_for_tensor_copies
            forward()
            gather(outputs, output_device)
            join(divide_by_initial_world_size: bool, enable: bool, throw_on_early_termination: bool)
            join_hook()
            no_sync()
            register_comm_hook(state: object, hook: Callable)
            scatter(inputs, kwargs, device_ids)
            to_kwargs(inputs, kwargs, device_id)
            train(mode)
            will_sync_module_buffers()
          }
          class DistributedDataParallelCPU {
          }
          class DistributedOptimizer {
            is_functional_optim
            remote_optimizers : list
            step(context_id)
          }
          class DistributedSampler {
            dataset
            drop_last : bool
            epoch : int
            num_replicas : Optional[int]
            num_samples
            rank : Optional[int]
            seed : int
            shuffle : bool
            total_size
            set_epoch(epoch: int) None
          }
          class DistributedState {
            all_states : Optional[List[LocalState]]
            compile_pg : Any
            local_state
          }
          class DistributedTest {
          }
          class DistributedTestBase {
            backend(device) str
            create_pg(device)
            rank_to_device(device)
            setUp()
            tearDown()
          }
          class DistributedVariable {
            value
            is_available()
            python_type()
          }
          class Distribution {
            arg_constraints
            batch_shape
            event_shape
            has_enumerate_support : bool
            has_rsample : bool
            mean
            mode
            stddev
            support
            variance
            cdf(value: torch.Tensor)* torch.Tensor
            entropy()* torch.Tensor
            enumerate_support(expand: bool)* torch.Tensor
            expand(batch_shape: _size, _instance)*
            icdf(value: torch.Tensor)* torch.Tensor
            log_prob(value: torch.Tensor)* torch.Tensor
            perplexity() torch.Tensor
            rsample(sample_shape: _size)* torch.Tensor
            sample(sample_shape: _size) torch.Tensor
            sample_n(n: int) torch.Tensor
            set_default_validate_args(value: bool) None
          }
          class DivElementwiseTypePromotionRule {
            promotion_kind : DEFAULT, INT_TO_FLOAT
            preview_type_promotion(args: tuple, kwargs: dict) TypePromotionSnapshot
          }
          class DivideByKey {
            divisor : int
            get(o: int) int
          }
          class DonatedBuffer {
          }
          class DoubleLinear {
            lin1
            lin2
            relu
            use_second_linear : bool
            forward(x: torch.Tensor) Union[Tuple[torch.Tensor, torch.Tensor], torch.Tensor]
          }
          class DoubleStorage {
            dtype()
          }
          class DoubleStorage {
            dtype()
          }
          class DraftExportReport {
            failures : List[FailureReport]
            str_to_filename : Dict[str, str]
            apply_suggested_fixes()* None
            successful() bool
          }
          class Dropout {
            forward(input)
            from_float(mod, use_precomputed_fake_quant)
            from_reference(mod, scale, zero_point)
          }
          class Dropout {
            forward(input: Tensor) Tensor
          }
          class Dropout1d {
            forward(input: Tensor) Tensor
          }
          class Dropout2d {
            forward(input: Tensor) Tensor
          }
          class Dropout3d {
            forward(input: Tensor) Tensor
          }
          class DropoutRemover {
            call_module(target: Target, args: Tuple[Argument, ...], kwargs: Dict[str, Any]) Any
          }
          class DtypePropagationOpsHandler {
            bucketize(values: DTypeArg, boundaries: Tuple[str, sympy.Expr, sympy.Expr, sympy.Expr], boundary_indices: DTypeArg, indexing_dtype: torch.dtype, right: bool) torch.dtype
            ceil_to_int(x: DTypeArg, dtype: torch.dtype) torch.dtype
            constant(value: torch.types.Number, dtype: torch.dtype) torch.dtype
            div(a: DTypeArg, b: DTypeArg) torch.dtype
            floor(x: DTypeArg) torch.dtype
            floor_to_int(x: DTypeArg, dtype: torch.dtype) torch.dtype
            floordiv(x: DTypeArg, y: DTypeArg) torch.dtype
            fmod(x: DTypeArg, y: DTypeArg) torch.dtype
            frexp(x: DTypeArg) Tuple[torch.dtype, torch.dtype]
            gelu(x: DTypeArg) torch.dtype
            getitem(x: DTypeArg, y: DTypeArg) torch.dtype
            halide_clamp(value, size, check)
            identity(x: DTypeArg) torch.dtype
            index_expr(expr: sympy.Expr, dtype: torch.dtype) torch.dtype
            indirect_indexing(x: DTypeArg, size: int, check: bool, wrap_neg: bool) torch.dtype
            inline_asm_elementwise()
            int_truediv(x: DTypeArg, y: DTypeArg) torch.dtype
            invert(x: DTypeArg) torch.dtype
            libdevice_abs(x: DTypeArg) torch.dtype
            load(name: str, index) torch.dtype
            load_seed(name: str, offset: int) torch.dtype
            lshift(x: DTypeArg, y: DTypeArg) torch.dtype
            masked(mask: DTypeArg, body: 'LoopBodyBlock', other: DTypeArg) torch.dtype
            matmul(x: DTypeArg, y: DTypeArg) torch.dtype
            mod(a: DTypeArg, b: DTypeArg) torch.dtype
            mul(a: DTypeArg, b: DTypeArg) torch.dtype
            op_dtype_rule() torch.dtype
            pow(a: DTypeArg, b: DTypeArg) torch.dtype
            rand(seed: int, offset: int) torch.dtype
            randint64(seed: int, offset: int, low: int, high: int) torch.dtype
            randn(seed: int, offset: int) torch.dtype
            reduction(dtype: torch.dtype, src_dtype: torch.dtype, reduction_type: str, value: DTypeArg) torch.dtype
            return_dtype() torch.dtype
            round(x: DTypeArg) torch.dtype
            round_decimal(x: DTypeArg, y: DTypeArg) torch.dtype
            round_to_int(x: DTypeArg, dtype: torch.dtype) torch.dtype
            rshift(x: DTypeArg, y: DTypeArg) torch.dtype
            scan(dtypes: Tuple[torch.dtype, ...], combine_fn: Callable[[Tuple[T, ...], Tuple[T, ...]], Tuple[T, ...]], values: Tuple[T, ...]) Tuple[torch.dtype, ...]
            sort(dtypes: Tuple[torch.dtype, ...], values: Tuple[T, ...], stable: bool, descending: bool) Tuple[torch.dtype, ...]
            store(name: str, index, value: DTypeArg, mode: Optional[str]) None
            store_reduction(name: str, index, value: DTypeArg) None
            to_dtype(x: DTypeArg, dtype: torch.dtype, src_dtype: Optional[torch.dtype], use_compute_types) torch.dtype
            to_dtype_bitcast(x: DTypeArg, dtype: torch.dtype, src_dtype: torch.dtype) torch.dtype
            truediv(a: DTypeArg, b: DTypeArg) torch.dtype
            trunc(x: DTypeArg) torch.dtype
            trunc_to_int(x: DTypeArg, dtype: torch.dtype) torch.dtype
            truncdiv(x: DTypeArg, y: DTypeArg) torch.dtype
            where(a: DTypeArg, b: DTypeArg, c: DTypeArg) torch.dtype
          }
          class DtypeView {
            dtype
            target_dtype
            create(x, new_dtype)
            get_size() Sequence[Expr]
            make_loader() Callable[[Sequence[Expr]], OpsValue]
          }
          class DualLevelContextManager {
            new_level
            create(tx: 'InstructionTranslator')
            enter(tx)
            exit(tx: 'InstructionTranslator')
          }
          class DummyDDP {
            module
            forward()
          }
          class DummyEnv {
            iter : int
            num_iters : int
            reward_threshold : float
            state_dim : int
            reset()
            seed(manual_seed)
            step(action)
          }
          class DummyModule {
            is_torch : bool
            name : str
          }
          class DummyModule {
          }
          class DummyModule {
            call()*
          }
          class DummyObserver {
            calculate_qparams()
            forward(x)
          }
          class DummyProcessGroup {
            allreduce()
            rank() int
            size() int
          }
          class DummyTestModel {
            fc
            forward(x)
          }
          class DumpUnpickler {
            catch_invalid_utf8 : bool
            dispatch : dict
            dump(in_stream, out_stream)
            find_class(module, name)
            load_binunicode()
            persistent_load(pid)
          }
          class DuplicateDQPass {
            call(graph_module: torch.fx.GraphModule) PassResult
          }
          class DuplicateInputs {
            input_source_a
            input_source_b
          }
          class DuplicateWarningChecker {
            maxsize : int
            set : OrderedDict
            add(key: Union[str, Tuple[object, object]]) bool
            reset()
          }
          class DynamicDimConstraintPrinter {
            source_name_to_debug_name : Mapping[str, str]
            symbol_to_source : Dict[sympy.Symbol, List[Source]]
          }
          class DynamicMetaLoadPlanner {
            metadata
            set_up_planner(state_dict: STATE_DICT_TYPE, metadata: Optional[Metadata], is_coordinator: bool) None
          }
          class DynamicOutputShapeException {
            func
          }
          class DynamicRendezvousHandler {
            settings
            use_agent_store
            from_backend(run_id: str, store: Store, backend: RendezvousBackend, min_nodes: int, max_nodes: int, local_addr: Optional[str], timeout: Optional[RendezvousTimeout])
            get_backend() str
            get_run_id() str
            is_closed() bool
            next_rendezvous() RendezvousInfo
            num_nodes_waiting() int
            set_closed() None
            shutdown() bool
          }
          class DynamicScalar {
            keypath
            name
            sym
            codegen(wrapper) None
            get_reads() OrderedSet[Dep]
            get_unbacked_symbol_defs() OrderedSet[sympy.Symbol]
            should_allocate() bool
          }
          class DynamicShapeAssert {
            forward(x)
          }
          class DynamicShapeConstructor {
            forward(x)
          }
          class DynamicShapeIfGuard {
            forward(x)
          }
          class DynamicShapeMap {
            forward(xs, y)
          }
          class DynamicShapeRound {
            forward(x)
          }
          class DynamicShapeSlicing {
            forward(x)
          }
          class DynamicShapeView {
            forward(x)
          }
          class DynamicShapesSpec {
            dims : Dict[str, RootDim]
            dynamic_shapes : Union[Dict[str, Any], Tuple[Any], List[Any], None]
          }
          class DynamicStaticDetector {
            DEFAULT_DYNAMIC_REC_KEY : str
            DEFAULT_DYNAMIC_STATIC_CHECK_SUPPORTED : set
            DEFAULT_DYNAMIC_STATIC_FUTURE_SUPPORTED : set
            DEFAULT_POST_OBSERVER_NAME : str
            DEFAULT_PRE_OBSERVER_NAME : str
            INPUT_ACTIVATION_PREFIX : str
            IS_CURRENTLY_SUPPORTED_KEY : str
            NON_STATIONARY_STR : str
            OUTPUT_ACTIVATION_PREFIX : str
            POST_OBS_COMP_STAT_KEY : str
            POST_OBS_DATA_DIST_KEY : str
            PRE_OBS_COMP_STAT_KEY : str
            PRE_OBS_DATA_DIST_KEY : str
            STATIONARY_STR : str
            TOLERANCE_KEY : str
            tolerance : float
            useful_observer_fqns : Set[str]
            determine_observer_insert_points(prepared_fx_model: GraphModule) Dict[str, Dict[str, Any]]
            generate_detector_report(model: GraphModule) Tuple[str, Dict[str, Any]]
            get_detector_name() str
            get_qconfig_info(model) Dict[str, DetectorQConfigInfo]
          }
          class DynamoCallbackFn {
          }
          class DynamoDistributedMultiProcTestCase {
            file_name : str
            rank : int
            world_size
            setUp()
            tearDown()
          }
          class DynamoDistributedSingleProcTestCase {
            setUpClass()
            tearDownClass()
          }
          class DynamoExport {
            aten_graph : bool
            generate_fx(options: _exporter_legacy.ResolvedExportOptions, model: torch.nn.Module | Callable, model_args: Sequence[Any], model_kwargs: Mapping[str, Any]) torch.fx.GraphModule
            pre_export_passes(options: _exporter_legacy.ResolvedExportOptions, original_model: torch.nn.Module | Callable, fx_module: torch.fx.GraphModule, fx_module_args: Sequence[Any])
          }
          class DynamoFlattenOutputStep {
            apply(model_outputs: Any, model: torch.nn.Module | Callable | torch_export.ExportedProgram | None) Sequence[Any]
          }
          class DynamoGuardHook {
          }
          class DynamoStance {
            backend : Optional[Union[str, Callable[..., Any], None]]
            skip_guard_eval_unsafe : bool
            stance : str
          }
          class DynamoTLS {
            traced_frame_infos : List[str]
          }
          class DynamoTritonHOPifier {
            call_HOP(variable, grids, combined_args_raw, tx) ConstantVariable
            call_grid(grid, meta, tx)
            check_grid(grid) Tuple[torch.fx.proxy.Proxy, ...]
            get_value(val: Any) Any
            is_callable(maybe_callable: Any) bool
            raise_unsupported(msg: str) Never
          }
          class ELEMENTWISE_PRIM_TYPE_PROMOTION_KIND {
            name
          }
          class ELEMENTWISE_TYPE_PROMOTION_KIND {
            name
          }
          class ELU {
            scale
            zero_point
            forward(input)
            from_float(mod, use_precomputed_fake_quant)
            from_reference(mod, scale, zero_point)
          }
          class ELU {
            alpha : float
            inplace : bool
            extra_repr() str
            forward(input: Tensor) Tensor
          }
          class EagerAndRecordGraphs {
            graphs : List[torch.fx.GraphModule]
          }
          class Edge {
            id : str
            label : Optional[_message.Message]
            properties : Optional[_property_bag.PropertyBag]
            source_node_id : str
            target_node_id : str
          }
          class EdgeTraversal {
            edge_id : str
            final_state : Optional[Any]
            message : Optional[_message.Message]
            properties : Optional[_property_bag.PropertyBag]
            step_over_edge_count : Optional[int]
          }
          class Edges {
            ins : int
            outs : List[int]
          }
          class EffectTokensWrapper {
            post_compile(compiled_fn, _aot_config)
          }
          class EffectfulKernel {
            effect_type : NoneType, ORDERED
            prev_effect_buffer
            get_read_writes() dependencies.ReadWrites
            has_side_effects() bool
          }
          class EinsumDims {
            batch_dims : List[str]
            contracting_dims : List[str]
            lhs_out_only_dims : List[str]
            rhs_out_only_dims : List[str]
            parse_dims(input_dims: List[str], output_dim: str) 'EinsumDims'
            parse_equation(equation: str) Tuple[List[str], str]
          }
          class ElasticAgent {
            get_worker_group(role: str)* WorkerGroup
            run(role: str)* RunResult
          }
          class ElasticDistributedSampler {
            num_samples : int
            start_index : int
            total_size
          }
          class ElementwiseBinaryPythonRefInfo {
            torch_opinfo
            torch_opinfo_name
            torch_opinfo_variant_name : str
          }
          class ElementwiseTypePromotionRule {
            promote_args_positions : Sequence[int]
            promote_kwargs_names : Sequence[str]
            promotion_kind : ELEMENTWISE_TYPE_PROMOTION_KIND
            preview_type_promotion(args: tuple, kwargs: dict) TypePromotionSnapshot
          }
          class ElementwiseTypePromotionRuleSetGenerator {
            generate_from_torch_refs() set[ElementwiseTypePromotionRule]
          }
          class ElementwiseUnaryPythonRefInfo {
            torch_opinfo
            torch_opinfo_name
            torch_opinfo_variant_name : str
            validate_view_consistency : bool
          }
          class EmbBagWrapper {
            emb_bag
            forward(indices, offsets)
          }
          class Embedding {
            forward(input: Tensor) Tensor
            from_float(mod, weight_qparams)
          }
          class Embedding {
            dtype
            embedding_dim : int
            num_embeddings : int
            extra_repr()
            forward(indices: Tensor) Tensor
            from_float(mod, use_precomputed_fake_quant)
            from_reference(ref_embedding)
            set_weight(w: torch.Tensor) None
            weight()
          }
          class Embedding {
            qconfig : NoneType
            weight_fake_quant
            forward(input) Tensor
            from_float(mod, use_precomputed_fake_quant)
            to_float()
          }
          class Embedding {
            embedding_dim : int
            freeze : bool
            max_norm : Optional[float]
            norm_type : float
            num_embeddings : int
            padding_idx : Optional[int]
            qconfig
            scale_grad_by_freq : bool
            sparse : bool
            weight
            extra_repr() str
            forward(input: Tensor) Tensor
            from_pretrained(embeddings, freeze, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)
            reset_parameters() None
          }
          class EmbeddingBag {
            forward(input: Tensor, offsets: Optional[Tensor], per_sample_weights: Optional[Tensor]) Tensor
            from_float(mod, weight_qparams, use_precomputed_fake_quant)
          }
          class EmbeddingBag {
            dtype
            include_last_offset : bool
            mode : str
            pruned_weights : bool
            forward(indices: Tensor, offsets: Optional[Tensor], per_sample_weights: Optional[Tensor], compressed_indices_mapping: Optional[Tensor]) Tensor
            from_float(mod, use_precomputed_fake_quant)
            from_reference(ref_embedding_bag)
          }
          class EmbeddingBag {
            qconfig : NoneType
            weight_fake_quant
            forward(input, offsets, per_sample_weights) Tensor
            from_float(mod, use_precomputed_fake_quant)
            to_float()
          }
          class EmbeddingBag {
            embedding_dim : int
            include_last_offset : bool
            max_norm : Optional[float]
            mode : str
            norm_type : float
            num_embeddings : int
            padding_idx : Optional[int]
            qconfig
            scale_grad_by_freq : bool
            sparse : bool
            weight
            extra_repr() str
            forward(input: Tensor, offsets: Optional[Tensor], per_sample_weights: Optional[Tensor]) Tensor
            from_pretrained(embeddings: Tensor, freeze: bool, max_norm: Optional[float], norm_type: float, scale_grad_by_freq: bool, mode: str, sparse: bool, include_last_offset: bool, padding_idx: Optional[int]) 'EmbeddingBag'
            reset_parameters() None
          }
          class EmbeddingBagModule {
            emb
            forward(indices, offsets, per_sample_weights)
          }
          class EmbeddingConvLinearModule {
            conv
            emb
            linear
            forward(indices)
          }
          class EmbeddingModule {
            emb
            forward(indices)
          }
          class EmbeddingModule {
            emb
            forward(indices)
          }
          class EmbeddingNetDifferentParams {
            embedding
            lin
            lin2
            forward(x)
          }
          class EmbeddingPackedParams {
            dtype
            forward(x)
            set_weight(weight: torch.Tensor) None
          }
          class EmbeddingPerSampleGrad {
            backward(ctx, grad_output)
            forward(ctx, kwarg_names, _)
          }
          class EmbeddingQuantizeHandler {
          }
          class EmbeddingQuantizer {
            annotate(model: torch.fx.GraphModule) torch.fx.GraphModule
            get_supported_operator_for_quantization_config(quantization_config: QuantizationConfig) List[OperatorPatternType]
            get_supported_operators() List[OperatorConfig]
            get_supported_quantization_configs() List[QuantizationConfig]
            validate(model: torch.fx.GraphModule)* None
          }
          class EmbeddingWithStaticLinear {
            dequant
            emb
            fc
            qconfig
            quant
            forward(indices, offsets, linear_in)
          }
          class EmitGemmUniversal3xInstanceWithEVT {
            builtin_epilogue_functor_template : str
            gemm_template : str
            includes : list
            operation_suffix : str
            emit(operation)
            instance_template()
          }
          class Empty {
            size_hint : int
            get_live_ranges()
            get_size_hint()
            get_symbolic_size()
            is_empty()
          }
          class EmptyMatchError {
          }
          class EnableReduction {
            filter(node_schedule: List[NodeScheduleEntry]) Iterable[SchedulerNode]
          }
          class EnabledProxy {
            enabled : bool
            parse_env(name, default, true_message, false_message)
          }
          class EnforceUnique {
            seen : set
            see()
          }
          class EnterDeviceContextManagerLine {
            device_idx : int
            last_seen_device_guard_index : Optional[int]
            codegen(code: IndentedBuffer) None
          }
          class EnterSubgraphLine {
            graph
            wrapper
            codegen(code: IndentedBuffer) None
          }
          class EnumEncoder {
            default(obj: object) str
          }
          class EnumEncoder {
            default(obj)
          }
          class EnumVariable {
            value
            as_proxy()
            as_python_constant()
            create(cls_type, value_vt, options)
            var_getattr(tx: 'InstructionTranslator', name)
          }
          class EnumerableShardingSpec {
            shards : List[ShardMetadata]
            build_metadata(tensor_sizes: torch.Size, tensor_properties: sharded_tensor_meta.TensorProperties) sharded_tensor_meta.ShardedTensorMetadata
            shard(tensor: torch.Tensor, src_rank: int, process_group)* 'ShardedTensor'
          }
          class EphemeralSource {
            desc : Optional[str]
            guard_source()
            is_ephemeral()
            make_guard(fn)*
            name()
          }
          class Equality {
            lhs
            rhs
          }
          class EqualityConstraint {
            derived_equalities : List[Tuple[Source, Union[Source, sympy.Symbol], Callable[[sympy.Expr], sympy.Expr]]]
            phantom_symbols : List[sympy.Symbol]
            relaxed_sources : Set[Source]
            source_pairs : List[Tuple[Source, Source]]
            is_derived(src: Source, symbol_src: Source, fn: Callable[[sympy.Expr], sympy.Expr]) bool
            is_equal(source1: Source, source2: Source) bool
          }
          class EqualizationQConfig {
          }
          class ErasedTensor {
            erased_name : Optional[str]
            owning_mod_ref
          }
          class ErrorFromChoice {
            choice
          }
          class ErrorHandler {
            dump_error_file(rootcause_error_file: str, error_code: int)
            initialize() None
            override_error_code_in_rootcause_data(rootcause_error_file: str, rootcause_error: Dict[str, Any], error_code: int)
            record_exception(e: BaseException) None
          }
          class ErrorInput {
            error_regex
            error_type : RuntimeError
            sample_input
          }
          class ErrorMeta {
            id : tuple
            msg : str
            type : Type[Exception]
            to_error(msg: Optional[Union[str, Callable[[str], str]]]) Exception
          }
          class ErrorModuleInput {
            error_on : CONSTRUCTION_ERROR
            error_regex
            error_type : RuntimeError
            module_error_input
          }
          class ErrorOptimizerInput {
            error_on : CONSTRUCTION_ERROR
            error_regex : str
            error_type : RuntimeError
            optimizer_error_input
          }
          class EtcdRendezvous {
            client
            announce_self_waiting(expected_version)
            confirm_membership(expected_version, this_rank)
            confirm_phase(expected_version, this_rank)
            create_path_if_not_exists(full_path, ttl)
            get_path(path)
            get_rdzv_state()
            handle_existing_rendezvous(expected_version)
            handle_join_last_call(expected_version, deadline)
            init_phase()
            join_phase(expected_version)
            join_rendezvous(expected_version)
            load_extra_data(rdzv_version, key, timeout)
            rendezvous_barrier()
            set_closed()
            setup_kv_store(rdzv_version)
            setup_lease_renewal(full_path, ttl)
            store_extra_data(rdzv_version, key, value)
            try_create_rendezvous()
            try_wait_for_state_change(etcd_index, timeout)
            wait_for_final(expected_version)
            wait_for_peers(expected_version)
            wait_for_rendezvous_to_free(expected_version)
          }
          class EtcdRendezvousBackend {
            name
            get_state() Optional[Tuple[bytes, Token]]
            set_state(state: bytes, token: Optional[Token]) Optional[Tuple[bytes, Token, bool]]
          }
          class EtcdRendezvousHandler {
            get_backend() str
            get_run_id() str
            is_closed()
            next_rendezvous()
            num_nodes_waiting()
            set_closed()
            shutdown() bool
          }
          class EtcdRendezvousRetryImmediately {
          }
          class EtcdRendezvousRetryableFailure {
          }
          class EtcdServer {
            get_client()
            get_endpoint() str
            get_host() str
            get_port() int
            start(timeout: int, num_retries: int, stderr: Union[int, TextIO, None]) None
            stop() None
          }
          class EtcdStore {
            client
            prefix
            add(key, num: int) int
            check(keys) bool
            get(key) bytes
            set(key, value)
            wait(keys, override_timeout: Optional[datetime.timedelta])
          }
          class EvalEnv {
            env : dict
            rcb
          }
          class Event {
            elapsed_time(end_event)
            query() bool
            record(stream) None
            synchronize() None
            wait(stream) None
          }
          class Event {
          }
          class Event {
            time : float
            elapsed_time(end_event) float
            record(stream)
          }
          class Event {
            elapsed_time(end_event)
            query()
            record()
            synchronize()
            wait()
          }
          class Event {
            query() bool
            record(stream)* None
            synchronize()* None
            wait(stream)* None
          }
          class Event {
            metadata : Dict[str, EventMetadataValue]
            name : str
            source
            timestamp : int
            deserialize(data: Union[str, 'Event']) 'Event'
            serialize() str
          }
          class Event {
            elapsed_time(end_event)
            from_ipc_handle(device, handle)
            ipc_handle()
            query()
            record(stream)
            synchronize() None
            wait(stream) None
          }
          class Event {
            name
          }
          class EventHandler {
            seq_num : int
            syncs
            tensors_accessed
          }
          class EventKey {
            event
            intervals_overlap(intervals: List[Interval])
          }
          class EventList {
            self_cpu_time_total
            export_chrome_trace(path)
            export_stacks(path: str, metric: str)
            key_averages(group_by_input_shapes, group_by_stack_n)
            supported_export_stacks_metrics()
            table(sort_by, row_limit, max_src_column_width, max_name_column_width, max_shapes_column_width, header, top_level_events_only)
            total_average()
          }
          class EventMetrics {
            duration_time_ns : int
            fraction_idle_time
            idle_time_ns : int
            queue_depth : int
            self_time_ns : int
          }
          class EventSource {
            name
          }
          class EventVariable {
            proxy
            value
            as_proxy()
            call_method(tx, name, args: 'List[VariableTracker]', kwargs: 'Dict[str, VariableTracker]') 'VariableTracker'
            reconstruct(codegen)
          }
          class ExactReaderInterp {
            run_node(n)
          }
          class ExactWeakKeyDictionary {
            refs : dict
            values : dict
            clear()
            get(key, default)
          }
          class ExampleAggregateAsDataFrames {
            columns : NoneType
            dataframe_size : int
            source_datapipe
          }
          class Exception {
            inner_exceptions : Optional[List[_exception.Exception]]
            kind : Optional[str]
            message : Optional[str]
            properties : Optional[_property_bag.PropertyBag]
            stack : Optional[_stack.Stack]
          }
          class ExceptionModule {
            param
            forward(_)
          }
          class ExceptionTableEntry {
            depth : int
            end : int
            lasti : bool
            start : int
            target : int
          }
          class ExceptionVariable {
            args
            exc_type
            reconstruct(codegen)
          }
          class ExceptionWrapper {
            exc_msg : str
            exc_type
            where : str
            reraise()
          }
          class ExclusiveKeywordArg {
            name : str
            pattern_eq(other: Any) bool
          }
          class ExecMode {
            name
          }
          class ExecutionRecord {
            builtins : Dict[str, Any]
            closure : Tuple[CellType]
            code
            code_options : Dict[str, Any]
            globals : Dict[str, Any]
            locals : Dict[str, Any]
            dump(f: IO[str]) None
            load(f: BinaryIO) Self
          }
          class ExecutionRecorder {
            LOCAL_MOD_PREFIX : str
            builtins : Dict[str, Any]
            closure : Tuple[CellType]
            code
            code_options : Dict[str, Any]
            globals : Dict[str, Any]
            locals : Dict[str, Any]
            name_to_modrec : Dict[str, ModuleRecord]
            add_global_var(name: str, var: Any) None
            add_local_mod(name: str, mod: ModuleType) None
            add_local_var(name: str, var: Any) None
            get_record() ExecutionRecord
            record_module_access(mod: ModuleType, name: str, val: Any) None
          }
          class ExecutionState {
            name
          }
          class ExecutionStats {
            benchmark_config
            iters_per_second
            latency_avg_ms
            num_iters
            total_time_seconds
          }
          class ExecutionTraceObserver {
            is_registered
            cleanup()
            get_output_file_path() str
            is_running()
            register_callback(output_file_path: str) Self
            start()
            stop()
            unregister_callback()
          }
          class ExecutorchCallDelegate {
          }
          class ExecutorchCallDelegateHigherOrderVariable {
            call_function(tx: 'InstructionTranslator', args: 'List[VariableTracker]', kwargs: 'Dict[str, VariableTracker]') 'VariableTracker'
          }
          class ExitDeviceContextManagerLine {
            codegen(code: IndentedBuffer) None
          }
          class ExitSubgraphLine {
            wrapper
            codegen(code: IndentedBuffer) None
          }
          class ExpRelaxedCategorical {
            arg_constraints : dict
            has_rsample : bool
            logits
            param_shape
            probs
            support
            temperature
            expand(batch_shape, _instance)
            log_prob(value)
            rsample(sample_shape: _size) torch.Tensor
          }
          class ExpTransform {
            bijective : bool
            codomain
            domain
            sign : int
            log_abs_det_jacobian(x, y)
          }
          class ExpandView {
            size : List[Expr]
            create(x, new_size)
            get_size() Sequence[Expr]
            make_reindexer()
          }
          class ExpandedWeight {
            allow_smaller_batches : bool
            batch_first : bool
            batch_size
            data
            device
            dtype
            handled_functions : dict
            is_cuda
            loss_reduction
            orig_weight
            shape
            data_ptr()
            get_device()
            set_allow_smaller_batches(is_allow_smaller_batches)
            set_batch_first(is_batch_first)
          }
          class ExplainOutput {
            break_reasons : List[Any]
            compile_times : Optional[str]
            graph_break_count : int
            graph_count : int
            graphs : List[torch.fx.GraphModule]
            op_count : int
            ops_per_graph : Optional[List[torch.fx.Node]]
            out_guards : Optional[List[_guards.Guard]]
          }
          class ExplainTS2FXGraphConverter {
            name_to_node
            unsupported_node_list : List[torch._C.Node]
            convert_node(node)
            explain()
          }
          class ExplainWithBackend {
            backend
            break_reasons : list
            graphs : list
            op_count : int
            output() ExplainOutput
          }
          class Exponential {
            arg_constraints : dict
            has_rsample : bool
            mean
            mode
            rate
            stddev
            support
            variance
            cdf(value)
            entropy()
            expand(batch_shape, _instance)
            icdf(value)
            log_prob(value)
            rsample(sample_shape: _size) torch.Tensor
          }
          class ExponentialFamily {
            entropy()
          }
          class ExponentialLR {
            gamma : float
            get_lr()
          }
          class ExportArtifact {
            aten
            fake_mode
            module_call_specs : Dict[str, Dict[str, pytree.TreeSpec]]
            out_spec
          }
          class ExportBackwardSignature {
            gradients_to_parameters : Dict[str, str]
            gradients_to_user_inputs : Dict[str, str]
            loss_output : str
          }
          class ExportCase {
            description : str
            dynamic_shapes : Optional[Dict[str, Any]]
            example_args : Tuple
            example_kwargs : Dict[str, Any]
            extra_args : Optional[ArgsType]
            model
            name : str
            support_level
            tags : Set[str]
          }
          class ExportDiagnosticEngine {
            background_context
            contexts : list[infra.DiagnosticContext]
            clear()
            create_diagnostic_context(name: str, version: str, options: infra.DiagnosticOptions | None) infra.DiagnosticContext
            dump(file_path: str, compress: bool) None
            sarif_log()
            to_json() str
          }
          class ExportDynamoConfig {
            allow_rnn : bool
          }
          class ExportDynamoConfig {
            allow_rnn : bool
            do_not_emit_runtime_asserts : bool
            reorderable_logging_functions : Set[Callable]
          }
          class ExportError {
          }
          class ExportErrorType {
            name
          }
          class ExportGraphSignature {
            assertion_dep_token
            backward_signature
            buffers
            buffers_to_mutate
            input_specs : List[InputSpec]
            input_tokens
            inputs_to_buffers
            inputs_to_lifted_custom_objs
            inputs_to_lifted_tensor_constants
            inputs_to_parameters
            lifted_custom_objs
            lifted_tensor_constants
            non_persistent_buffers
            output_specs : List[OutputSpec]
            output_tokens
            parameters
            user_inputs
            user_inputs_to_mutate
            user_outputs
            get_replace_hook(replace_inputs)
            replace_all_uses(old: str, new: str)
          }
          class ExportInterpreter {
            callback : str
            node
            call_function(target: torch.fx.node.Target, args: Tuple[Argument, ...], kwargs: Dict[str, Argument]) ProxyValue
            call_method(target: str, args: Tuple[Argument, ...], kwargs: Dict[str, Argument]) None
            call_module(target: torch.fx.node.Target, args: Tuple[Argument, ...], kwargs: Dict[str, Argument]) None
            get_attr(target: str, args: Tuple[Argument, ...], kwargs: Dict[str, Argument]) Argument
            output(target: torch.fx.node.Target, args: Tuple[Argument, ...], kwargs: Dict[str, Argument]) ProxyValue
            placeholder(target: str, args: Tuple[Argument, ...], kwargs: Dict[str, Argument]) ProxyValue
            run_node(n: torch.fx.Node) Argument
          }
          class ExportOptions {
            custom_opsets : Optional[Mapping[str, int]]
            do_constant_folding : bool
            dynamic_axes : Optional[Mapping[str, Union[Mapping[int, str], Sequence[int]]]]
            export_modules_as_functions : Union[bool, Set[Type[torch.nn.Module]]]
            export_params : bool
            input_names : Optional[Sequence[str]]
            keep_initializers_as_inputs : Optional[bool]
            operator_export_type
            opset_version : Optional[int]
            output_names : Optional[Sequence[str]]
            training
            verbose : bool
          }
          class ExportOptions {
            diagnostic_options
            dynamic_shapes : bool | None
            fake_context : ONNXFakeContext | None
            onnx_registry : OnnxRegistry | None
          }
          class ExportPassBaseError {
          }
          class ExportResult {
            graph_module
            guards
          }
          class ExportStatus {
            decomposition : bool | None
            onnx_checker : bool | None
            onnx_runtime : bool | None
            onnx_translation : bool | None
            output_accuracy : bool | None
            torch_export : bool | None
            torch_export_non_strict : bool | None
            torch_jit : bool | None
          }
          class ExportTracepoint {
          }
          class ExportTracepointHigherOrderVariable {
            call_function(tx: 'InstructionTranslator', args: 'List[VariableTracker]', kwargs: 'Dict[str, VariableTracker]') 'VariableTracker'
          }
          class ExportTracer {
            callback : str
            fake_tensor_mode : NoneType, Optional[FakeTensorMode]
            graph
            root
            submodules : Dict[torch.nn.Module, str]
            tensor_attrs : Dict[str, torch.Tensor]
            create_arg(a: Argument) torch.fx.Node
            set_metadata(node: torch.fx.Node, value: Argument) None
            trace() None
          }
          class ExportTypes {
            COMPRESSED_ZIP_ARCHIVE : str
            DIRECTORY : str
            PROTOBUF_FILE : str
            ZIP_ARCHIVE : str
          }
          class ExportedProgram {
            graph_module : Annotated[GraphModule, 10]
            opset_version : Annotated[Dict[str, int], 20]
            range_constraints : Annotated[Dict[str, RangeConstraint], 30]
            schema_version : Annotated[SchemaVersion, 60]
            torch_version : Annotated[str, 80]
            verifiers : Annotated[List[str], 70]
          }
          class ExportedProgram {
            call_spec
            constants
            dialect
            example_inputs
            graph
            graph_module
            graph_signature
            module_call_graph
            range_constraints
            state_dict
            tensor_constants
            verifier
            verifiers
            buffers() Iterator[torch.Tensor]
            module() torch.nn.Module
            named_buffers() Iterator[Tuple[str, torch.Tensor]]
            named_parameters() Iterator[Tuple[str, torch.nn.Parameter]]
            parameters() Iterator[torch.nn.Parameter]
            run_decompositions(decomp_table: Optional[Dict[torch._ops.OperatorBase, Callable]]) 'ExportedProgram'
            validate()
          }
          class ExportedProgramDeserializer {
            expected_opset_version : Dict[str, int]
            deserialize(exported_program: ExportedProgram, state_dict: Union[Dict[str, torch.Tensor], bytes], constants: Union[Dict[str, torch.Tensor], bytes], example_inputs: Optional[Union[Tuple[Tuple[torch.Tensor, ...], Dict[str, Any]], bytes]]) ep.ExportedProgram
            deserialize_range_constraints(symbol_name_to_range: Dict[str, symbolic_shapes.ValueRanges], symbol_name_to_symbol: Dict[str, sympy.Symbol]) Dict[sympy.Symbol, ValueRanges]
          }
          class ExportedProgramSerializer {
            opset_version : Dict[str, int]
            serialize(exported_program: ep.ExportedProgram) _SerializedProgram
          }
          class Exporter {
            model : torch.nn.Module | Callable
            model_args : Sequence[Any]
            model_kwargs : Mapping[str, Any]
            options
            export() _onnx_program.ONNXProgram
          }
          class ExprBuilder {
            binop_map : dict
            boolop_map : dict
            cmpop_map : dict
            unop_map : dict
            build_Attribute(ctx, expr)
            build_BinOp(ctx, expr)
            build_BoolOp(ctx, expr)
            build_Call(ctx, expr)
            build_Compare(ctx, expr)
            build_Constant(ctx, expr)
            build_Dict(ctx, expr)
            build_DictComp(ctx, stmt)
            build_Ellipsis(ctx, expr)
            build_GeneratorExp(ctx, stmt)
            build_IfExp(ctx, expr)
            build_JoinedStr(ctx, expr)
            build_List(ctx, expr)
            build_ListComp(ctx, stmt)
            build_Name(ctx, expr)
            build_NameConstant(ctx, expr)
            build_Num(ctx, expr)
            build_Starred(ctx, expr)
            build_Str(ctx, expr)
            build_Subscript(ctx, expr)
            build_Tuple(ctx, expr)
            build_UnaryOp(ctx, expr)
          }
          class ExprCounter {
            visit(node: ast.AST) Any
          }
          class ExprPrinter {
            printmethod : str
          }
          class ExtensionHandler {
            from_op_name(name: str)*
            namespace()* str
            op_schema(op)* torch.FunctionSchema
            to_op_name(op)* str
          }
          class ExtensionVersioner {
            entries : dict
            bump_version_if_changed(name, source_files, build_arguments, build_directory, with_cuda, is_python_module, is_standalone)
            get_version(name)
          }
          class ExtensionsData {
            all_gather_input_sizes : Sequence[torch.Size]
            all_gather_metadata : Optional[Any]
            clear()
          }
          class ExternKernel {
            allarg_properties
            arg_properties : Optional[List[Dict[str, Any]]]
            constant_args : Tuple[Any, ...]
            cpp_kernel_name : Optional[str]
            fx_node
            kwarg_properties : Optional[Dict[str, Dict[str, Any]]]
            kwargs : Dict[str, Any]
            mutation_outputs : List[MutationOutput]
            op_overload : Optional[Union[torch._ops.OpOverload, torch._ops.HigherOrderOperator]]
            ordered_kwargs_for_cpp_kernel : Iterable[str]
            output_view : Optional[ReinterpretView]
            python_kernel_name : Optional[str]
            schema_kwargs
            unbacked_bindings : Dict[sympy.Symbol, pytree.KeyPath]
            apply_constraint()* None
            canonicalize()
            codegen(wrapper)*
            codegen_args()
            codegen_comment(wrapper) None
            codegen_const_args(names: Optional[List[str]])
            codegen_kwargs(skip_out)
            codegen_size_asserts(wrapper) None
            collect_arg_kwarg_properties()
            convert_to_reinterpret_view(x)
            copy_input(x)
            decide_layout()
            fill_non_provided_args(args, kwargs)
            get_group_stride()
            get_kernel_name()
            get_kwargs_value(arg_name)
            get_outputs() List[Buffer]
            get_unbacked_symbol_defs() OrderedSet[sympy.Symbol]
            get_unbacked_symbol_uses() OrderedSet[sympy.Symbol]
            process_kernel(kernel) Tuple[Any, List[Any], List[Any], Callable[[Any, Any], Any], Optional[Dict[sympy.Symbol, pytree.KeyPath]]]
            realize_input(x)
            require_channels_last(x)
            require_channels_last_3d(x)
            require_contiguous(x)
            require_exact_strides(x, exact_strides, allow_padding)
            require_stride1(x)
            require_stride_order(x, order, allow_padding)
            require_strides(x, order: Optional[Sequence[int]], exact_strides: Optional[Sequence[_IntLike]], allow_padding)
            set_cpp_kernel_name(cpp_kernel_name: Optional[str]) None
            set_python_kernel_name(python_kernel_name: Optional[str]) None
          }
          class ExternKernelAlloc {
            name
            outputs : Sequence[Any]
            apply_constraint()*
            codegen(wrapper) None
            should_allocate() bool
          }
          class ExternKernelCaller {
            choice
            has_out_variant : bool
            kwargs : dict
            autoheuristic_id()
            benchmark()
            hash_key()
            info_dict() Dict[str, Union[PrimitiveInfoType, List[PrimitiveInfoType]]]
            output_node()
            to_callable()
          }
          class ExternKernelChoice {
            cpp_kernel_name : NoneType
            has_out_variant : bool
            kernel_creator : NoneType
            name
            op_overload : NoneType
            ordered_kwargs_for_cpp_kernel : tuple
            use_fallback_kernel : bool
            bind(input_nodes, layout, ordered_kwargs_for_cpp_kernel)
            call_name()
            hash_key()
            to_callable()
          }
          class ExternKernelNode {
            name : str
            node
          }
          class ExternKernelNode {
            name : str
            node
          }
          class ExternKernelNodes {
            nodes : List[ExternKernelNode]
          }
          class ExternKernelOut {
            name
            codegen(wrapper) None
            should_allocate() bool
          }
          class ExternKernelSchedulerNode {
            last_usage
            max_order
            min_order
            debug_str_extra() str
            has_side_effects() bool
            is_extern() bool
          }
          class ExternalProperties {
            addresses : Optional[List[_address.Address]]
            artifacts : Optional[List[_artifact.Artifact]]
            conversion : Optional[_conversion.Conversion]
            driver : Optional[_tool_component.ToolComponent]
            extensions : Optional[List[_tool_component.ToolComponent]]
            externalized_properties : Optional[_property_bag.PropertyBag]
            graphs : Optional[List[_graph.Graph]]
            guid : Optional[str]
            invocations : Optional[List[_invocation.Invocation]]
            logical_locations : Optional[List[_logical_location.LogicalLocation]]
            policies : Optional[List[_tool_component.ToolComponent]]
            properties : Optional[_property_bag.PropertyBag]
            results : Optional[List[_result.Result]]
            run_guid : Optional[str]
            schema : Optional[str]
            taxonomies : Optional[List[_tool_component.ToolComponent]]
            thread_flow_locations : Optional[List[_thread_flow_location.ThreadFlowLocation]]
            translations : Optional[List[_tool_component.ToolComponent]]
            version : Optional[Literal['2.1.0']]
            web_requests : Optional[List[_web_request.WebRequest]]
            web_responses : Optional[List[_web_response.WebResponse]]
          }
          class ExternalPropertyFileReference {
            guid : Optional[str]
            item_count : int
            location : Optional[_artifact_location.ArtifactLocation]
            properties : Optional[_property_bag.PropertyBag]
          }
          class ExternalPropertyFileReferences {
            addresses : Optional[List[_external_property_file_reference.ExternalPropertyFileReference]]
            artifacts : Optional[List[_external_property_file_reference.ExternalPropertyFileReference]]
            conversion : Optional[_external_property_file_reference.ExternalPropertyFileReference]
            driver : Optional[_external_property_file_reference.ExternalPropertyFileReference]
            extensions : Optional[List[_external_property_file_reference.ExternalPropertyFileReference]]
            externalized_properties : Optional[_external_property_file_reference.ExternalPropertyFileReference]
            graphs : Optional[List[_external_property_file_reference.ExternalPropertyFileReference]]
            invocations : Optional[List[_external_property_file_reference.ExternalPropertyFileReference]]
            logical_locations : Optional[List[_external_property_file_reference.ExternalPropertyFileReference]]
            policies : Optional[List[_external_property_file_reference.ExternalPropertyFileReference]]
            properties : Optional[_property_bag.PropertyBag]
            results : Optional[List[_external_property_file_reference.ExternalPropertyFileReference]]
            taxonomies : Optional[List[_external_property_file_reference.ExternalPropertyFileReference]]
            thread_flow_locations : Optional[List[_external_property_file_reference.ExternalPropertyFileReference]]
            translations : Optional[List[_external_property_file_reference.ExternalPropertyFileReference]]
            web_requests : Optional[List[_external_property_file_reference.ExternalPropertyFileReference]]
            web_responses : Optional[List[_external_property_file_reference.ExternalPropertyFileReference]]
          }
          class ExternalStream {
          }
          class ExtraCUDACopyPattern {
            description : str
            init_ops : set
            name : str
            skip
            url : str
            benchmark(events: List[_ProfilerEvent])
            match(event)
          }
          class ExtraOpData {
            dim_args : Optional[List[List[str]]]
            is_view : bool
            get_dim_argnames() Tuple[Optional[str], Optional[str]]
          }
          class ExtractConstantsHandler {
            device
            constant(value: Any, dtype: torch.dtype) 'torch._inductor.ir.Constant'
          }
          class F {
          }
          class FP32MatMulPattern {
            description : str
            name : str
            skip
            url : str
            benchmark(events: List[_ProfilerEvent])
            match(event: _ProfilerEvent)
            report(event: _ProfilerEvent)
          }
          class FPGMPruner {
            dist_fn : NoneType, int
            update_mask(module, tensor_name, sparsity_level)
          }
          class FSDPCommContext {
            all_gather_copy_in_stream
            all_gather_state : NoneType, Optional[AllGatherState]
            all_gather_stream
            all_reduce_stream
            device_handle
            post_forward_order : List[FSDPParamGroup]
            reduce_scatter_state : NoneType, Optional[ReduceScatterState]
            reduce_scatter_stream
            get_all_gather_streams(async_op: bool, training_state: TrainingState) Tuple[torch.Stream, torch.Stream]
            lazy_init(device: torch.device)
          }
          class FSDPExtensions {
            all_gather_dtensor(tensor: DTensor, parent_mesh: Optional[DeviceMesh])* torch.Tensor
            chunk_dtensor(tensor: torch.Tensor, rank: int, device_mesh: DeviceMesh)* torch.Tensor
            chunk_tensor(tensor: torch.Tensor, rank: int, world_size: int, num_devices_per_node: int, pg: dist.ProcessGroup, device: Optional[torch.device])* torch.Tensor
            post_unflatten_transform(tensor: torch.Tensor, param_extension: Any)* torch.Tensor
            pre_flatten_transform(tensor: torch.Tensor)* Tuple[torch.Tensor, Optional[Any]]
            pre_load_state_dict_transform(tensor: torch.Tensor)* Tuple[torch.Tensor, List[Shard]]
          }
          class FSDPInitMode {
            name
          }
          class FSDPManagedNNModuleVariable {
            source
          }
          class FSDPMemTracker {
            track_external()* None
            track_inputs(inputs: Tuple[Any, ...]) None
          }
          class FSDPMeshInfo {
            shard_mesh_rank : int
            shard_mesh_size : int
            shard_process_group
          }
          class FSDPModule {
            reshard() None
            set_is_last_backward(is_last_backward: bool) None
            set_modules_to_backward_prefetch(modules: List['FSDPModule']) None
            set_modules_to_forward_prefetch(modules: List['FSDPModule']) None
            set_post_optim_event(event: torch.Event) None
            set_reduce_scatter_divide_factor(factor: float) None
            set_requires_all_reduce(requires_all_reduce: bool) None
            set_requires_gradient_sync(requires_gradient_sync: bool) None
            set_reshard_after_backward(reshard_after_backward: bool) None
            set_unshard_in_backward(unshard_in_backward: bool) None
            unshard(async_op: bool) Optional['UnshardHandle']
          }
          class FSDPNNModuleSource {
            guard_source()
          }
          class FSDPParam {
            all_gather_inputs
            all_gather_outputs : List[torch.Tensor]
            contiguous_sharded_post_forward_stride : Tuple[int, ...]
            contiguous_sharded_stride : Tuple[int, ...]
            device
            fsdp_placement : NoneType
            grad_offload_event : Optional[torch.Event]
            is_dtensor
            mesh_info
            mp_policy
            offload_to_cpu : bool
            orig_dtype
            padded_sharded_param_size
            param_dtype : Optional[torch.dtype]
            pin_memory
            post_forward_mesh_info : Optional[FSDPMeshInfo]
            reduce_dtype : Optional[torch.dtype]
            shard_mesh
            sharded_param
            sharded_post_forward_size
            sharded_size
            sharded_state : SHARDED_POST_FORWARD, UNSHARDED
            unsharded_accumulated_grad : Optional[torch.Tensor]
            unsharded_accumulated_grad_data
            unsharded_grad_data
            unsharded_param
            accumulate_unsharded_grad_if_needed() None
            alloc_all_gather_outputs() None
            free_unsharded_param() None
            init_all_gather_outputs(all_gather_input_numels: List[int], all_gather_input_dtypes: List[torch.dtype], world_size: int, device: torch.device, force_recreate: bool)
            init_dtype_attrs(mp_policy: MixedPrecisionPolicy)
            init_unsharded_param()
            reset_sharded_param()
            to_accumulated_grad_if_needed() None
            to_sharded() None
            to_sharded_dtensor(tensor: torch.Tensor) DTensor
            to_sharded_post_forward() None
            to_sharded_post_forward_dtensor(tensor: torch.Tensor) DTensor
            to_unsharded() None
          }
          class FSDPParamGroup {
            all_reduce_grads : bool
            comm_ctx
            device
            device_handle
            fsdp_params
            is_sharded
            is_sharded_post_forward
            is_unsharded
            mesh_info
            modules : Tuple[nn.Module, ...]
            mp_policy
            offload_policy
            post_forward_mesh_info : Optional[FSDPMeshInfo]
            reduce_grads : bool
            reduce_scatter_reduce_op : Optional[dist.ReduceOp]
            reshard_after_backward : bool
            unshard_async_op : bool
            unshard_in_backward : bool
            finalize_backward()
            lazy_init()
            post_backward()
            post_forward(module: nn.Module, input: Any, output: Any)
            pre_backward(default_prefetch: bool)
            pre_forward(module: nn.Module, args: Tuple[Any, ...], kwargs: Dict[str, Any]) Tuple[Tuple[Any, ...], Dict[str, Any]]
            reshard()
            unshard(async_op: bool)
            use_training_state(training_state: TrainingState)
            wait_for_unshard()
          }
          class FSDPParamGroupUseTrainingStateVariable {
            param_group_var
            call_function(tx: 'InstructionTranslator', args: 'List[VariableTracker]', kwargs: 'Dict[str, VariableTracker]')
            create(tx: 'InstructionTranslator', param_group_var, target_value)
            enter(tx)
            exit(tx: 'InstructionTranslator')
            fn_name()
            module_name()
          }
          class FSDPParamInfo {
            handle
            param_indices : Dict[str, int]
            param_requires_grad : List[bool]
            state
          }
          class FSDPState {
            init(modules: Tuple[nn.Module, ...], device: torch.device, mp_policy: MixedPrecisionPolicy) None
          }
          class FSDPStateContext {
            all_states : List[FSDPState]
            is_last_backward : bool
            iter_forward_root : NoneType, Optional[FSDPState]
            post_backward_final_callback_queued : bool
            post_optim_event : NoneType, Optional[torch.Event]
          }
          class FSDPTest {
            destroy_pg_upon_exit
            file_name
            init_method
            process_group
            rank
            world_size
            run_subtests()
            setUp()
          }
          class FSDPTestModel {
            get_input(device)* Tuple[torch.Tensor, ...]
            get_loss(input, output)* torch.Tensor
            init()* nn.Module
            run_backward(loss)* None
          }
          class FSDPTestMultiThread {
            world_size
            perThreadSetUp()
            perThreadTearDown()
            run_subtests()
            setUp()
          }
          class FXFloatFunctional {
            add(x: Tensor, y: Tensor) Tensor
            add_relu(x: Tensor, y: Tensor) Tensor
            add_scalar(x: Tensor, y: float) Tensor
            cat(x: List[Tensor], dim: int) Tensor
            forward(x)
            matmul(x: Tensor, y: Tensor) Tensor
            mul(x: Tensor, y: Tensor) Tensor
            mul_scalar(x: Tensor, y: float) Tensor
          }
          class FXGraphCacheLoadable {
            fx_graph_cache_key : str
            is_backward()
            load(example_inputs, fx_config: _CompileFxKwargs) CompiledFxGraph
          }
          class FXGraphCacheMiss {
          }
          class FXGraphExtractor {
            input_adapter
            output_adapter
            generate_fx(options: ResolvedExportOptions, model: torch.nn.Module | Callable, model_args: Sequence[Any], model_kwargs: Mapping[str, Any])* torch.fx.GraphModule
            pre_export_passes(options: ResolvedExportOptions, original_model: torch.nn.Module | Callable, fx_module: torch.fx.GraphModule, fx_module_args: Sequence[Any])*
          }
          class FXSymbolicTracer {
            concrete_args : dict[str, Any] | None
            generate_fx(options: _exporter_legacy.ResolvedExportOptions, model: torch.nn.Module | Callable, model_args: Sequence[Any], model_kwargs: Mapping[str, Any]) torch.fx.GraphModule
            pre_export_passes(options: _exporter_legacy.ResolvedExportOptions, original_model: torch.nn.Module | Callable, fx_module: torch.fx.GraphModule, fx_module_args: Sequence[Any])
          }
          class FailOnRecompileLimitHit {
          }
          class FailedMatch {
            args : tuple
            format_string : str
            kwargs : dict
          }
          class FailingOptimizer {
            step(closure)
          }
          class FailureReport {
            data : Dict[str, Any]
            failure_type
            xfail : bool
            print(str_to_filename: Dict[str, str]) str
          }
          class FailureType {
            name
          }
          class FailuresDict {
            data : Dict
            path : str
            get_status(qualname: str, test_name: str) str
            load(path) 'FailuresDict'
            save() None
            set_status(qualname: str, test_name: str, status: str)
          }
          class FakeBackwardCFunction {
            real
            saved_tensors : List[torch.Tensor]
          }
          class FakeClass {
            module
            name
            fake_new()
          }
          class FakeClassRegistry {
            clear() None
            deregister(full_qualname: str) Any
            get_impl(full_qualname: str) Any
            has_impl(full_qualname: str) bool
            register(full_qualname: str, fake_class) None
          }
          class FakeCompiledAutogradEngine {
            exec_final_callbacks(final_callbacks: List[Callable[[], None]]) None
            queue_callback(final_callbacks: List[Callable[[], None]], cb: Callable[[], None]) None
          }
          class FakeContainsTensor {
            t
            get()
          }
          class FakeCopyMode {
            fake_mode
          }
          class FakeFoo {
            x : int
            y : int
            add_tensor(z)
          }
          class FakeImplCtx {
            create_unbacked_symint() torch.SymInt
            new_dynamic_size() torch.SymInt
          }
          class FakeImplHolder {
            kernel : NoneType, Optional[Kernel]
            lib : NoneType, Optional[torch.library.Library]
            qualname : str
            register(func: Callable, source: str) RegistrationHandle
          }
          class FakeIndentedBuffer {
          }
          class FakeItemVariable {
            need_unwrap
            from_tensor_variable(tensor_variable)
          }
          class FakeObject {
            args
            module
            name
            state : NoneType
            pp_format(printer, obj, stream, indent, allowance, context, level)
          }
          class FakeQuantPerChannel {
            backward(ctx, gy)
            forward(ctx, input, scales, zero_points, axis, quant_min, quant_max)
          }
          class FakeQuantize {
            activation_post_process
            ch_axis : int
            dtype
            is_dynamic : bool
            is_per_channel
            qscheme
            quant_max : int
            quant_min : int
            scale
            zero_point
            calculate_qparams()
            extra_repr()
            forward(X)
          }
          class FakeQuantizeBase {
            fake_quant_enabled
            observer_enabled
            calculate_qparams()*
            disable_fake_quant()
            disable_observer()
            enable_fake_quant(enabled: bool) None
            enable_observer(enabled: bool) None
            forward(x)*
            with_args()
          }
          class FakeRootModule {
          }
          class FakeScriptMethod {
            method_name : str
            schema : Optional[torch.FunctionSchema]
            self_fake_obj
          }
          class FakeScriptObject {
            real_obj
            script_class_name : str
            wrapped_obj : Any
          }
          class FakeSequential {
            forward(x: torch.Tensor) torch.Tensor
          }
          class FakeSparsity {
            mask
            forward(x)
            state_dict()
          }
          class FakeStore {
          }
          class FakeStructuredSparsity {
            forward(x)
            state_dict()
          }
          class FakeTensor {
            constant : Optional[Tensor]
            device
            fake_device
            fake_mode
            item_memo
            names
            nested_int_memo
            nonzero_memo
            real_tensor : Optional[Tensor]
            unique_memo
            from_tensor(t: Tensor, fake_mode: FakeTensorMode) FakeTensor
            get_nested_int() torch.SymInt
            tolist() Any
          }
          class FakeTensorConfig {
            debug
          }
          class FakeTensorConverter {
            constant_storage_mapping : Dict[StorageWeakRef, List[ReferenceType]]
            export : bool
            meta_converter
            tensor_memo
            add_constant_storage_mapping(fake_tensor: FakeTensor) None
            from_meta_and_device(fake_mode: FakeTensorMode, t: Tensor, device: torch.device) FakeTensor
            from_real_tensor(fake_mode: FakeTensorMode, t: Tensor, make_constant: bool, shape_env: Optional[ShapeEnv]) FakeTensor
            invalidate_constant_aliases(tensor: Tensor) None
            set_tensor_memo(t: Tensor, v: FakeTensor) None
          }
          class FakeTensorMeta {
            is_nested : bool
            tensor_size : Tuple[Union[int, torch.SymInt], ...]
            tensor_storage_offset : Union[int, torch.SymInt]
            tensor_stride : Tuple[Union[int, torch.SymInt], ...]
            dim() int
            from_fake(fake) 'FakeTensorMeta'
            size() Tuple[Union[int, torch.SymInt], ...]
            storage_offset() Union[int, torch.SymInt]
            stride() Tuple[Union[int, torch.SymInt], ...]
          }
          class FakeTensorMode {
            allow_fallback_kernels : bool
            allow_meta : bool
            allow_non_fake_inputs : bool
            allow_scalar_outputs : bool
            avoid_device_init
            cache : Dict[_DispatchCacheKey, _DispatchCacheEntry]
            cache_bypasses : Dict[str, int]
            cache_crosscheck_enabled
            cache_enabled
            cache_hits : int
            cache_misses : int
            enter_stack : List[Tuple[bool, Optional[TorchDispatchMode], Optional[bool]]]
            epoch : int
            fake_tensor_converter
            in_kernel_invocation : bool
            lift_fns : dict
            nt_tensor_id_counter : int
            nt_tensor_id_initial_count : int
            propagate_real_tensors : bool
            shape_env : Optional[ShapeEnv]
            stack
            static_shapes : bool
            cache_clear() None
            cache_info() DispatchCacheInfo
            can_run_unsafe_fallback(func: OpOverload) bool
            cpp_meta_supports_symint(func: OpOverload) bool
            create_symbolic_nested_int() torch.SymInt
            dispatch(func: OpOverload, types: Sequence[Type], args: Sequence[object], kwargs: Mapping[str, object]) object
            from_tensor(tensor: Tensor) FakeTensor
            invalidate_written_to_constants(func: OpOverload, flat_arg_fake_tensors: Sequence[FakeTensor], args: Sequence[object], kwargs: Mapping[str, object]) None
            is_infra_mode() bool
            is_our_fake(t: object) TypeGuard[FakeTensor]
            may_turn_const(t: Tensor) bool
            reset_nt_tensor_id_counter() None
            validate_and_convert_non_fake_tensors(func: OpOverload, converter: FakeTensorConverter, flat_args: Sequence[object], args_spec: TreeSpec) Tuple[List[object], List[FakeTensor]]
            wrap_meta_outputs_with_default_device_logic(r: object, func: OpOverload, flat_args: Sequence[object], device: torch.device) PyTree
          }
          class FakeTensorProp {
            propagate()
            propagate_dont_convert_inputs()
            run_node(n: Node)
          }
          class FakeTensorUpdater {
            graph
            processed_hashes
            hash_node(node: torch.fx.Node)
            incremental_update()
          }
          class FakeWork {
            get_future() Future
            wait(timeout: Optional[timedelta]) bool
          }
          class FakeifyFirstAOTInvocationGuard {
            tc
          }
          class FakifiedOutWrapper {
            fwd_output_strides : Optional[List[List[int]]]
            needs_post_compile : bool
            out_metas : List[torch.Tensor]
            post_compile(compiled_fn, aot_config: AOTConfig)
            pre_compile(fw_module, flat_args, aot_config) Tuple[Callable, List[Tensor], ViewAndMutationMeta]
            set_fwd_output_strides(fwd_output_strides)
          }
          class FallbackKernel {
            alias_names : List[str]
            kwargs : NoneType, dict
            mutation_names : List[str]
            op_overload
            outputs : NoneType, list
            unbacked_bindings : NoneType
            unflatten_args
            use_runtime_dispatch : bool
            apply_constraint()
            codegen(wrapper) None
            codegen_args()
            codegen_unbacked_symbol_defs(wrapper) None
            create(kernel)
            export_extern_kernel_node()
            find_device(tensor_args, example_output)
            get_inputs_that_alias_output()
            get_mutation_names()
            get_unbacked_symbol_defs() OrderedSet[sympy.Symbol]
            has_side_effects()
            tensor_to_layout(output: torch.Tensor)
          }
          class FaultyAgentDistAutogradTest {
            context_cleanup_test_helper(rpc_args, func)
            test_context_cleanup_tensor_with_grad()
            test_verify_backend_options()
          }
          class FaultyAgentRpcTest {
            test_builtin_remote_message_dropped_timeout()
            test_builtin_remote_message_dropped_timeout_to_self()
            test_check_failed_messages()
            test_custom_faulty_messages()
            test_custom_messages_to_delay()
            test_no_faulty_messages()
            test_remote_message_builtin_delay_timeout()
            test_remote_message_builtin_delay_timeout_to_self()
            test_remote_message_dropped_pickle()
            test_remote_message_dropped_pickle_to_self()
            test_remote_message_script_delay_timeout()
            test_remote_message_script_delay_timeout_to_self()
            test_rpc_builtin_timeout()
            test_rpc_script_timeout()
            test_rref_to_here_timeout()
            test_udf_remote_message_delay_timeout()
            test_udf_remote_message_delay_timeout_to_self()
            test_udf_remote_message_dropped_timeout()
            test_udf_remote_message_dropped_timeout_to_self()
            test_verify_backend_options()
          }
          class FaultyRpcAgentTestFixture {
            messages_to_delay : dict
            messages_to_fail : list
            rpc_backend
            rpc_backend_options
            get_shutdown_error_regex()
            get_timeout_error_regex()
            setup_fault_injection(faulty_messages, messages_to_delay)
          }
          class FauxTorch {
            add()
            cat()
            extra_overhead(result)
            matmul()
            mul()
          }
          class FeatureAlphaDropout {
            forward(input: Tensor) Tensor
          }
          class FeatureSet {
            dense_features
            sparse_features
            values
          }
          class FeedForward {
            gelu
            resid_dropout
            w1
            w2
            forward(x)
          }
          class FileBaton {
            fd : NoneType
            lock_file_path
            wait_seconds : float
            release()
            try_acquire()
            wait()
          }
          class FileListerIterDataPipe {
            abspath : bool
            datapipe
            length : int
            masks : Union[str, List[str]]
            non_deterministic : bool
            recursive : bool
          }
          class FileLock {
            region_counter
          }
          class FileOpenerIterDataPipe {
            datapipe : Iterable
            encoding : Optional[str]
            length : int
            mode : str
          }
          class FileSystem {
            fs : Optional[AbstractFileSystem]
            concat_path(path: Union[str, os.PathLike], suffix: str) Union[str, os.PathLike]
            create_stream(path: Union[str, os.PathLike], mode: str) Generator[io.IOBase, None, None]
            exists(path: Union[str, os.PathLike]) bool
            init_path(path: Union[str, os.PathLike]) Union[str, os.PathLike]
            mkdir(path: Union[str, os.PathLike]) None
            rename(path: Union[str, os.PathLike], new_path: Union[str, os.PathLike]) None
            rm_file(path: Union[str, os.PathLike]) None
            validate_checkpoint_id(checkpoint_id: Union[str, os.PathLike]) bool
          }
          class FileSystem {
            concat_path(path: Union[str, os.PathLike], suffix: str) Union[str, os.PathLike]
            create_stream(path: Union[str, os.PathLike], mode: str) Generator[io.IOBase, None, None]
            exists(path: Union[str, os.PathLike]) bool
            init_path(path: Union[str, os.PathLike]) Union[str, os.PathLike]
            mkdir(path: Union[str, os.PathLike]) None
            rename(path: Union[str, os.PathLike], new_path: Union[str, os.PathLike]) None
            rm_file(path: Union[str, os.PathLike]) None
            validate_checkpoint_id(checkpoint_id: Union[str, os.PathLike]) bool
          }
          class FileSystemBase {
            concat_path(path: Union[str, os.PathLike], suffix: str)* Union[str, os.PathLike]
            create_stream(path: Union[str, os.PathLike], mode: str)* Generator[io.IOBase, None, None]
            exists(path: Union[str, os.PathLike])* bool
            init_path(path: Union[str, os.PathLike])* Union[str, os.PathLike]
            mkdir(path: Union[str, os.PathLike])* None
            rename(path: Union[str, os.PathLike], new_path: Union[str, os.PathLike])* None
            rm_file(path: Union[str, os.PathLike])* None
            validate_checkpoint_id(checkpoint_id: Union[str, os.PathLike])* bool
          }
          class FileSystemReader {
            checkpoint_id
            fs
            load_id : str
            path : NoneType, Path
            storage_data : Dict[MetadataIndex, _StorageInfo], dict
            prepare_global_plan(plans: List[LoadPlan]) List[LoadPlan]
            prepare_local_plan(plan: LoadPlan) LoadPlan
            read_data(plan: LoadPlan, planner: LoadPlanner) Future[None]
            read_metadata() Metadata
            reset(checkpoint_id: Union[str, os.PathLike, None]) None
            set_up_storage_reader(metadata: Metadata, is_coordinator: bool) None
            validate_checkpoint_id(checkpoint_id: Union[str, os.PathLike]) bool
          }
          class FileSystemWriter {
            per_thread_copy_ahead : int
            stage(state_dict: STATE_DICT_TYPE) STATE_DICT_TYPE
          }
          class FileTimerClient {
            signal
            acquire(scope_id: str, expiration_time: float) None
            release(scope_id: str) None
          }
          class FileTimerRequest {
            expiration_time : float
            scope_id : str
            signal : int
            version : int
            worker_pid : int
            to_json() str
          }
          class FileTimerServer {
            clear_timers(worker_pids: Set[int]) None
            get_expired_timers(deadline: float) Dict[int, List[FileTimerRequest]]
            get_last_progress_time() int
            is_process_running(pid: int)
            register_timers(timer_requests: List[FileTimerRequest]) None
            run_once() None
            start() None
            stop() None
          }
          class FileWriter {
            event_writer
            add_event(event, step, walltime)
            add_graph(graph_profile, walltime)
            add_onnx_graph(graph, walltime)
            add_summary(summary, global_step, walltime)
            close()
            flush()
            get_logdir()
            reopen()
          }
          class FilterDataFramesPipe {
            filter_fn
            source_datapipe
          }
          class FilterIterDataPipe {
            datapipe : IterDataPipe[_T_co]
            filter_fn : Callable
            input_col : NoneType
          }
          class FilterVariable {
            fn
            index : int
            iterable : Union[List[VariableTracker], VariableTracker]
            has_unpack_var_sequence(tx) bool
            next_variable(tx)
            python_type()
            reconstruct(codegen)
            reconstruct_items(codegen)
            unpack_var_sequence(tx) List['VariableTracker']
          }
          class Final {
          }
          class FisherSnedecor {
            arg_constraints : dict
            df1
            df2
            has_rsample : bool
            mean
            mode
            support
            variance
            expand(batch_shape, _instance)
            log_prob(value)
            rsample(sample_shape: _size) torch.Tensor
          }
          class Fix {
            artifact_changes : List[_artifact_change.ArtifactChange]
            description : Optional[_message.Message]
            properties : Optional[_property_bag.PropertyBag]
          }
          class FixedLayout {
            dtype
            make_indexer() Callable[[Sequence[Expr]], Expr]
          }
          class FixedPointBox {
            value : bool
          }
          class FixedQParamsFakeQuantize {
            scale
            zero_point
            calculate_qparams()
            extra_repr()
          }
          class FixedQParamsObserver {
            dtype
            qscheme
            quant_max : int
            quant_min : int
            scale
            zero_point
            calculate_qparams()
            forward(X)
          }
          class FixedQParamsOpQuantizeHandler {
          }
          class FixedQParamsQuantizationSpec {
            dtype
            is_dynamic : bool
            qscheme : Optional[torch.qscheme]
            quant_max : Optional[int]
            quant_min : Optional[int]
            scale : float
            zero_point : int
          }
          class FixedTritonConfig {
            config : Dict[str, int]
          }
          class Flags {
          }
          class FlatArgsAdapter {
            adapt(target_spec: pytree.TreeSpec, input_spec: pytree.TreeSpec, input_args: List[Any])* List[Any]
          }
          class FlatParamHandle {
            device
            flat_param
            process_group
            rank
            sharded_grad
            uses_sharded_strategy
            world_size
            flat_param_to()
            flatten_tensors(tensors: List[Tensor], aligned_numel: int) Tensor
            flatten_tensors_into_flat_param(tensors: List[Tensor], aligned_numel: int, requires_grad: bool) FlatParameter
            init_flat_param_attributes() None
            is_sharded(tensor: Tensor) bool
            needs_unshard() bool
            param_module_names() Iterator[Tuple[str, str]]
            post_reshard()
            post_unshard()
            pre_unshard() bool
            prepare_gradient_for_backward()
            prepare_gradient_for_optim()
            reshard(free_unsharded_flat_param: bool)
            reshard_grad()
            shard()
            shard_metadata() FlatParamShardMetadata
            shared_param_module_names() Iterator[Tuple[str, str]]
            to_cpu()
            unflatten_as_params() Generator
            unshard()
            unshard_grad()
          }
          class FlatParamShardMetadata {
            param_contiguities : Tuple[bool, ...]
            param_names : Tuple[str, ...]
            param_numels : Tuple[int, ...]
            param_offsets : Tuple[Tuple[int, int], ...]
            param_shapes : Tuple[torch.Size, ...]
            param_strides : Tuple[Tuple[int, ...], ...]
          }
          class FlatParameter {
            data
            grad : NoneType
            requires_grad : bool
          }
          class Flatten {
            input_dims : Sequence[DimSpec]
            inputs() Iterable[DimSpec]
            new(dims: Sequence[DimSpec]) DimSpec
          }
          class Flatten {
            end_dim : int
            start_dim : int
            extra_repr() str
            forward(input: Tensor) Tensor
          }
          class FlattenInputOutputSignature {
            current_node
            flat_results : List[Any]
            matched_output_elements_positions : List[int]
            new_args : list
            old_args_gen
            output(target, args, kwargs)
            placeholder(target, args, kwargs)
            run_node(n)
            transform()
          }
          class FlattenInputWithTreeSpecValidationInputStep {
            apply(model_args: Sequence[Any], model_kwargs: Mapping[str, Any], model: torch.nn.Module | Callable | torch_export.ExportedProgram | None) tuple[Sequence[Any], Mapping[str, Any]]
          }
          class FlattenOutputStep {
            apply(model_outputs: Any, model: torch.nn.Module | Callable | torch_export.ExportedProgram | None) Sequence[Any]
          }
          class FlattenOutputWithTreeSpecValidationOutputStep {
            apply(model_outputs: Any, model: torch.nn.Module | Callable | torch_export.ExportedProgram | None) Sequence[Any]
          }
          class FlattenScriptObjectSource {
            guard_source()
            name()
            reconstruct(codegen)
          }
          class FlexAttentionAutogradOp {
            backward(ctx: Any, grad_out: Tensor, grad_logsumexp: Tensor) Tuple[Optional[Tensor], ...]
            forward(ctx: Any, query: Tensor, key: Tensor, value: Tensor, fw_graph: Callable, joint_graph: Callable, block_mask: Tuple[Any, ...], scale: float, kernel_options: Dict[str, Any], mask_mod_other_buffers: Tuple[Any, ...]) Tuple[torch.Tensor, torch.Tensor]
          }
          class FlexAttentionBackwardHOP {
          }
          class FlexAttentionHOP {
          }
          class FlexAttentionHigherOrderVariable {
            call_function(tx: 'InstructionTranslator', args: 'List[VariableTracker]', kwargs: 'Dict[str, VariableTracker]') 'VariableTracker'
            create_wrapped_node(tx: 'InstructionTranslator', query: 'VariableTracker', fn: 'VariableTracker', fn_name: str)
            normalize_to_args(args, kwargs)
          }
          class FlexibleLayout {
            allow_indexing : bool
            as_exact_strides(exact_strides, allow_padding)
            as_fill_order(order)
            as_same_order(stride)
            as_stride_order(order, allow_padding)
            contiguous_strides(sizes)
            fill_ordered(sizes, order)
            same_ordered(sizes, stride)
            stride_ordered(sizes, order)
            stride_ordered_for_memory_format(sizes, memory_format)
          }
          class FloatFunctional {
            activation_post_process
            add(x: Tensor, y: Tensor) Tensor
            add_relu(x: Tensor, y: Tensor) Tensor
            add_scalar(x: Tensor, y: float) Tensor
            cat(x: List[Tensor], dim: int) Tensor
            forward(x)
            matmul(x: Tensor, y: Tensor) Tensor
            mul(x: Tensor, y: Tensor) Tensor
            mul_scalar(x: Tensor, y: float) Tensor
          }
          class FloatPow {
            is_real : bool
            precedence : int
            eval(base, exp)
          }
          class FloatStorage {
            dtype()
          }
          class FloatStorage {
            dtype()
          }
          class FloatTensorSource {
            guard_source()
            name() str
          }
          class FloatTrueDiv {
            is_real : bool
            precedence : int
            eval(base, divisor)
          }
          class FloorDiv {
            base
            divisor
            is_integer : bool
            nargs : Tuple[int, ...]
            precedence : int
            eval(base: sympy.Integer, divisor: sympy.Integer) Union[sympy.Basic, None]
          }
          class FloorToInt {
            is_integer : bool
            eval(number)
          }
          class FlopCounterMode {
            depth : int
            display : bool
            flop_counts : Dict[str, Dict[Any, int]]
            flop_registry
            mod_tracker
            mode : NoneType, Optional[_FlopCounterMode]
            get_flop_counts() Dict[str, Dict[Any, int]]
            get_table(depth)
            get_total_flops() int
          }
          class FnWithKwargs {
            forward(pos0, tuple0)
          }
          class Fold {
            dilation : Union
            kernel_size : Union
            output_size : Union
            padding : Union
            stride : Union
            extra_repr() str
            forward(input: Tensor) Tensor
          }
          class FoldedGraphModule {
            const_subgraph_module : NoneType
            device_for_folded_attrs : str
            fx_const_folded_attrs_name : Optional[str]
            has_folding_been_run : bool
            run_folding()
          }
          class Foo {
            bar : int
            x
          }
          class FooBackendOptions {
            init_method
          }
          class ForLoopIndexingPattern {
            description : str
            name : str
            visited : Set[int]
            eventTreeTraversal()
            match(event: _ProfilerEvent)
          }
          class ForeachFuncInfo {
            backward_requires_result : bool
            dtypes
            has_no_in_place
            inplace_variant
            method_variant
            name : str
            op
            ref
            ref_inplace
            supports_alpha_param : bool
            supports_scalar_self_arg : bool
            sample_zero_size_inputs(device, dtype, requires_grad)
          }
          class ForeachKernelSchedulerNode {
            ancestors
            enable_autotune : bool
            group : tuple
            group_algorithm_for_combo_kernels : Callable[[Scheduler], List[List[BaseSchedulerNode]]]
            max_order
            min_order
            name_to_node : dict
            node : NoneType
            origins
            read_to_node : dict
            scheduler
            snodes
            unmet_dependencies
            use_custom_partition_algo : bool
            users : List[NodeUser]
            can_fuse(producer: BaseSchedulerNode, consumer: BaseSchedulerNode) bool
            codegen()* None
            combinable_nodes(nodes: List[BaseSchedulerNode]) List[BaseSchedulerNode]
            fuse(producer: BaseSchedulerNode, consumer: BaseSchedulerNode) ForeachKernelSchedulerNode
            get_consumer_subnode_for(producer: BaseSchedulerNode) Optional[BaseSchedulerNode]
            get_first_name() str
            get_nodes() Sequence[BaseSchedulerNode]
            get_producer_subnode_for(consumer: BaseSchedulerNode) Optional[BaseSchedulerNode]
            get_subkernel_nodes() List[BaseSchedulerNode]
            group_nodes_for_combo_kernels(scheduler: Scheduler) List[List[BaseSchedulerNode]]
            is_foreach() bool
            mark_run()* None
            prune_redundant_deps(name_to_fused_node: Dict[str, BaseSchedulerNode]) None
            set_group_algorithm_for_combo_kernels(custom_group_algorithm: Callable[[Scheduler], List[List[BaseSchedulerNode]]]) None
          }
          class ForeachMap {
          }
          class ForeachRightmostArgType {
            name
          }
          class ForeachSampleInput {
            disable_fastpath : bool
            ref_args : Any
          }
          class ForkerIterDataPipe {
          }
          class FormatMode {
            name
          }
          class FormattedTimesMixin {
            cpu_time
            cpu_time_str
            cpu_time_total_str
            cuda_time
            device_time
            device_time_str
            device_time_total_str
            self_cpu_time_total_str
            self_device_time_total_str
          }
          class ForwardHasDefaultArgs {
            backward(ctx, grad_output)
            forward(x, idx)
            jvp(ctx, x_tangent, _)
            setup_context(ctx, inputs, output)
            vmap(info, in_dims, x, idx)
          }
          class FractionalMaxPool2d {
            kernel_size : Union
            output_ratio : Union
            output_size : Union
            return_indices : bool
            forward(input: Tensor)
          }
          class FractionalMaxPool3d {
            kernel_size : Union
            output_ratio : Union
            output_size : Union
            return_indices : bool
            forward(input: Tensor)
          }
          class FrameStateSizeEntry {
            scalar : Union[int, AutoDynamic, AutoUnset]
            size : Union[AutoDynamic, AutoUnset, Tuple[Union[int, AutoDynamic], ...]]
            stride : Union[AutoDynamic, AutoUnset, Tuple[Union[int, AutoDynamic, InferStride], ...]]
            is_size_dynamic(dim: int) bool
            is_stride_dynamic(dim: int) bool
            make_scalar(x: int) FrameStateSizeEntry
            make_size(size: Tuple[int, ...]) FrameStateSizeEntry
            make_tensor(size: Tuple[int, ...], stride: Tuple[int, ...]) FrameStateSizeEntry
            render() str
          }
          class FreeIfNotReusedLine {
            is_reused : bool
            node : Union
            codegen(code: IndentedBuffer) None
            plan(state: MemoryPlanningState) MemoryPlanningLine
          }
          class FreeUnbackedSymbolsOpsHandler {
            symbols : OrderedSet[sympy.Symbol]
            frexp(x)
            indirect_indexing(index_var, size, check, wrap_neg) sympy.Symbol
            masked(mask, body, other) None
            reduction(dtype: torch.dtype, src_dtype: torch.dtype, reduction_type: ReductionType, value: Union[None, Tuple[None, ...]]) Union[None, Tuple[None, ...]]
            scan(dtypes, combine_fn, values)
            sort(dtypes, values, stable, descending)
          }
          class FreeableInputBuffer {
            mpi_buffer
            name : str
            get_name() str
          }
          class Freezer {
            frozen_modules : List[FrozenModule]
            indent : int
            verbose : bool
            compile_file(path: Path, top_package_path: Path)
            compile_package(path: Path, top_package_path: Path)
            compile_path(path: Path, top_package_path: Path)
            compile_string(file_content: str) types.CodeType
            get_module_qualname(file_path: Path, top_package_path: Path) List[str]
            msg(path: Path, code: str)
            write_bytecode(install_root)
            write_frozen(m: FrozenModule, outfp)
            write_main(install_root, oss, symbol_name)
          }
          class FrontendError {
            error_report
            msg
            source_range
          }
          class FrontendTypeError {
          }
          class FrozenDataClassVariable {
            fields : NoneType, dict
            as_proxy()
            create(tx, value, source)
            method_setattr_standard(tx: 'InstructionTranslator', name, value)
          }
          class FrozenModule {
            bytecode : bytes
            c_name : str
            module_name : str
            size : int
          }
          class FrozensetVariable {
            set_items
            as_python_constant()
            call_method(tx, name, args: List[VariableTracker], kwargs: Dict[str, VariableTracker]) 'VariableTracker'
            debug_repr()
            python_type()
            reconstruct(codegen)
          }
          class FsspecReader {
            fs
            path : Path
            validate_checkpoint_id(checkpoint_id: Union[str, os.PathLike]) bool
          }
          class FsspecWriter {
            fs
            path : Path
            validate_checkpoint_id(checkpoint_id: Union[str, os.PathLike]) bool
          }
          class FullOptimStateDictConfig {
            rank0_only : bool
          }
          class FullStateDictConfig {
            rank0_only : bool
          }
          class FullyShardMode {
            name
          }
          class FullyShardedDataParallel {
            module
            training_state : IDLE
            apply(fn: Callable[[nn.Module], None]) 'FullyShardedDataParallel'
            check_is_root() bool
            clip_grad_norm_(max_norm: Union[float, int], norm_type: Union[float, int]) torch.Tensor
            flatten_sharded_optim_state_dict(sharded_optim_state_dict: Dict[str, Any], model: torch.nn.Module, optim: torch.optim.Optimizer) Dict[str, Any]
            forward() Any
            fsdp_modules(module: nn.Module, root_only: bool) List['FullyShardedDataParallel']
            full_optim_state_dict(model: torch.nn.Module, optim: torch.optim.Optimizer, optim_input: Optional[Union[List[Dict[str, Any]], Iterable[torch.nn.Parameter]]], rank0_only: bool, group: Optional[dist.ProcessGroup]) Dict[str, Any]
            get_state_dict_type(module: nn.Module) StateDictSettings
            named_buffers() Iterator[Tuple[str, torch.Tensor]]
            named_parameters() Iterator[Tuple[str, torch.nn.Parameter]]
            no_sync() Generator
            optim_state_dict(model: torch.nn.Module, optim: torch.optim.Optimizer, optim_state_dict: Optional[Dict[str, Any]], group: Optional[dist.ProcessGroup]) Dict[str, Any]
            optim_state_dict_to_load(model: torch.nn.Module, optim: torch.optim.Optimizer, optim_state_dict: Dict[str, Any], is_named_optimizer: bool, load_directly: bool, group: Optional[dist.ProcessGroup]) Dict[str, Any]
            register_comm_hook(state: object, hook: callable)
            rekey_optim_state_dict(optim_state_dict: Dict[str, Any], optim_state_key_type: OptimStateKeyType, model: torch.nn.Module, optim_input: Optional[Union[List[Dict[str, Any]], Iterable[torch.nn.Parameter]]], optim: Optional[torch.optim.Optimizer]) Dict[str, Any]
            scatter_full_optim_state_dict(full_optim_state_dict: Optional[Dict[str, Any]], model: torch.nn.Module, optim_input: Optional[Union[List[Dict[str, Any]], Iterable[torch.nn.Parameter]]], optim: Optional[torch.optim.Optimizer], group: Optional[Any]) Dict[str, Any]
            set_state_dict_type(module: nn.Module, state_dict_type: StateDictType, state_dict_config: Optional[StateDictConfig], optim_state_dict_config: Optional[OptimStateDictConfig]) StateDictSettings
            shard_full_optim_state_dict(full_optim_state_dict: Dict[str, Any], model: torch.nn.Module, optim_input: Optional[Union[List[Dict[str, Any]], Iterable[torch.nn.Parameter]]], optim: Optional[torch.optim.Optimizer]) Dict[str, Any]
            sharded_optim_state_dict(model: torch.nn.Module, optim: torch.optim.Optimizer, group: Optional[dist.ProcessGroup]) Dict[str, Any]
            state_dict_type(module: nn.Module, state_dict_type: StateDictType, state_dict_config: Optional[StateDictConfig], optim_state_dict_config: Optional[OptimStateDictConfig]) Generator
            summon_full_params(module: nn.Module, recurse: bool, writeback: bool, rank0_only: bool, offload_to_cpu: bool, with_grads: bool) Generator
          }
          class FuncAndLocation {
            func : Callable
            location : str
          }
          class FuncTorchInterpreter {
            check_state(state)
            get_state()*
            key()
            level()
            lower()
            process(op, args, kwargs)*
          }
          class Function {
            generate_vmap_rule : bool
            apply()
            vmap(info, in_dims)*
          }
          class FunctionCount {
            count : int
            function : str
          }
          class FunctionCounts {
            inclusive : bool
            truncate_rows : bool
            denoise() 'FunctionCounts'
            filter(filter_fn: Callable[[str], bool]) 'FunctionCounts'
            sum() int
            transform(map_fn: Callable[[str], str]) 'FunctionCounts'
          }
          class FunctionCtx {
            dirty_tensors : tuple
            materialize_grads : bool
            non_differentiable : tuple
            saved_for_forward : tuple
            to_save : tuple
            mark_dirty()
            mark_non_differentiable()
            mark_shared_storage()*
            save_for_backward()
            save_for_forward()
            set_materialize_grads(value: bool)
          }
          class FunctionEvent {
            concrete_inputs : Optional[List[Any]]
            count : int
            cpu_children : List[FunctionEvent]
            cpu_memory_usage : int
            cpu_parent : Optional[FunctionEvent]
            cpu_time_total
            cuda_time_total
            device_index : int
            device_memory_usage : int
            device_resource_id : int
            device_time_total
            device_type
            flops : Optional[int]
            fwd_thread : Optional[int]
            id : int
            input_shapes : Optional[Tuple[int, ...]]
            is_async : bool
            is_legacy : bool
            is_remote : bool
            is_user_annotation : Optional[bool]
            kernels : List[Kernel]
            key
            kwinputs : Optional[Dict[str, Any]]
            name : str
            node_id : int
            scope : int
            self_cpu_memory_usage
            self_cpu_percent : int
            self_cpu_time_total
            self_cuda_memory_usage
            self_cuda_time_total
            self_device_memory_usage
            self_device_time_total
            sequence_nr : int
            stack : Optional[List]
            thread : int
            time_range
            total_cpu_percent : int
            total_device_percent : int
            trace_name : Optional[str]
            use_device : Optional[str]
            append_cpu_child(child)
            append_kernel(name, device, duration)
            set_cpu_parent(parent)
          }
          class FunctionEventAvg {
            count : int
            cpu_children : Optional[List[FunctionEvent]]
            cpu_memory_usage : int
            cpu_parent : Optional[FunctionEvent]
            cpu_time_total : int
            device_memory_usage : int
            device_time_total : int
            device_type
            flops : int
            input_shapes : Optional[List[List[int]]]
            is_async : bool
            is_legacy : bool
            is_remote : bool
            is_user_annotation
            key : Optional[str]
            node_id : int
            scope : Optional[int]
            self_cpu_memory_usage : int
            self_cpu_time_total : int
            self_device_memory_usage : int
            self_device_time_total : int
            stack : Optional[List]
            use_device : Optional[str]
            add(other)
          }
          class FunctionID {
            id : int
          }
          class FunctionIdSet {
            function_ids : Optional[Set[int]]
            function_names : Optional[Dict[int, str]]
            lazy_initializer : Callable[[], Union[Dict[int, str], Set[int]]]
            add(idx: int)
            get_name(idx: int, default: str)
            remove(idx: int)
          }
          class FunctionInfo {
            code : Optional[types.CodeType]
            filename : str
            name : Optional[str]
            py_obj : Optional[object]
          }
          class FunctionInput {
            args : tuple
            kwargs : dict
          }
          class FunctionMeta {
          }
          class FunctionModifiers {
            COPY_TO_SCRIPT_WRAPPER : str
            DEFAULT : str
            EXPORT : str
            IGNORE : str
            UNUSED : str
          }
          class FunctionWithNoFreeVars {
            fn
          }
          class FunctionalCallVariable {
            call_function(tx, args: List[VariableTracker], kwargs: Dict[str, VariableTracker]) VariableTracker
          }
          class FunctionalConv2d {
            bias
            dilation : tuple
            groups : int
            padding : tuple
            stride : tuple
            weight
            forward(x)
            get_example_inputs() Tuple[Any, ...]
          }
          class FunctionalConvReluConvModel {
            conv1
            conv2
            relu
            forward(x)
            get_example_inputs() Tuple[Any, ...]
          }
          class FunctionalConvReluModel {
            conv
            forward(x)
            get_example_inputs() Tuple[Any, ...]
          }
          class FunctionalLinear {
            bias
            weight
            forward(x)
            get_example_inputs() Tuple[Any, ...]
          }
          class FunctionalLinearAddModel {
            linear1
            linear2
            forward(x)
            get_example_inputs() Tuple[Any, ...]
          }
          class FunctionalLinearReluLinearModel {
            linear1
            linear2
            relu
            forward(x)
            get_example_inputs() Tuple[Any, ...]
          }
          class FunctionalLinearReluModel {
            linear
            forward(x)
            get_example_inputs() Tuple[Any, ...]
          }
          class FunctionalModule {
            forward()
          }
          class FunctionalModule {
            names_map : Dict[str, List[str]]
            param_names : Tuple[str, ...]
            stateless_model
            forward(params: Iterable[Tensor]) Any
          }
          class FunctionalModuleWithBuffers {
            all_names_map : dict
            buffer_names : Tuple[str, ...]
            param_names : Tuple[str, ...]
            stateless_model
            forward(params: Iterable[Tensor], buffers: Iterable[Tensor]) Any
          }
          class FunctionalOperatorSupport {
            is_node_supported(submodules: t.Mapping[str, torch.nn.Module], node: torch.fx.Node) bool
          }
          class FunctionalTensor {
            bfloat16
            bool
            byte
            char
            cpu
            double
            elem
            float
            half
            int
            layout
            long
            metadata_fns : list
            commit_update() None
            cuda(device)
            from_functional()
            is_base_tensor() bool
            mark_mutation_hidden_from_autograd() None
            replace_(output) None
            sync() None
            to()
            to_dense()
            to_functional(x)
            tolist() Any
          }
          class FunctionalTensorMetadataEq {
            tensor
          }
          class FunctionalTensorMode {
            enter_stack : list
            export : bool
            is_on_stack : bool
            pre_dispatch : bool
            is_infra_mode() bool
          }
          class Functionalize {
            allow_fake_constant : bool | None
            enable_dynamic_axes : bool
          }
          class FunctionalizeInterpreter {
            functionalize_add_back_views()
            get_state()
            process(op, args, kwargs)
          }
          class FunctionalizedRngRuntimeWrapper {
            return_new_outs : bool
            post_compile(compiled_fn, aot_config: AOTConfig)
            pre_compile(flat_fn, flat_args, aot_config) Tuple[Callable, List[Tensor], ViewAndMutationMeta]
          }
          class FunctoolsPartialVariable {
            args
            func
            keywords
            as_python_constant()
            call_function(tx: 'InstructionTranslator', args: 'List[VariableTracker]', kwargs: 'Dict[str, VariableTracker]') 'VariableTracker'
            call_hasattr(tx: 'InstructionTranslator', name: str) VariableTracker
            get_function()
            guard_as_python_constant()
            reconstruct(codegen)
          }
          class FunctorchFunctionalizeAPI {
            interpreter
            commit_update(tensor) None
            functionalize(inner_f: Callable) Callable
            mark_mutation_hidden_from_autograd(tensor) None
            redispatch_to_next() ContextManager
            replace(input_tensor, output_tensor) None
            sync(tensor) None
            unwrap_tensors(args: Union[torch.Tensor, Tuple[torch.Tensor, ...]]) Union[torch.Tensor, Tuple[torch.Tensor, ...]]
            wrap_tensors(args: Tuple[Any]) Tuple[Any]
          }
          class FunctorchHigherOrderVariable {
            call_function(tx: 'InstructionTranslator', args: 'List[VariableTracker]', kwargs: 'Dict[str, VariableTracker]') 'VariableTracker'
          }
          class FuseCustomConfig {
            preserved_attributes : List[str]
            from_dict(fuse_custom_config_dict: Dict[str, Any]) FuseCustomConfig
            set_preserved_attributes(attributes: List[str]) FuseCustomConfig
            to_dict() Dict[str, Any]
          }
          class FuseHandler {
            fuse(load_arg: Callable, named_modules: Dict[str, torch.nn.Module], fused_graph: Graph, root_node: Node, extra_inputs: List[Any], matched_node_pattern: NodePattern, fuse_custom_config: FuseCustomConfig, fuser_method_mapping: Dict[Pattern, Union[torch.nn.Sequential, Callable]], is_qat: bool)* Node
          }
          class FusedGraphModule {
            preserved_attr_names : Set[str]
          }
          class FusedMovingAvgObsFakeQuantize {
            is_symmetric_quant
            calculate_qparams() Tuple[torch.Tensor, torch.Tensor]
            extra_repr() str
            forward(X: torch.Tensor) torch.Tensor
          }
          class FusedSchedulerNode {
            group
            snodes : List[BaseSchedulerNode]
            users : List[NodeUser]
            add_fake_dep(name: Dep)* None
            can_inplace(read_dep: dependencies.Dep)* bool
            debug_str() str
            debug_str_extra() str
            debug_str_short() str
            fuse(node1: BaseSchedulerNode, node2: BaseSchedulerNode) FusedSchedulerNode
            get_buffer_names() OrderedSet[str]
            get_device() torch.device
            get_first_name() str
            get_name() str
            get_nodes() Sequence[BaseSchedulerNode]
            get_outputs() List[SchedulerBuffer]
            get_template_node() Optional[ir.TemplateBuffer]
            has_aliasing_or_mutation() bool
            is_reduction() bool
            is_split_scan() bool
            is_template() bool
            reorder_loops_by_dep_pair(self_dep: MemoryDep, other_dep: MemoryDep) None
            set_last_usage(future_used_buffers: OrderedSet[str], mutation_real_name: Dict[str, str]) None
            update_mutated_names(renames: Dict[str, str])* None
            used_buffer_names() OrderedSet[str]
            used_or_aliased_buffer_names() OrderedSet[str]
          }
          class FusionGroup {
            inputs : Set
            nodes : Set
            nodes_need_process : Set
            top_node_idx : int
            add_node(node)
          }
          class Future {
            add_done_callback(callback: Callable[[Future[T]], None]) None
            done() bool
            set_exception(result: T) None
            set_result(result: T) None
            then(callback: Callable[[Future[T]], S]) Future[S]
            value() T
            wait() T
          }
          class FutureTypingTest {
            test_future_passed_between_python_and_jit()
            test_future_python_annotation()
          }
          class FuzzedParameter {
            name
            strict : bool
            sample(state)
          }
          class FuzzedSparseTensor {
            sparse_tensor_constructor(size, dtype, sparse_dim, nnz, is_coalesced)
          }
          class FuzzedTensor {
            name
            default_tensor_constructor(size, dtype)
            satisfies_constraints(params)
          }
          class Fuzzer {
            rejection_rate
            take(n)
          }
          class FxCompile {
            codegen_and_compile(gm: GraphModule, example_inputs: Sequence[InputType], inputs_to_check: Sequence[int], graph_kwargs: _CompileFxKwargs)* OutputCode
          }
          class FxGraphCache {
            clear() None
            get_remote_cache() Optional[RemoteCache[JsonDataTy]]
            load_with_key(key: str, debug_lines: List[str], example_inputs: Sequence[InputType], local: bool, remote_cache: Optional[RemoteCache[JsonDataTy]], is_backward: bool, constants: CompiledFxGraphConstants) Tuple[Optional[CompiledFxGraph], Dict[str, Any]]
            prepare_key(gm: torch.fx.GraphModule, example_inputs: Sequence[InputType], fx_kwargs: _CompileFxKwargs, inputs_to_check: Sequence[int], remote: bool) Tuple[Optional[Tuple[str, List[str]]], Dict[str, Any]]
          }
          class FxGraphCachePickler {
            dispatch_table : dict
            fast : bool
            include_non_inlined : bool
            debug_lines(inp: FxGraphHashDetails) List[str]
            dumps(obj: Any) bytes
            get_hash(obj: Any) str
          }
          class FxGraphDrawer {
            dot_graph_shape : NoneType, str
            normalize_args : bool
            get_all_dot_graphs() Dict[str, pydot.Dot]
            get_dot_graph(submod_name) pydot.Dot
            get_main_dot_graph() pydot.Dot
            get_submod_dot_graph(submod_name) pydot.Dot
          }
          class FxGraphHashDetails {
            EXCLUDED_KWARGS : list
            cache_key_tag : str
            cuda_matmul_settings : tuple
            deterministic_algorithms_settings : tuple
            example_inputs : Sequence[InputType]
            fx_kwargs : Dict[str, object]
            gm
            inductor_config
            inputs_to_check : Sequence[int]
            post_grad_custom_post_pass : NoneType
            post_grad_custom_pre_pass : NoneType
            system_info : dict
            torch_version : bytes
            user_defined_triton_source : List[Any]
          }
          class FxNetAccFusionsFinder {
            acc_nodes : Set
            module
            nodes : list
            recursive_add_node(fusion_group: 'FxNetAccFusionsFinder.FusionGroup', inputs: Union[NodeSet, NodeList], visited: Optional[NodeSet])
          }
          class FxNetAccNodesFinder {
            acc_nodes : Set
            allow_non_tensor : bool
            module
            operator_support
            reduce_acc_nodes_non_tensor_input()
            reduce_acc_nodes_non_tensor_input_helper(cpu_worklist: NodeList)
            reduce_acc_nodes_non_tensor_output()
          }
          class FxNetMinimizerBadModuleError {
          }
          class FxNetMinimizerResultMismatchError {
          }
          class FxNetMinimizerRunFuncError {
          }
          class FxNetSplitterInternalError {
          }
          class FxOnnxInterpreter {
            diagnostic_context
            call_function(node: torch.fx.Node, onnxscript_tracer: onnxscript_graph_building.TorchScriptTracingEvaluator, fx_name_to_onnxscript_value: dict[str, onnxscript_graph_building.TorchScriptTensor | tuple[onnxscript_graph_building.TorchScriptTensor, ...]], onnxfunction_dispatcher: onnxfunction_dispatcher.OnnxFunctionDispatcher, fx_graph_module: torch.fx.GraphModule)
            call_method(node: torch.fx.Node)
            call_module(node: torch.fx.Node, parent_onnxscript_graph: onnxscript_graph_building.TorchScriptGraph, fx_name_to_onnxscript_value: dict[str, onnxscript_graph_building.TorchScriptTensor | tuple[onnxscript_graph_building.TorchScriptTensor, ...]], tracer: onnxscript_graph_building.TorchScriptTracingEvaluator, root_fx_graph_module: torch.fx.GraphModule, onnxfunction_dispatcher: onnxfunction_dispatcher.OnnxFunctionDispatcher) None
            get_attr(node: torch.fx.Node, onnxscript_graph: onnxscript_graph_building.TorchScriptGraph, fx_name_to_onnxscript_value: dict[str, onnxscript_graph_building.TorchScriptTensor | tuple[onnxscript_graph_building.TorchScriptTensor, ...]], fx_graph_module: torch.fx.GraphModule)
            output(node: torch.fx.Node, onnxscript_graph: onnxscript_graph_building.TorchScriptGraph, fx_name_to_onnxscript_value: dict[str, onnxscript_graph_building.TorchScriptTensor | tuple[onnxscript_graph_building.TorchScriptTensor, ...]])
            placeholder(node: torch.fx.Node, onnxscript_graph: onnxscript_graph_building.TorchScriptGraph, fx_name_to_onnxscript_value: dict[str, onnxscript_graph_building.TorchScriptTensor | tuple[onnxscript_graph_building.TorchScriptTensor, ...]])
            run(fx_graph_module: torch.fx.GraphModule, onnxfunction_dispatcher: onnxfunction_dispatcher.OnnxFunctionDispatcher, parent_onnxscript_graph: onnxscript_graph_building.TorchScriptGraph | None) onnxscript_graph_building.TorchScriptGraph
            run_node(node, fx_graph_module: torch.fx.GraphModule, onnxfunction_dispatcher: onnxfunction_dispatcher.OnnxFunctionDispatcher, onnxscript_graph: onnxscript_graph_building.TorchScriptGraph, onnxscript_tracer: onnxscript_graph_building.TorchScriptTracingEvaluator, fx_name_to_onnxscript_value: dict[str, onnxscript_graph_building.TorchScriptTensor | tuple[onnxscript_graph_building.TorchScriptTensor, ...]])
          }
          class GELU {
            approximate : str
            extra_repr() str
            forward(input: Tensor) Tensor
          }
          class GLU {
            dim : int
            extra_repr() str
            forward(input: Tensor) Tensor
          }
          class GPUDeviceBenchmarkMixin {
            do_bench(fn) float
          }
          class GRU {
            check_forward_args(input: Tensor, hidden: Tensor, batch_sizes: Optional[Tensor]) None
            forward(input, hx)
            forward_impl(input: Tensor, hx: Optional[Tensor], batch_sizes: Optional[Tensor], max_batch_size: int, sorted_indices: Optional[Tensor]) Tuple[Tensor, Tensor]
            forward_packed(input: PackedSequence, hx: Optional[Tensor]) Tuple[PackedSequence, Tensor]
            forward_tensor(input: Tensor, hx: Optional[Tensor]) Tuple[Tensor, Tensor]
            from_float(mod, use_precomputed_fake_quant)
            from_reference(ref_mod)
            permute_hidden(hx: Tensor, permutation: Optional[Tensor]) Tensor
          }
          class GRU {
            forward(input, hx)
            from_float(mod, weight_qparams_dict)
            get_flat_weights()
            get_quantized_weight_bias_dict()
          }
          class GRU {
            forward(input: Tensor, hx: Optional[Tensor])* Tuple[Tensor, Tensor]
          }
          class GRUCell {
            forward(input: Tensor, hx: Optional[Tensor]) Tensor
            from_float(mod, use_precomputed_fake_quant)
          }
          class GRUCell {
            bias_hh
            bias_ih
            weight_hh
            weight_ih
            forward(input: Tensor, hx: Optional[Tensor]) Tensor
            from_float(mod, weight_qparams_dict)
          }
          class GRUCell {
            forward(input: Tensor, hx: Optional[Tensor]) Tensor
          }
          class Gamma {
            arg_constraints : dict
            concentration
            has_rsample : bool
            mean
            mode
            rate
            support
            variance
            cdf(value)
            entropy()
            expand(batch_shape, _instance)
            log_prob(value)
            rsample(sample_shape: _size) torch.Tensor
          }
          class Gather {
            dst
            work(data)
          }
          class Gather {
            backward(ctx, grad_output)
            forward(ctx, target_device, dim)
          }
          class GatherTraceback {
            cpp : bool
            python : bool
            script : bool
            filter(record)
          }
          class GaussianNLLLoss {
            eps : float
            full : bool
            forward(input: Tensor, target: Tensor, var: Union[Tensor, float]) Tensor
          }
          class GeneralTensorShapeOpQuantizeHandler {
          }
          class GeneratedFileCleaner {
            dirs_to_clean : list
            files_to_clean : set
            keep_intermediates : bool
            makedirs(dn, exist_ok)
            open(fn)
          }
          class GenerationTracker {
            dynamic_classes
            generation : int
            generation_values
            check(obj: Any) bool
            clear() None
            get_generation_value(obj: Any) int
            mark_class_dynamic(cls: Type[torch.nn.Module]) None
            tag(obj: Any) None
          }
          class GenericContextWrappingVariable {
            cm_obj
            enter(tx)
            exit(tx: 'InstructionTranslator')
            exit_on_graph_break()
            fn_name()
            module_name()
            supports_graph_breaks()
          }
          class GenericMeta {
          }
          class GenericTorchDispatchRuleHolder {
            qualname
            find(torch_dispatch_class)
            register(torch_dispatch_class: type, func: Callable) RegistrationHandle
          }
          class GenericView {
            reindex : Callable[..., Any]
            size : List[Expr]
            create(x, new_size, reindex)
            get_size() Sequence[Expr]
            make_reindexer()
            reindex_str() str
          }
          class Geometric {
            arg_constraints : dict
            logits
            mean
            mode
            probs
            support
            variance
            entropy()
            expand(batch_shape, _instance)
            log_prob(value)
            logits()
            probs()
            sample(sample_shape)
          }
          class GetAttrKey {
            name : str
            get(obj: Any) Any
          }
          class GetAttrVariable {
            name
            obj
            py_type : NoneType
            as_proxy()
            as_python_constant()
            call_function(tx: 'InstructionTranslator', args: 'List[VariableTracker]', kwargs: 'Dict[str, VariableTracker]') 'VariableTracker'
            call_method(tx, name, args: List[VariableTracker], kwargs: Dict[str, VariableTracker]) VariableTracker
            const_getattr(tx: 'InstructionTranslator', name)
            create_getattr_proxy(base_proxy: torch.fx.Proxy, attr)
            python_type()
            reconstruct(codegen)
          }
          class GetData {
            backward(ctx, grad_output)
            forward(ctx, self)
          }
          class GetItem {
            index
            input_var
            res
            tensor_size
          }
          class GetItem {
            find_anchor_nodes(ctx: MatchContext, searched: OrderedSet[torch.fx.Node])
          }
          class GetItemSource {
            index : Any
            index_is_slice : bool
            guard_source()
            name()
            reconstruct(codegen)
            unpack_slice()
          }
          class GetItemTensor {
            index_tuple
            input_var
            res
            tensor_size
          }
          class GetMethodMode {
          }
          class GetSetDescriptorVariable {
            desc
            as_python_constant()
            is_python_constant()
            var_getattr(tx: 'InstructionTranslator', name)
          }
          class GlobGroup {
            exclude : list
            include : list
            separator : str
            matches(candidate: str) bool
          }
          class GlobalContext {
            global_state : Dict[str, Tuple[Callable, ...]], dict
            copy_graphstate()
            restore_graphstate(state)
          }
          class GlobalContextCheckpointState {
            global_state : Dict[str, Tuple[Callable, ...]]
            diff(other)
          }
          class GlobalSource {
            global_name : str
            guard_source()
            name()
            reconstruct(codegen)
          }
          class GlobalStateSource {
            guard_source()
            name()
          }
          class GlobalWeakRefSource {
            global_name : str
            guard_source()
            name()
            reconstruct(codegen)
          }
          class GlobalsBridge {
            construct() str
          }
          class GmWrapper {
            gm
            unflatten_fn
            forward()
          }
          class GradIncrementNestingCtxManagerVariable {
            create(tx: 'InstructionTranslator')
            enter(tx)
            exit(tx: 'InstructionTranslator')
          }
          class GradInplaceRequiresGradCtxManagerVariable {
            prev_state
            create(tx: 'InstructionTranslator', target_values)
            enter(tx)
            exit(tx: 'InstructionTranslator')
          }
          class GradInterpreter {
            get_state()
            lift(args, kwargs)
            lower()
            prev_grad_mode()
            process(op, args, kwargs)
          }
          class GradModeVariable {
            call_function(tx: 'InstructionTranslator', args: 'List[VariableTracker]', kwargs: 'Dict[str, VariableTracker]')
            create(tx: 'InstructionTranslator', target_value, initialized)
            enter(tx)
            exit(tx: 'InstructionTranslator')
            fn_name()
            module_name()
          }
          class GradNotSetToNonePattern {
            description : str
            name : str
            url : str
            match(event: _ProfilerEvent)
          }
          class GradScaler {
          }
          class GradScaler {
          }
          class GradScaler {
            get_backoff_factor() float
            get_growth_factor() float
            get_growth_interval() int
            get_scale() float
            is_enabled() bool
            load_state_dict(state_dict: Dict[str, Any]) None
            scale(outputs: torch.Tensor) torch.Tensor
            set_backoff_factor(new_factor: float) None
            set_growth_factor(new_factor: float) None
            set_growth_interval(new_interval: int) None
            state_dict() Dict[str, Any]
            step(optimizer: torch.optim.Optimizer) Optional[float]
            unscale_(optimizer: torch.optim.Optimizer) None
            update(new_scale: Optional[Union[float, torch.Tensor]]) None
          }
          class GradSource {
            member : str
            guard_source()
            name()
            reconstruct(codegen)
          }
          class GradcheckError {
          }
          class GradientEdge {
            node
            output_nr : int
          }
          class GradientToParameterSpec {
            arg : Annotated[TensorArgument, 10]
            parameter_name : Annotated[str, 20]
          }
          class GradientToUserInputSpec {
            arg : Annotated[TensorArgument, 10]
            user_input_name : Annotated[str, 20]
          }
          class Graph {
            nodes
            old_modules : dict
            owning_module
            call_function(the_function: Callable[..., Any], args: Optional[Tuple['Argument', ...]], kwargs: Optional[Dict[str, 'Argument']], type_expr: Optional[Any]) Node
            call_method(method_name: str, args: Optional[Tuple['Argument', ...]], kwargs: Optional[Dict[str, 'Argument']], type_expr: Optional[Any]) Node
            call_module(module_name: str, args: Optional[Tuple['Argument', ...]], kwargs: Optional[Dict[str, 'Argument']], type_expr: Optional[Any]) Node
            create_node(op: str, target: 'Target', args: Optional[Tuple['Argument', ...]], kwargs: Optional[Dict[str, 'Argument']], name: Optional[str], type_expr: Optional[Any]) Node
            eliminate_dead_code(is_impure_node: Optional[Callable[[Node], bool]]) bool
            erase_node(to_erase: Node) None
            find_nodes()
            get_attr(qualified_name: str, type_expr: Optional[Any]) Node
            graph_copy(g: 'Graph', val_map: Dict[Node, Node], return_output_node) 'Optional[Argument]'
            inserting_after(n: Optional[Node])
            inserting_before(n: Optional[Node])
            lint()
            node_copy(node: Node, arg_transform: Callable[[Node], 'Argument']) Node
            on_generate_code(make_transformer: Callable[[Optional[TransformCodeFunc]], TransformCodeFunc])
            output(result: 'Argument', type_expr: Optional[Any])
            output_node() Node
            placeholder(name: str, type_expr: Optional[Any], default_value: Any) Node
            print_tabular()
            process_inputs()
            process_outputs(out)
            python_code(root_module: str) PythonCode
            set_codegen(codegen: CodeGen)
          }
          class Graph {
            description : str | None
            graph : str
            name : str
            sarif() sarif.Graph
          }
          class Graph {
            description : Optional[_message.Message]
            edges : Optional[List[_edge.Edge]]
            nodes : Optional[List[_node.Node]]
            properties : Optional[_property_bag.PropertyBag]
          }
          class Graph {
            ad_matrix : ndarray
            fw_post_order : List[str]
            name2node : Dict[str, Node]
            nodes : List[Node]
            add_node(node: Node) None
          }
          class Graph {
            custom_obj_values : Annotated[Dict[str, CustomObjArgument], 80]
            inputs : Annotated[List[Argument], 10]
            is_single_tensor_return : Annotated[bool, 70]
            nodes : Annotated[List[Node], 30]
            outputs : Annotated[List[Argument], 20]
            sym_bool_values : Annotated[Dict[str, SymBool], 60]
            sym_float_values : Annotated[Dict[str, SymFloat], 90]
            sym_int_values : Annotated[Dict[str, SymInt], 50]
            tensor_values : Annotated[Dict[str, TensorMeta], 40]
          }
          class GraphAppendingTracer {
            graph
            module_stack : OrderedDict
            node_name_to_scope : dict
            scope
          }
          class GraphArg {
            example
            example_strong_ref : Optional[torch.Tensor]
            fake_tensor : Optional[torch._subclasses.fake_tensor.FakeTensor]
            is_tensor : bool
            pass_arg_as_tensor : bool
            source
            erase()
            reconstruct(codegen)
          }
          class GraphArgument {
            graph : Annotated['Graph', 20]
            name : Annotated[str, 10]
          }
          class GraphCompileReason {
            graph_break : bool
            reason : str
            user_stack : List[traceback.FrameSummary]
          }
          class GraphConstructionError {
          }
          class GraphContext {
            at
            block
            env : dict[_C.Value, _C.Value]
            graph
            new_nodes : list[_C.Node]
            opset : int
            original_node
            params_dict : dict[str, _C.IValue]
            values_in_env : set[_C.Value]
            aten_op(operator: str)
            onnxscript_op(onnx_fn)
            op(opname: str)
          }
          class GraphID {
            id : int
          }
          class GraphInfo {
            export_options
            graph
            id : str
            input_args : tuple[Any, ...]
            lower_graph_info : GraphInfo | None
            mismatch_error : AssertionError | None
            params_dict : dict[str, Any]
            pt_outs : Sequence[_NumericType] | None
            upper_graph_info : GraphInfo | None
            all_mismatch_leaf_graph_info() list[GraphInfo]
            clear()
            essential_node_count() int
            essential_node_kinds() set[str]
            export_repro(repro_dir: str | None, name: str | None) str
            find_mismatch(options: VerificationOptions | None)
            find_partition(id: str) GraphInfo | None
            has_mismatch() bool
            pretty_print_mismatch(graph: bool)
            pretty_print_tree()
            verify_export(options: VerificationOptions) tuple[AssertionError | None, torch.Graph, _OutputsType, _OutputsType]
          }
          class GraphInfoPrettyPrinter {
            children_str_lambdas : Mapping[int, str]
            connector_str_lambdas : Mapping[int, str]
            graph_info : GraphInfo | None
            graph_str_lambdas : Mapping[int, str]
            lower_printer : GraphInfoPrettyPrinter | None
            upper_printer : GraphInfoPrettyPrinter | None
            pretty_print()
          }
          class GraphInputMatcher {
            graph_input_ivalues : List[Any]
            graph_input_tensor_ids : List[int]
            tensor_id_to_arg_idx : Dict[int, int]
          }
          class GraphLowering {
            aligned_inputs
            all_codegen_kernel_names
            allocated_constant_name : Dict[str, str]
            aot_mode : bool
            bound_unbacked_symbols
            buffers : List[ir.Buffer]
            bw_donated_idxs : NoneType
            cache_key : str
            cache_linemap : List[Tuple[int, str]]
            cache_path : str
            const_code : Optional[str]
            const_module : Optional['GraphLowering']
            const_output_index : Dict[str, int]
            constant_reprs : Dict[str, str]
            constants : Dict[str, torch.Tensor]
            cpp_wrapper : bool
            creation_time
            current_device : NoneType, Optional[torch.device]
            current_node : NoneType
            device_idxs : OrderedSet[int]
            device_node_mapping : Dict[torch.device, torch.fx.Node]
            device_ops : Optional[DeviceOpOverrides]
            device_type : str
            device_types : OrderedSet[str]
            disable_cudagraphs_reason : Optional[str]
            dynamo_flat_name_to_original_fqn
            effectful_ops : Dict[_EffectType, ir.Buffer]
            example_inputs : Optional[Sequence[object]]
            extern_kernel_nodes : List[ir.ExternKernelNode]
            extern_node_serializer : Callable[[List[ir.ExternKernelNode]], Any]
            extra_traceback : bool
            fake_mode
            folded_constants : OrderedSet[str]
            get_backend_features
            graph_id : Optional[int]
            graph_input_names : List[str]
            graph_inputs : Dict[str, TensorBox]
            graph_inputs_original : Dict[str, InputBuffer]
            graph_outputs : List[ir.IRNode]
            inplaced_to_remove
            inputs_to_check : Optional[Sequence[int]]
            is_backward : bool
            is_const_graph : bool
            is_inference : bool
            layout_opt : NoneType, bool
            lists : Dict[str, List[str]]
            multi_kernel_to_choice : Dict[str, str]
            mutated_buffers
            mutated_input_idxs : List[int]
            mutated_inputs
            name : Optional[str]
            name_to_buffer : Dict[str, ir.Buffer]
            name_to_op : Dict[str, ir.Operation]
            name_to_users : DefaultDict[str, List[ir.IRNode]]
            never_reuse_buffers
            no_fuse_buffer_names
            nodes_prefer_channels_last
            num_channels_last_conv : int
            operations : List[ir.Operation]
            orig_gm
            placeholder_idx : int
            post_grad_graph_id
            ras_by_symbol : Dict[Optional[sympy.Symbol], List[RuntimeAssert]]
            record_multi_kernel_choice : bool
            removed_buffers
            removed_inplace_buffers
            removed_operations
            reuse_shape_env : bool
            scheduler
            seen_subgraphs : Dict[str, ir.Subgraph]
            sizevars
            torchbind_constants : Dict[str, torch._C.ScriptObject]
            user_visible_output_strides : dict
            workspace_id : count
            wrapper_code : Optional[PythonWrapperCodegen]
            zero_dim_cpu_tensor_list
            add_device_info(device: torch.device) None
            add_symbol_graph_input(symbol: sympy.Expr) None
            add_tensor_constant(data: Tensor, name: Optional[str]) TensorBox
            allocate_non_dup_const_name(name: Optional[str], data: Union[Tensor]) str
            call_function(target: Callable, args: Any, kwargs: Dict[str, Any]) Any
            call_method(target: Any, args: Any, kwargs: Any) NoReturn
            call_module(target: Any, args: Any, kwargs: Any) NoReturn
            can_inline_constant(t: torch.Tensor) bool
            codegen() Tuple[str, List[Tuple[int, Node]]]
            codegen_subgraph(parent_graph: 'GraphLowering') None
            codegen_with_cpp_wrapper() Tuple[str, List[Tuple[int, Node]]]
            compile_to_module() ModuleType
            constant_name(name: str, device_override: Optional[torch.device]) str
            count_bytes() Tuple[int, List[Tuple[BaseSchedulerNode, int]], List[Tuple[BaseSchedulerNode, float]]]
            decide_layout_opt(gm: GraphModule) bool
            finalize() None
            find_nodes_prefer_channels_last() OrderedSet[Node]
            get_attr(target: str, args: Tuple[()], kwargs: Dict[str, object]) Union[Constant, TensorBox, ir.Subgraph, TorchBindObject]
            get_buffer(buffer_name: str) Union[ir.TensorBox, ir.Buffer]
            get_current_device_or_throw() torch.device
            get_dtype(buffer_name: str) torch.dtype
            get_numel(buffer_name: str) Union[int, Expr]
            get_original_value_of_constant(name: str) torch.Tensor
            get_output_names() List[str]
            get_training_phase() str
            has_feature(device: Union[torch._inductor.ir.IRNode, device, None], feature: BackendFeature) bool
            init_wrapper_code(is_subgraph: bool, subgraph_name: Optional[str], parent_wrapper_code: Optional[PythonWrapperCodegen]) None
            is_unspec_arg(name: str) bool
            make_subgraph(gm: torch.fx.GraphModule, example_inputs: List[torch.Tensor], subgraph_name: str) 'SubgraphLowering'
            mark_buffer_mutated(name: str) None
            output(target: str, args: Tuple[object], kwargs: Dict[str, object]) None
            placeholder(target: str, args: Tuple[object], kwargs: Dict[str, object]) Union[Expr, TensorBox, None]
            propagate_mutation(fx_node: torch.fx.Node, old_args: Tuple[Any], old_kwargs: Dict[str, Any], new_args: Tuple[Any], new_kwargs: Dict[str, Any]) None
            qualify_name(name: str) str
            register_buffer(buffer: ir.Buffer) str
            register_operation(op: ir.Operation) str
            register_operation_list(operation_names: List[str]) str
            register_users_of(node_output: Union[Iterable[ir.IRNode], ir.IRNode]) None
            run() Any
            run_node(n: torch.fx.Node) object
            save_output_code(code: str)* None
            set_current_device(device: torch.device) Iterator[None]
            set_current_node(node: torch.fx.Node)
            static_sizes_strides(ex: torch.Tensor) Tuple[List[sympy.Expr], List[sympy.Expr]]
            symbolic_sizes_strides(ex: torch.Tensor) Tuple[Sequence[Union[int, Expr]], Sequence[Union[int, Expr]]]
            try_get_buffer(buffer_name: str) Optional[Union[ir.TensorBox, ir.Buffer]]
            try_match_insignificant_strides(tensor: Union[ir.TensorBox, ir.BaseView], meta_strides_inp: Tuple[Union[int, torch.SymInt], ...]) Union[ir.TensorBox, ir.BaseView]
            validate_can_generate_cpp_wrapper() None
            warn_fallback(name: str) None
          }
          class GraphMatchingException {
          }
          class GraphModule {
            code
            compile_subgraph_reason : NoneType
            graph
            meta : Dict[str, Any], dict
            shape_env : NoneType
            training
            add_submodule(target: str, m: torch.nn.Module) bool
            delete_all_unused_submodules() None
            delete_submodule(target: str) bool
            print_readable(print_output, include_stride, include_device, colored)
            recompile() PythonCode
            to_folder(folder: Union[str, os.PathLike], module_name: str)
          }
          class GraphModule {
            graph : Annotated[Graph, 10]
            metadata : Annotated[Dict[str, str], 40]
            module_call_graph : Annotated[List[ModuleCallEntry], 60]
            signature : Annotated[GraphSignature, 50]
          }
          class GraphModuleDeserializer {
            constants : dict
            example_inputs : NoneType, dict
            fake_tensor_mode
            graph
            module
            serialized_name_to_meta : Dict[str, MetaType], dict
            serialized_name_to_node : Dict[str, torch.fx.Node], dict
            shape_env
            signature
            symbol_name_to_range : dict
            symbol_name_to_symbol : Dict[str, sympy.Symbol]
            sympy_functions : dict
            deserialize(serialized_graph_module: GraphModule, serialized_state_dict: Union[Dict[str, torch.Tensor], bytes], constants: Union[Dict[str, Any], bytes], example_inputs: Optional[Union[Tuple[Tuple[torch.Tensor, ...], Dict[str, Any]], bytes]], symbol_name_to_range: Optional[Dict[str, symbolic_shapes.ValueRanges]]) Result
            deserialize_argument_spec(x: Argument) ep.ArgumentSpec
            deserialize_constant_input(inp: ConstantValue) Any
            deserialize_extension_operator(serialized_target: str)
            deserialize_graph(serialized_graph: Graph) torch.fx.Graph
            deserialize_graph_output(output) Optional[Union[torch.fx.Node, int]]
            deserialize_hoo_inputs(inputs: List[NamedArgument])
            deserialize_input(inp: Argument) Any
            deserialize_input_spec(i: InputSpec) ep.InputSpec
            deserialize_inputs(target, serialized_node: Node)
            deserialize_metadata(metadata: Dict[str, str]) Dict[str, Any]
            deserialize_module_call_graph(module_call_graph: List[ModuleCallEntry]) List[ep.ModuleCallEntry]
            deserialize_module_call_signature(module_call_signature: ModuleCallSignature) ep.ModuleCallSignature
            deserialize_multiple_outputs(serialized_node: Node, fx_node: torch.fx.Node) None
            deserialize_node(serialized_node: Node, target: Callable) None
            deserialize_operator(serialized_target: str)
            deserialize_output_spec(o: OutputSpec) ep.OutputSpec
            deserialize_outputs(serialized_node: Node, fx_node: torch.fx.Node)
            deserialize_script_obj_meta(script_obj_meta: CustomObjArgument) ep.CustomObjArgument
            deserialize_signature(sig: GraphSignature) ep.ExportGraphSignature
            deserialize_sym_argument(sym_arg)
            deserialize_sym_bool(s: SymBool) Union[bool, torch.SymBool]
            deserialize_sym_float(s: SymFloat) Union[float, torch.SymFloat]
            deserialize_sym_int(s: SymInt) Union[int, torch.SymInt]
            deserialize_sym_op_inputs(inputs)
            deserialize_sym_op_outputs(serialized_node: Node, fx_node: torch.fx.Node)
            deserialize_tensor_meta(tensor_meta: TensorMeta) FakeTensor
            save_graph_module() Iterator[None]
            sync_fx_node(name: str, fx_node: torch.fx.Node)
          }
          class GraphModuleImpl {
          }
          class GraphModuleOnnxMeta {
            package_info
          }
          class GraphModuleSerializer {
            custom_objs : Dict[str, torch._C.ScriptObject]
            duplicate_getitem_nodes : Dict[str, str]
            graph_signature
            graph_state
            module_call_graph : List[ep.ModuleCallEntry]
            handle_call_function(node: torch.fx.Node)
            handle_get_attr(node)*
            handle_output(node: torch.fx.Node)
            handle_placeholder(node: torch.fx.Node)
            is_sym_bool_arg(arg) bool
            is_sym_float_arg(arg) bool
            is_sym_int_arg(arg) bool
            save_graph_state()
            serialize(graph_module: torch.fx.GraphModule) GraphModule
            serialize_argument_spec(x: ep.ArgumentSpec) Argument
            serialize_graph(graph_module: torch.fx.GraphModule) Graph
            serialize_graph_module_metadata(meta: Dict[str, Any])
            serialize_hoo_inputs(args, kwargs) List[NamedArgument]
            serialize_hoo_outputs(node: torch.fx.Node) List[Argument]
            serialize_input(arg, arg_type: Optional[torch._C.Argument]) Argument
            serialize_input_spec(spec: ep.InputSpec) InputSpec
            serialize_inputs(target: Any, args, kwargs) List[NamedArgument]
            serialize_metadata(node: torch.fx.Node) Dict[str, str]
            serialize_module_call_graph(module_call_graph: List[ep.ModuleCallEntry]) List[ModuleCallEntry]
            serialize_module_call_signature(module_call_signature: ep.ModuleCallSignature) ModuleCallSignature
            serialize_operator(target) str
            serialize_output(name: str, meta_val: Any) Argument
            serialize_output_spec(spec: ep.OutputSpec) OutputSpec
            serialize_outputs(node: torch.fx.Node) List[Argument]
            serialize_script_obj_meta(script_obj_meta: ep.CustomObjArgument) CustomObjArgument
            serialize_signature(sig: ep.ExportGraphSignature) GraphSignature
            serialize_sym_bool_output(name, meta_val) SymIntArgument
            serialize_sym_float_output(name, meta_val) SymFloatArgument
            serialize_sym_int_output(name, meta_val) SymIntArgument
            serialize_sym_op_inputs(op, args) List[NamedArgument]
            serialize_tensor_output(name, meta_val) TensorArgument
          }
          class GraphOutputEntry {
            index : int
            variable
          }
          class GraphPatternEntry {
            handler : Callable[..., Any]
            apply(match: Match, graph: torch.fx.Graph, node: torch.fx.Node) None
          }
          class GraphPy {
            nodes_io : OrderedDict
            nodes_op : list
            scope_name_appeared : list
            shallowest_scope_name : str
            unique_name_to_scoped_name : dict
            append(x)
            find_common_root()
            populate_namespace_from_OP_to_IO()
            printall()
            to_proto()
          }
          class GraphRegionTracker {
            hash_to_duplicates : Dict[str, IdenticalNodes]
            input_pickler
            node_to_duplicates : Dict[Node, IdenticalNodes]
            get_identical_regions(graph: torch.fx.Graph) List[List[Region]]
            track_node(tx: 'InstructionTranslatorBase', node: Node) None
          }
          class GraphSignature {
            input_specs : Annotated[List[InputSpec], 10]
            output_specs : Annotated[List[OutputSpec], 20]
          }
          class GraphSignature {
            backward_signature : Optional[BackwardSignature]
            buffers : List[FQN]
            buffers_to_mutate : Dict[GraphOutputName, FQN]
            in_spec
            input_tokens : List[GraphInputName]
            inputs_to_buffers : Dict[GraphInputName, FQN]
            inputs_to_parameters : Dict[GraphInputName, FQN]
            out_spec
            output_tokens : List[GraphOutputName]
            parameters : List[FQN]
            user_inputs : List[GraphInputName]
            user_inputs_to_mutate : Dict[GraphOutputName, GraphInputName]
            user_outputs : List[GraphOutputName]
            from_tracing_metadata() 'GraphSignature'
          }
          class GraphState {
            custom_obj_values : Dict[str, CustomObjArgument]
            inputs : List[Argument]
            is_single_tensor_return : bool
            nodes : List[Node]
            outputs : List[Argument]
            sym_bool_values : Dict[str, SymBool]
            sym_float_values : Dict[str, SymFloat]
            sym_int_values : Dict[str, SymInt]
            tensor_values : Dict[str, TensorMeta]
          }
          class GraphTransformObserver {
            created_nodes : set
            erased_nodes : set
            gm
            input_dot_graph
            log_url : Optional[str]
            passname : str
            subsystem : Optional[str]
            apply_gm_pass(pass_fn: Callable[[GraphModule], T]) Optional[T]
            apply_graph_pass(pass_fn: Callable[[Graph], T]) Optional[T]
            get_current_pass_count()
            on_node_creation(node)
            on_node_erase(node)
          }
          class GraphTraversal {
            description : Optional[_message.Message]
            edge_traversals : Optional[List[_edge_traversal.EdgeTraversal]]
            immutable_state : Optional[Any]
            initial_state : Optional[Any]
            properties : Optional[_property_bag.PropertyBag]
            result_graph_index : int
            run_graph_index : int
          }
          class GraphTypeChecker {
            env
            traced
            type_check()
            type_check_node(n: Node)
          }
          class Graphed {
            backward(ctx)
            forward(ctx)
          }
          class GridSamplerInterpolation {
            name
          }
          class GroupBatchFusionBase {
            graph_search_options
            fuse(graph, subset)*
            match(node)*
          }
          class GroupFusion {
          }
          class GroupLinearFusion {
            fuse(graph: torch.fx.GraphModule, subset: List[torch.fx.Node])
            match(node: torch.fx.Node) Optional[Tuple[str, bool]]
          }
          class GroupMember {
            NON_GROUP_MEMBER : int
          }
          class GroupNorm {
            bias
            weight
            forward(input)
            from_float(mod, use_precomputed_fake_quant)
          }
          class GroupNorm {
            affine : bool
            bias
            eps : float
            num_channels : int
            num_groups : int
            weight
            extra_repr() str
            forward(input: Tensor) Tensor
            reset_parameters() None
          }
          class GroupNormPerSampleGrad {
            backward(ctx, grad_output)
            forward(ctx, kwarg_names, _)
          }
          class GroupedSchedulerNode {
            snodes : List[BaseSchedulerNode]
            add_fake_dep(fake_dep: Dep) None
            can_fuse(producer: BaseSchedulerNode, consumer: BaseSchedulerNode) bool
            create(snodes: List[BaseSchedulerNode]) GroupedSchedulerNode
            get_buffer_names() OrderedSet[str]
            get_first_name() str
            get_name() str
            get_nodes() Sequence[BaseSchedulerNode]
            get_outputs() List[SchedulerBuffer]
            unpack() List[BaseSchedulerNode]
          }
          class GrouperIterDataPipe {
            buffer_elements : DefaultDict[Any, List], defaultdict
            curr_buffer_size : int
            datapipe : IterDataPipe[_T_co]
            drop_remaining : bool
            group_key_fn : Callable[[_T_co], Any]
            group_size : NoneType
            guaranteed_group_size : NoneType
            keep_key : bool
            max_buffer_size : int
            wrapper_class
            reset() None
          }
          class GroupwiseConv2d {
            conv
            example_inputs()
            forward(x)
          }
          class Guard {
            code_list : Optional[List[str]]
            create_fn : Callable[[GuardBuilderBase, Guard], None]
            guard_types : Optional[List[str]]
            guarded_class_weakref : Optional[type]
            name
            obj_weakref : Optional[object]
            originating_source
            source
            stack : Optional[CapturedTraceback]
            user_stack : Optional[traceback.StackSummary]
            create(builder: GuardBuilderBase)
            inner_create_fn()
            is_fsdp_module()
            is_local()
            is_specialized_nn_module()
            set_export_info(guard_type, guarded_class, code_list, obj_weakref)
            sort_key()
            weakref_to_str(obj_weakref)
          }
          class GuardBuilder {
            argnames : List[str]
            check_fn_manager
            code : List[GuardCodeList]
            guard_manager
            id_matched_objs : Dict[str, ReferenceType[object]]
            id_ref : Callable[[Any, str], str]
            key_order_guarded_dict_ids : set
            lookup_weakrefs : Callable[[object], ReferenceType[object]]
            no_tensor_aliasing_guard_managers : List[GuardManagerWrapper]
            no_tensor_aliasing_names : List[str]
            scope : Dict[str, Dict[str, object]]
            shape_env_code : List[GuardCodeList]
            source_ref : Callable[[Source], str]
            BUILTIN_MATCH(guard: Guard)
            CLOSURE_MATCH(guard: Guard)
            CONSTANT_MATCH(guard: Guard)
            DATA_PTR_MATCH(guard: Guard)
            DEFAULT_DEVICE(guard: Guard)
            DETERMINISTIC_ALGORITHMS(guard: Guard)*
            DICT_CONST_KEYS(guard)
            DICT_CONTAINS(guard: Guard, key: str, invert: bool)
            DICT_KEYS(guard)
            DICT_VERSION(guard: Guard)
            DUAL_LEVEL(guard: Guard)
            DUPLICATE_INPUT(guard, source_b)
            EMPTY_NN_MODULE_HOOKS_DICT(guard)
            EQUALS_MATCH(guard: Guard)
            FSDP_TRAINING_STATE(guard: Guard)*
            FUNCTION_MATCH(guard: Guard)
            FUNCTORCH_STACK_MATCH(guard: Guard)
            GRAD_MODE(guard: Guard)*
            HASATTR(guard: Guard)
            ID_MATCH(guard: Guard)
            NAME_MATCH(guard: Guard)
            NN_MODULE(guard: Guard)
            NOT_NONE_MATCH(guard: Guard, value)
            NOT_PRESENT_IN_GENERIC_DICT(guard: Guard, attr) None
            OBJECT_MUTATION(guard: Guard)
            PYMODULE_MATCH(guard: Guard)
            RANGE_ITERATOR_MATCH(guard)
            SEQUENCE_LENGTH(guard)
            SHAPE_ENV(guard: Guard)
            TENSOR_MATCH(guard: Guard, value)
            TENSOR_SUBCLASS_METADATA_MATCH(guard: Guard)
            TORCH_FUNCTION_STATE(guard: Guard)*
            TUPLE_ITERATOR_LEN(guard)
            TYPE_MATCH(guard: Guard) None
            WEAKREF_ALIVE(guard)
            add_python_lambda_leaf_guard_to_root(code_parts, verbose_code_parts, closure_vars, is_epilogue)
            arg_ref(guard: Union[str, Guard]) str
            get(name: str) Any
            get_global_guard_manager()
            get_guard_manager(guard: Guard)
            get_guard_manager_from_source(source)
            get_guard_manager_type(source, example_value)
            getattr_on_nn_module(source, base_guard_manager, base_example_value, example_value, base_source_name, source_name, guard_manager_enum)
            guard_on_dict_keys_and_ignore_order(example_value, guard)
            guard_on_dict_keys_and_order(value, guard)
            manager_guards_on_keys(mgr_enum)
            requires_key_order_guarding(source)
          }
          class GuardBuilderBase {
          }
          class GuardCodeList {
            code_list : List[str]
            guard
          }
          class GuardEnvExpr {
          }
          class GuardFail {
            orig_code
            reason : str
          }
          class GuardFn {
            args : List[str]
            cache_entry : Optional[CacheEntry]
            closure_vars : Dict[str, object]
            code_parts : List[str]
            extra_state : Optional[ExtraState]
            global_scope : Dict[str, object]
            guard_fail_fn : Optional[Callable[[GuardFail], None]]
            verbose_code_parts : List[str]
          }
          class GuardInstallException {
          }
          class GuardManagerType {
            name
          }
          class GuardManagerWrapper {
            args : NoneType
            cache_entry : NoneType
            closure_vars : NoneType
            code_parts : list
            diff_guard_root : NoneType
            diff_guard_sources : OrderedSet[str], set
            extra_state : NoneType
            global_scope : NoneType, dict
            guard_fail_fn : NoneType
            id_matched_objs : dict
            no_tensor_aliasing_sources : list
            print_no_tensor_aliasing_guard : bool
            root
            verbose_code_parts : NoneType, list
            check(x)
            check_verbose(x)
            clone_with_chosen_sources(chosen_sources)
            collect_diff_guard_sources()
            construct_dict_manager_string(mgr, body)
            construct_manager_string(mgr, body)
            finalize()
            get_guard_lines(guard)
            get_manager_line(guard_manager, accessor_str)
            populate_code_parts_for_debugging()
            populate_diff_guard_manager()
          }
          class GuardOnDataDependentSymNode {
            cond : Basic
          }
          class GuardSource {
            name
            is_fsdp_module() bool
            is_local()
            is_specialized_nn_module() bool
            is_unspecialized_builtin_nn_module() bool
            is_unspecialized_nn_module() bool
          }
          class GuardedCode {
            code
            compile_id : CompileId
            guard_manager
            trace_annotation : str
          }
          class GuardsCheckpointState {
            dynamo_guards : Set[Guard]
            diff(other)
          }
          class GuardsContext {
            aotautograd_guards : List[GuardEnvExpr]
            dynamo_guards
            copy_graphstate()
            restore_graphstate(state)
          }
          class GuardsSet {
            inner : NoneType, set
            add(guard: Guard)
            remove_guards_with_source(source)
            update()
          }
          class Gumbel {
            arg_constraints : dict
            loc
            mean
            mode
            scale
            stddev
            support
            variance
            entropy()
            expand(batch_shape, _instance)
            log_prob(value)
          }
          class HFPretrainedConfigVariable {
            obj
            call_hasattr(tx: 'InstructionTranslator', name: str) 'VariableTracker'
            is_matching_cls(cls)
            is_matching_object(obj)
            var_getattr(tx: 'InstructionTranslator', name: str) 'VariableTracker'
          }
          class HPUTestBase {
            device_type : str
            primary_device : ClassVar[str]
            get_primary_device()
            setUpClass()
          }
          class HSDPMeshInfo {
          }
          class HalfCauchy {
            arg_constraints : dict
            has_rsample : bool
            mean
            mode
            scale
            support
            variance
            cdf(value)
            entropy()
            expand(batch_shape, _instance)
            icdf(prob)
            log_prob(value)
          }
          class HalfNormal {
            arg_constraints : dict
            has_rsample : bool
            mean
            mode
            scale
            support
            variance
            cdf(value)
            entropy()
            expand(batch_shape, _instance)
            icdf(prob)
            log_prob(value)
          }
          class HalfStorage {
            dtype()
          }
          class HalfStorage {
            dtype()
          }
          class HalideCSEVariable {
            undefined_re
            used_dims : Optional[List[sympy.Symbol]]
            index_str(dims)
            subs_str(replacements)
            update_on_args(name, args, kwargs)
          }
          class HalideCodeCache {
            cache : Dict[str, Callable[[], Union[ModuleType, CDLL]]]
            cache_clear : staticmethod
            glue_template_cpp
            glue_template_cuda
            prefix
            standalone_runtime_cuda_init
            build_standalone_runtime() str
            config_hash() str
            find_header(name: str) str
            find_libautoschedule(name: str) str
            generate_halide() Callable[[], Any]
            generate_halide_async(meta: HalideMeta, source_code: str, submit_fn: Any) Callable[[], Any]
          }
          class HalideInputSpec {
            alias_of : Optional[str]
            ctype : str
            name : str
            offset : Optional[str]
            shape : Optional[List[str]]
            stride : Optional[List[str]]
            bindings_type() str
            halide_type() str
            is_buffer() bool
            is_scalar() bool
          }
          class HalideKernel {
            buffer_aliases : Dict[str, List[str]]
            buffer_dimensions : Dict[str, List[DimensionInfo]]
            buffer_offsets : Dict[str, sympy.Expr]
            compute
            dom_renames : Dict[str, Dict[sympy.Symbol, sympy.Symbol]]
            halide_vars : Dict[sympy.Symbol, sympy.Expr]
            has_indirect_indexing : bool
            has_reduction : bool
            index_replacements : Dict[sympy.Expr, sympy.Expr]
            indexing_code_dom
            kexpr : Callable[[sympy.Expr], str]
            loads
            needs_dom_indexing : bool
            overrides
            reduction_renames : Dict[sympy.Symbol, sympy.Symbol]
            stores
            apply_offset_to_dimension(dims, offset)
            call_kernel(name: str, node)
            check_bounds(expr: sympy.Expr, size: sympy.Expr, lower: bool, upper: bool)*
            codegen_kernel(name)
            codegen_rdom(name, vars)
            create_cse_var(name, bounds, dtype)
            dtype_to_str(dtype: torch.dtype) str
            finalize_indexing(indices: Sequence[sympy.Expr])
            generate_assert(check)
            genfunc(line, used_dims) HalideCSEVariable
            halide_argdefs()
            halide_buffer_numel(name: str)
            halide_kernel_meta() HalideMeta
            indexing_to_dimensions(var: str, index: sympy.Expr, is_store: bool)
            install_dims(var, dims, offset, is_store)
            load(name: str, index: sympy.Expr)
            lookup_cse_var(name: str)
            make_index_str(dims, replacements, zero_vars)
            newfunc(used_dims) HalideCSEVariable
            prepare_indexing(index: sympy.Expr)
            reduction(dtype: torch.dtype, src_dtype: torch.dtype, reduction_type: ReductionType, value: Union[CSEVariable, Tuple[CSEVariable, ...]]) Union[CSEVariable, Tuple[CSEVariable, ...]]
            scan(dtypes: Tuple[torch.dtype, ...], combine_fn: Callable[[Tuple[CSEVariable, ...], Tuple[CSEVariable, ...]], Tuple[CSEVariable, ...]], values_orig: Tuple[CSEVariable, ...]) Tuple[CSEVariable, ...]
            setup_dom_indexing()
            sort_used_dims(used_dims)
            store(name: str, index: sympy.Expr, value: CSEVariable, mode: StoreMode) None
            sym_size(sym)
            used_dims_from_index(index: sympy.Expr)
            welford_combine_impl(mean, m2, weight)
          }
          class HalideMeta {
            argtypes : List[HalideInputSpec]
            cuda_device : Optional[int]
            scheduler : Optional[str]
            scheduler_flags : Optional[Dict[str, Union[int, str]]]
            target : str
            args() List[str]
            is_cuda() bool
          }
          class HalideOverrides {
            abs(x)
            acos(x)
            acosh(x)
            asin(x)
            asinh(x)
            atan(x)
            atan2(x, y)
            atanh(x)
            bitwise_and(a, b)
            bitwise_left_shift(a, b)
            bitwise_not(a)
            bitwise_or(a, b)
            bitwise_right_shift(a, b)
            bitwise_xor(a, b)
            ceil(x)
            constant(value, dtype)
            copysign(x, y)
            cos(x)
            cosh(x)
            erf(x)
            erfinv(x)
            exp(x)
            floor(x)
            floordiv(a, b)
            fmod(a, b)
            halide_clamp(value, size, check)
            hypot(x, y)
            index_expr(expr, dtype)
            indirect_indexing(index_var, size, check, wrap_neg)
            int_truediv(a, b)
            isinf(x)
            isnan(x)
            lgamma(x)
            libdevice_exp(x)
            load_seed(name, offset)
            log(x)
            logical_and(a, b)
            logical_not(a)
            logical_or(a, b)
            logical_xor(a, b)
            masked(mask, body, other)
            maximum(a, b)
            minimum(a, b)
            nextafter(x, y)
            pow(a, b)
            rand(seed, offset)
            randint64(seed, offset, low, high)
            randn(seed, offset)
            relu(x)
            round(x)
            rsqrt(x)
            sign(x)
            signbit(x)
            sin(x)
            sinh(x)
            sqrt(x)
            tan(x)
            tanh(x)
            to_dtype(x, dtype: torch.dtype, src_dtype: Optional[torch.dtype], use_compute_types)
            to_dtype_bitcast(x, dtype: torch.dtype, src_dtype: torch.dtype)
            trunc(x)
            truncdiv(a, b)
            where(a, b, c)
          }
          class HalidePrinter {
            cast_float(expr)
            cast_index(expr)
          }
          class HalideScheduling {
            kernel_type
            define_kernel(src_code, node_schedule, kernel)
            get_backend_features(device: torch.device)
          }
          class HandleShardingStrategy {
            name
          }
          class HandleTrainingState {
            name
          }
          class Hardshrink {
            lambd : float
            extra_repr() str
            forward(input: Tensor) Tensor
          }
          class Hardsigmoid {
            inplace : bool
            forward(input: Tensor) Tensor
          }
          class Hardswish {
            forward(input)
            from_float(mod, use_precomputed_fake_quant)
            from_reference(mod, scale, zero_point)
          }
          class Hardswish {
            inplace : bool
            forward(input: Tensor) Tensor
          }
          class Hardtanh {
            inplace : bool
            max_val : float
            min_val : float
            extra_repr() str
            forward(input: Tensor) Tensor
          }
          class HasStaticMethodFromReal {
            from_real(real_obj: torch.ScriptObject)*
          }
          class HealthCheckServer {
            start() None
            stop() None
          }
          class HelperFunctions {
            finalized_helpers : List[str]
            add(template_code: str) str
          }
          class HeuristicType {
            name
          }
          class HierarchicalModelAverager {
            period_process_group_dict : OrderedDict
            step
            warmup_steps : int
            average_parameters(params: Union[Iterable[torch.nn.Parameter], Iterable[Dict[str, torch.nn.Parameter]]])
          }
          class HigherOrderOperator {
            namespace
            non_fallthrough_keys
            cacheable()
            dispatch()
            fallthrough(dispatch_key)
            name()
            py_impl(k: Any) Callable[[_F], _F]
          }
          class HingeEmbeddingLoss {
            margin : float
            forward(input: Tensor, target: Tensor) Tensor
          }
          class HintsWrapper {
          }
          class HintsWrapperHigherOrderVariable {
            call_function(tx, args: 'List[VariableTracker]', kwargs: 'Dict[str, VariableTracker]') 'VariableTracker'
          }
          class HipifyResult {
            current_state
            hipified_path
            status : str
          }
          class HistogramObserver {
            bins : int
            dst_nbins
            histogram
            max_val
            min_val
            upsample_rate : int
            calculate_qparams()
            extra_repr()
            forward(x_orig: torch.Tensor) torch.Tensor
            reset_histogram(x: torch.Tensor, min_val: torch.Tensor, max_val: torch.Tensor) None
          }
          class HolderModule {
          }
          class Hooks {
            guard_export_fn : Optional[Callable[[GuardsSet], None]]
            guard_fail_fn : Optional[Callable[[GuardFail], None]]
          }
          class HopDispatchSetCache {
            hop_cache_map : dict
            get_cache(op: torch._ops.HigherOrderOperator) Optional[HopSubgraphCache]
          }
          class HopSubgraphCache {
            add_autograd_key_entry(identifier: str, key: Callable)*
            add_dynamo_identifier(cache_key: str, identifier: str)*
            add_proxy_dispatch_entry(identifier: str, key: Callable)*
            get_autograd_key_entry(identifier: str)*
            get_dynamo_identifier(cache_key: str)* Optional[str]
            get_proxy_dispatch_entry(identifier: str)*
          }
          class HuberLoss {
            delta : float
            forward(input: Tensor, target: Tensor) Tensor
          }
          class HybridModel {
            ddp_params : tuple
            fc1
            fc2
            non_ddp_params : tuple
            remote_em_rref : RRef
            remote_net_rref : RRef
            forward(input: FeatureSet)
          }
          class IRNode {
            dtype
            origin_node : Optional[torch.fx.Node]
            origins : OrderedSet[Any]
            shape
            traceback : Optional[List[str]]
            codegen_reference(writer: Optional[IndentedBuffer])* str
            common_repr(shorten: bool) Sequence[str]
            constant_to_device(device: torch.device)* IRNode
            current_origins(origins: OrderedSet[Node]) Generator[None, None, None]
            freeze_layout()* None
            freeze_layout_with_exact_strides(exact_strides: List[_IntLike], allow_padding: bool)* None
            freeze_layout_with_fill_order(order: List[int])* None
            freeze_layout_with_same_order(stride: List[_IntLike])* None
            freeze_layout_with_stride_order(order: List[int], allow_padding: bool)* None
            get_defining_op() Optional[Operation]
            get_device() Optional[torch.device]
            get_device_or_error() torch.device
            get_dtype() torch.dtype
            get_inputs_that_alias_output()* Sequence[str]
            get_layout()* Layout
            get_mutation_names()* Sequence[str]
            get_name()* str
            get_numel() Expr
            get_operation_name()* str
            get_origin_node() Optional[torch.fx.Node]
            get_output_spec() OutputSpec
            get_read_names() OrderedSet[str]
            get_read_writes()* dependencies.ReadWrites
            get_reads() OrderedSet[Dep]
            get_reduction_size()* Sequence[sympy.Expr]
            get_reduction_type()* Optional[str]
            get_size()* Sequence[Expr]
            get_storage_numel()* _IntLike
            get_stride()* Sequence[_IntLike]
            get_traceback() Optional[List[str]]
            get_unbacked_symbol_uses()* OrderedSet[Symbol]
            has_exceeded_max_reads() bool
            has_large_inner_fn(threshold: Optional[int]) bool
            has_tensor_output() bool
            is_extern() bool
            is_no_op() bool
            is_zero_elements() bool
            make_indexer()* Callable[[Sequence[Expr]], Expr]
            make_loader()* Callable[[Sequence[Expr]], OpsValue]
            mark_reuse(users: int)* None
            maybe_get_dtype() Optional[torch.dtype]
            maybe_get_layout() Optional[Layout]
            maybe_get_name() Optional[str]
            maybe_get_output_spec() Optional[OutputSpec]
            maybe_get_size() Optional[Sequence[_IntLike]]
            maybe_get_stride() Optional[Sequence[_IntLike]]
            num_reads() int
            realize()* Optional[str]
            realize_hint()* None
            str_helper(lines: Sequence[object], shorten: bool, multiline: bool) str
            unwrap_view()* IRNode
          }
          class Identity {
            precedence : int
          }
          class Identity {
            PRUNING_TYPE : str
            apply(module, name)
            compute_mask(t, default_mask)
          }
          class Identity {
            training
            forward(input: Tensor) Tensor
          }
          class IdentityRemover {
            call_module(target, args, kwargs)
          }
          class IgnoreException {
          }
          class Ignored {
            pretty_print(pp: PatternPrettyPrinter) str
          }
          class ImageHandler {
            imagespec
          }
          class ImplementedSparsifier {
            update_mask(module: nn.Module, tensor_name: str) None
          }
          class Importer {
            modules : Dict[str, ModuleType]
            get_name(obj: Any, name: Optional[str]) Tuple[str, str]
            import_module(module_name: str)* ModuleType
            whichmodule(obj: Any, name: str) str
          }
          class InconsistentMetadata {
          }
          class IncorrectUsage {
          }
          class IncrementRecursionCount {
          }
          class IndentedBuffer {
            tabwidth : int
            clear()
            do_indent(offset)
            do_unindent(offset)
            getrawvalue() str
            getvalue() str
            getvaluewithlinemap() tuple[str, list[tuple[int, LineContext]]]
            indent(offset)
            map(func: Callable[[Any], Any]) IndentedBuffer
            newline()
            prefix()
            splice(other_code, strip)
            writeline(line)
            writelines(lines)
          }
          class IndentedBufferWithPrefix {
            tabwidth : int
            prefix()
            writeline(line, skip_prefix)
          }
          class Independent {
            arg_constraints : Dict[str, constraints.Constraint]
            base_dist
            has_enumerate_support
            has_rsample
            mean
            mode
            reinterpreted_batch_ndims
            variance
            entropy()
            enumerate_support(expand)
            expand(batch_shape, _instance)
            log_prob(value)
            rsample(sample_shape: _size) torch.Tensor
            sample(sample_shape)
            support()
          }
          class IndependentTransform {
            base_transform
            bijective
            reinterpreted_batch_ndims
            sign
            codomain()
            domain()
            forward_shape(shape)
            inverse_shape(shape)
            log_abs_det_jacobian(x, y)
            with_cache(cache_size)
          }
          class IndexExprDep {
            index : Expr
            size : Tuple[sympy.Expr, ...]
            var_names : Tuple[sympy.Symbol, ...]
          }
          class IndexExpression {
            maketuple
          }
          class IndexPropVar {
            is_symbolic : bool
            value : Any
            new_symbolic(expr: TypedExpr) 'IndexPropVar'
          }
          class IndexPropagation {
            axioms
            indirect_var_ranges : Dict[sympy.Symbol, sympy.Expr]
            shape_env
            var_to_range : tuple
            fallback(name: Literal['indirect_indexing'], args: Tuple[Any, ...], kwargs: Dict[str, Any]) IndexPropVar
            indirect_indexing(index: Union[Any, IndexPropVar], size: Any, check: bool, wrap_neg) Any
            materialize_expr(expr: sympy.Expr, dtype: torch.dtype) Any
            propagate_sympy(name: str, args: Tuple[Any, ...], kwargs: Dict[str, Any]) IndexPropResult
            statically_true(e)
            unwrap(a: Union[Any, IndexPropVar]) Any
            wrap(a) IndexPropResult
          }
          class IndexPutFallback {
            indices
            name
            codegen(wrapper) None
            get_mutation_names()
            get_unbacked_symbol_defs() OrderedSet[sympy.Symbol]
            should_allocate() bool
          }
          class IndexSelect {
            dim_replace
            index
            input_var
            output
            tensor_size
          }
          class IndexingConstant {
            device
            dtype
            index : Any
            constant_to_device(device: torch.device) IRNode
            make_loader() Callable[[Sequence[Expr]], OpsValue]
          }
          class IndexingOptions {
            expand_str : Optional[str]
            index : Expr
            index_str : str
            mask_str : str
            mask_vars : OrderedSet[str]
            has_indirect()
            has_mask()
            has_rindex()
            has_rmask()
            has_tmpmask()
          }
          class InductorChoices {
            can_fuse(scheduler: Scheduler, node1: BaseSchedulerNode, node2: BaseSchedulerNode, shared_data_score: int) bool
            can_fuse_horizontal(scheduler: Scheduler, node1: BaseSchedulerNode, node2: BaseSchedulerNode, shared_data_score: int) bool
            can_fuse_vertical(scheduler: Scheduler, node1: BaseSchedulerNode, node2: BaseSchedulerNode, shared_data_score: int) bool
            reduction_split_factor(device: torch.device, reduction_numel_hint: int, numel_hint: int, inner_reduction: bool) int
            score_fusion(scheduler: Scheduler, node1: BaseSchedulerNode, node2: BaseSchedulerNode) Sortable
            should_use_cooperative_reduction(features: SIMDKernelFeatures) bool
            should_use_persistent_reduction(features: SIMDKernelFeatures, cooperative_reduction: bool) bool
            triton_kernel_kwargs(kernel_cls: Type[TritonKernel], features: SIMDKernelFeatures, groups: List[sympy.Expr], kernel_kwargs: Dict[str, Any]) Dict[str, Any]
            want_no_x_dim(features: SIMDKernelFeatures) bool
          }
          class InferStride {
            dim : int
          }
          class InferenceModeVariable {
            target_values
            create(tx: 'InstructionTranslator', target_value)
            enter(tx)
            exit(tx: 'InstructionTranslator')
            fn_name()
            module_name()
          }
          class InfinityType {
          }
          class InflatableArg {
            fmt : str
            fmt_fn : str
            value : Any
          }
          class Info {
          }
          class InfoProtocol {
          }
          class InliningGeneratorInstructionTranslator {
            generated_items : List[VariableTracker]
            instruction_pointer
            GET_YIELD_FROM_ITER(inst)
            SEND(inst)
            YIELD_FROM(inst)
            YIELD_VALUE(inst: Instruction)
          }
          class InliningInstructionTranslator {
            fake_mode
            instruction_pointer : NoneType
            nn_module_stack
            num_calls
            one_graph
            parent
            symbolic_result : Optional[TensorVariable]
            RETURN_CONST(inst)
            RETURN_VALUE(inst)
            STORE_GLOBAL(inst)
            check_inlineable(func)
            create_call_resume_at(offset)
            get_globals_source_and_value(name)
            inline_call(parent, func, args, kwargs)
            inline_call_(parent, func: VariableTracker, args: List[VariableTracker], kwargs)
            run_ctx_mgr()
            should_compile_partial_graph()
          }
          class InnerModule {
            fc1
            fc2
            relu1
            relu2
            forward(x)
            fuse_modules()
          }
          class InnerTensorKey {
            inner_name : str
            get(o: Any) Any
          }
          class InplaceBernoulliFallback {
            name
            codegen(wrapper) None
            get_mutation_names()
            get_unbacked_symbol_defs() OrderedSet[sympy.Symbol]
            should_allocate() bool
          }
          class InplaceCopyFallback {
            name
            codegen(wrapper) None
            create(dst, src, non_blocking: bool)
            get_mutation_names()
            get_unbacked_symbol_defs() OrderedSet[sympy.Symbol]
            should_allocate() bool
          }
          class InplaceFunction {
            inplace : bool
          }
          class InplaceableOp {
            extra_check : Callable[[torch.fx.Node], bool]
            inplace_op : Callable[..., Any]
            mutated_arg : int
          }
          class InplacedBuffer {
            inner_name : str
            other_names : List[str]
          }
          class InputAdaptStep {
            apply(model_args: Sequence[Any], model_kwargs: Mapping[str, Any], model: torch.nn.Module | Callable | torch_export.ExportedProgram | None) tuple[Sequence[Any], Mapping[str, Any]]
          }
          class InputAdapter {
            append_step(step: InputAdaptStep) None
            apply() Sequence[int | float | bool | str | torch.Tensor | torch.dtype | None]
          }
          class InputAliasInfo {
            is_leaf : bool
            keep_input_mutations : bool
            mutates_data : bool
            mutates_metadata : bool
            mutates_storage_metadata : bool
            mutation_inductor_storage_resize : bool
            mutation_type
            mutations_hidden_from_autograd : bool
            mutations_under_no_grad_or_inference_mode : bool
            requires_grad : bool
          }
          class InputBuffer {
            num_reads() int
          }
          class InputDescriptor {
            device
            dtype
          }
          class InputDim {
            input_dim : int
          }
          class InputDim {
            dim : int
            input_name : str
          }
          class InputError {
            message
          }
          class InputKind {
            name
          }
          class InputPickler {
            dispatch_table : dict
            fast : bool
            dumps(obj: Any) bytes
          }
          class InputReader {
            args : list
            pbar : NoneType
            store : NoneType
            storage(storage_hash, nbytes)
            symint(val)
            tensor(storage, shape, stride)
          }
          class InputSpec {
            buffer : Annotated[InputToBufferSpec, 30]
            constant_input : Annotated[InputToConstantInputSpec, 60]
            custom_obj : Annotated[InputToCustomObjSpec, 50]
            parameter : Annotated[InputToParameterSpec, 20]
            tensor_constant : Annotated[InputToTensorConstantSpec, 40]
            token : Annotated[InputTokenSpec, 70]
            user_input : Annotated[UserInputSpec, 10]
          }
          class InputSpec {
            arg : Union
            kind
            persistent : Optional[bool]
            target : Optional[str]
          }
          class InputToBufferSpec {
            arg : Annotated[TensorArgument, 10]
            buffer_name : Annotated[str, 20]
            persistent : Annotated[bool, 30]
          }
          class InputToConstantInputSpec {
            name : Annotated[str, 10]
            value : Annotated[ConstantValue, 20]
          }
          class InputToCustomObjSpec {
            arg : Annotated[CustomObjArgument, 10]
            custom_obj_name : Annotated[str, 20]
          }
          class InputToParameterSpec {
            arg : Annotated[TensorArgument, 10]
            parameter_name : Annotated[str, 20]
          }
          class InputToTensorConstantSpec {
            arg : Annotated[TensorArgument, 10]
            tensor_constant_name : Annotated[str, 20]
          }
          class InputTokenSpec {
            arg : Annotated[TokenArgument, 10]
          }
          class InputVariableMixin {
          }
          class InputWeightEqualizationDetector {
            ACTIVATION_PREFIX : str
            CHANNEL_KEY : str
            COMP_METRIC_KEY : str
            DEFAULT_PRE_OBSERVER_NAME : str
            DEFAULT_RECOMMEND_INPUT_WEIGHT_CHANNEL_RATIO : float
            GLOBAL_MAX_KEY : str
            GLOBAL_MIN_KEY : str
            INPUT_STR : str
            PER_CHANNEL_MAX_KEY : str
            PER_CHANNEL_MIN_KEY : str
            RECOMMENDED_KEY : str
            SUPPORTED_MODULES : Set[Callable]
            THRESHOLD_KEY : str
            WEIGHT_PREFIX : str
            WEIGHT_STR : str
            ch_axis : int
            ratio_threshold : float
            determine_observer_insert_points(prepared_fx_model: GraphModule) Dict[str, Dict[str, Any]]
            generate_detector_report(model: GraphModule) Tuple[str, Dict[str, Any]]
            get_detector_name() str
            get_qconfig_info(model) Dict[str, DetectorQConfigInfo]
          }
          class InputWriter {
            save_dir
            seen_storages : dict
            storage_counter : count
            store : NoneType
            const(name) None
            lines()
            storage(untyped_storage) str
            symint(name, val) None
            tensor(name, t) None
            unsupported(name, arg)
          }
          class InputsKernel {
            inputs : List[Buffer]
            get_read_writes() dependencies.ReadWrites
            get_reads() OrderedSet[Dep]
            is_extern() bool
            num_reads() int
            unwrap_storage(inputs)
            unwrap_storage_for_input(x: IRNode) IRNode
          }
          class InsertTypePromotion {
            interpreter
          }
          class InspectBoundArgumentsVariable {
            bound_arguments : BoundArguments
            bound_arguments_var
            defaults : Dict[str, VariableTracker]
            packed_vars : set
            signature
            call_method(tx, name, args: 'List[VariableTracker]', kwargs: 'Dict[str, VariableTracker]') 'VariableTracker'
            reconstruct(codegen)
            var_getattr(tx: 'InstructionTranslator', name: str) 'VariableTracker'
          }
          class InspectParameterVariable {
            value
            var_getattr(tx: 'InstructionTranslator', name: str) 'VariableTracker'
          }
          class InspectSignatureVariable {
            fn
            inspected
            parameters : list
            signature
            call_method(tx, name, args: 'List[VariableTracker]', kwargs: 'Dict[str, VariableTracker]') 'VariableTracker'
            create(callable)
            reconstruct(codegen)
            var_getattr(tx: 'InstructionTranslator', name: str) 'VariableTracker'
          }
          class InstanceNorm1d {
            bias
            weight
            forward(input)
            from_float(mod, use_precomputed_fake_quant)
            from_reference(mod, scale, zero_point)
          }
          class InstanceNorm1d {
          }
          class InstanceNorm2d {
            bias
            weight
            forward(input)
            from_float(mod, use_precomputed_fake_quant)
            from_reference(mod, scale, zero_point)
          }
          class InstanceNorm2d {
          }
          class InstanceNorm3d {
            bias
            weight
            forward(input)
            from_float(mod, use_precomputed_fake_quant)
            from_reference(mod, scale, zero_point)
          }
          class InstanceNorm3d {
          }
          class InstanceNormDecompSkip {
            new_op_name : str
            new_op_schema : str
            onnxscript_function
            op_callable
            abstract(input, weight, bias, running_mean, running_var, use_input_stats: bool, momentum: float, eps: float, cudnn_enabled: bool)
            register(export_options: torch.onnx.ExportOptions)
            unregister()
          }
          class InstanceNormPerSampleGrad {
            backward(ctx, grad_output)
            forward(ctx, kwarg_names, _)
          }
          class Instruction {
            arg : Optional[int]
            argval : Any
            exn_tab_entry : Optional[InstructionExnTabEntry]
            is_jump_target : bool
            offset : Optional[int]
            opcode : int
            opname : str
            positions : Optional['dis.Positions']
            starts_line : Optional[int]
            target : Optional['Instruction']
            short_inst_repr() str
          }
          class InstructionExnTabEntry {
            depth : int
            end : str
            lasti : bool
            start : str
            target : str
          }
          class InstructionTranslator {
            debug_locals : List[Tuple[VariableTracker, List[VariableTracker]]]
            export
            instruction_pointer : NoneType
            one_graph : bool
            symbolic_locals : dict, tuple
            symbolic_torch_function_state
            RETURN_CONST(inst)
            RETURN_VALUE(inst)
            create_call_resume_at(inst)
            current_tx() 'InstructionTranslator'
            get_example_value(source: Source)
            run()
            set_current_tx()
            should_compile_partial_graph()
            symbolic_locals_contain_module_class()
          }
          class InstructionTranslatorBase {
            BINARY_ADD
            BINARY_AND
            BINARY_FLOOR_DIVIDE
            BINARY_LSHIFT
            BINARY_MATRIX_MULTIPLY
            BINARY_MODULO
            BINARY_MULTIPLY
            BINARY_OR
            BINARY_POWER
            BINARY_REMAINDER
            BINARY_RSHIFT
            BINARY_SUBSCR
            BINARY_SUBTRACT
            BINARY_TRUE_DIVIDE
            BINARY_XOR
            BUILD_MAP_UNPACK_WITH_CALL
            BUILD_TUPLE_UNPACK_WITH_CALL
            DICT_UPDATE
            INPLACE_ADD
            INPLACE_AND
            INPLACE_FLOOR_DIVIDE
            INPLACE_LSHIFT
            INPLACE_MATRIX_MULTIPLY
            INPLACE_MODULO
            INPLACE_MULTIPLY
            INPLACE_OR
            INPLACE_POWER
            INPLACE_REMAINDER
            INPLACE_RSHIFT
            INPLACE_SUBTRACT
            INPLACE_TRUE_DIVIDE
            INPLACE_XOR
            JUMP_ABSOLUTE
            JUMP_BACKWARD
            JUMP_BACKWARD_NO_INTERRUPT
            JUMP_FORWARD
            JUMP_IF_FALSE_OR_POP
            JUMP_IF_TRUE_OR_POP
            LOAD_CLOSURE
            POP_JUMP_BACKWARD_IF_FALSE
            POP_JUMP_BACKWARD_IF_TRUE
            POP_JUMP_FORWARD_IF_FALSE
            POP_JUMP_FORWARD_IF_TRUE
            POP_JUMP_IF_FALSE
            POP_JUMP_IF_TRUE
            UNARY_INVERT
            UNARY_NEGATIVE
            UNARY_NOT
            UNARY_POSITIVE
            accept_prefix_inst : bool
            block_stack : List[BlockStackEntry]
            code_options : Dict[str, Any]
            current_instruction
            current_speculation : Optional[SpeculationEntry]
            dispatch_table : List[Any]
            distributed_state : Optional[DistributedState]
            exec_recorder : Optional[ExecutionRecorder]
            exn_vt_stack : List[VariableTracker]
            export : bool
            f_builtins : Dict[str, Any]
            f_code
            f_globals : Dict[str, Any]
            f_locals : Dict[str, Any]
            fake_mode
            generic_context_manager_depth : int
            inconsistent_side_effects : bool
            indexof : Dict[Instruction, int]
            inline_depth : int
            instruction_pointer : Optional[int]
            instructions : List[Instruction]
            is_non_empty_graph
            kw_names : Optional[ConstantVariable]
            lineno : int
            next_instruction
            nn_module_stack : Dict[str, Tuple[str, Type[Any]]]
            nn_modules_globals_vt
            num_calls : Dict[str, int]
            one_graph : bool
            output
            prefix_insts : List[Instruction]
            speculation_log
            stack : List[VariableTracker]
            strict_checks_fn : Optional[Callable[[VariableTracker], bool]]
            symbolic_globals : Dict[str, VariableTracker]
            symbolic_locals : Dict[str, VariableTracker]
            symbolic_torch_function_state
            BEFORE_WITH(inst)
            BEGIN_FINALLY(inst)
            BINARY_OP(inst)
            BUILD_CONST_KEY_MAP(inst)
            BUILD_LIST(inst)
            BUILD_LIST_UNPACK(inst, cls)
            BUILD_MAP(inst)
            BUILD_MAP_UNPACK(inst)
            BUILD_SET(inst)
            BUILD_SLICE(inst)
            BUILD_STRING(inst)
            BUILD_TUPLE(inst)
            BUILD_TUPLE_UNPACK(inst)
            CACHE(inst)*
            CALL(inst)
            CALL_FINALLY(inst)
            CALL_FUNCTION(inst)
            CALL_FUNCTION_EX(inst)
            CALL_FUNCTION_KW(inst)
            CALL_INTRINSIC_1(inst)
            CALL_KW(inst)
            CALL_METHOD(inst)
            CHECK_EXC_MATCH(inst)
            COMPARE_OP(inst)
            CONTAINS_OP(inst)
            CONVERT_VALUE(inst)
            COPY(inst)
            COPY_FREE_VARS(inst)
            DELETE_ATTR(inst)
            DELETE_FAST(inst)
            DELETE_SUBSCR(inst)
            DICT_MERGE(inst)
            DUP_TOP(inst)
            DUP_TOP_TWO(inst)
            END_FINALLY(inst)
            END_FOR(inst)
            END_SEND(inst)
            FORMAT_SIMPLE(inst)
            FORMAT_VALUE(inst)
            FORMAT_WITH_SPEC(inst)
            FOR_ITER(inst)
            GEN_START(inst)
            GET_ITER(inst)
            GET_LEN(inst)
            IMPORT_FROM(inst)
            IMPORT_NAME(inst)
            IS_OP(inst)
            JUMP_IF_NOT_EXC_MATCH(inst)
            KW_NAMES(inst)
            LIST_APPEND(inst)
            LIST_EXTEND(inst)
            LIST_TO_TUPLE(inst)
            LOAD_ASSERTION_ERROR(inst)
            LOAD_ATTR(inst)
            LOAD_ATTR_SUPER(inst)
            LOAD_CONST(inst)
            LOAD_DEREF(inst)
            LOAD_FAST(inst)
            LOAD_FAST_AND_CLEAR(inst)
            LOAD_FAST_CHECK(inst)
            LOAD_GLOBAL(inst)
            LOAD_METHOD(inst)
            LOAD_METHOD_SUPER(inst)
            LOAD_SUPER_ATTR(inst)
            MAKE_CELL(inst)
            MAKE_FUNCTION(inst)
            MAP_ADD(inst)
            MATCH_KEYS(inst)
            MATCH_MAPPING(inst)
            MATCH_SEQUENCE(inst)
            NOP(inst)*
            POP_BLOCK(inst)
            POP_EXCEPT(inst)
            POP_FINALLY(inst)
            POP_TOP(inst)
            PRECALL(inst)*
            PUSH_EXC_INFO(inst)
            PUSH_NULL(inst)
            RAISE_VARARGS(inst)
            RERAISE(inst)
            RESUME(inst)
            RETURN_GENERATOR(inst)
            ROT_FOUR(inst)
            ROT_THREE(inst)
            ROT_TWO(inst)
            SETUP_EXCEPT(inst)
            SETUP_FINALLY(inst)
            SETUP_LOOP(inst)
            SETUP_WITH(inst)
            SET_ADD(inst)
            SET_FUNCTION_ATTRIBUTE(inst)
            SET_UPDATE(inst)
            STORE_ATTR(inst)
            STORE_DEREF(inst)
            STORE_FAST(inst)
            STORE_GLOBAL(inst)
            STORE_SUBSCR(inst)
            SWAP(inst)
            TO_BOOL(inst)
            UNPACK_EX(inst)
            UNPACK_SEQUENCE(inst)
            WITH_CLEANUP_FINISH(inst)
            WITH_CLEANUP_START(inst)
            append_prefix_inst(inst)
            calc_package()
            call_function(fn: VariableTracker, args: List[VariableTracker], kwargs: Dict[str, VariableTracker])
            cell_and_freevars()
            cellvars()
            check_if_exc_matches()
            create_call_resume_at(offset)
            exception_handler(raised_exception)
            format_frame_summary(additional_stack_frames)
            frame_summary()
            freevars()
            get_line_of_code_header(lineno)
            get_log_starts_line_log_str()
            import_source(module_name)
            inline_user_function_return(fn, args, kwargs)
            is_co_filename_from_nn_modules()
            is_non_empty_graph()
            jump(inst)
            load_builtin(inst)
            load_builtin_from_argval(argval)
            mark_inconsistent_side_effects()
            maybe_has_backedge()
            pop() VariableTracker
            popn(n: int) List[VariableTracker]
            prune_dead_locals()
            push(val: Optional[VariableTracker])
            push_many(vals: List[VariableTracker])
            resolve_name(name, package, level)
            run()
            run_ctx_mgr()
            setup_or_before_with(inst)
            should_compile_partial_graph() bool
            speculate() SpeculationEntry
            starts_line(lineno)
            step()
            step_graph_break(continue_inst)
            store_attr_graph_break(inst)
            store_global_weakref_by_id(prefix, value)
            strict_translation_mode(check_fn: Callable[[VariableTracker], bool])
            update_block_stack(inst)
          }
          class IntInfinity {
            is_commutative : bool
            is_comparable : bool
            is_extended_positive : bool
            is_extended_real : bool
            is_integer : bool
            is_number : bool
            is_prime : bool
            ceiling()
            floor()
          }
          class IntStorage {
            dtype()
          }
          class IntStorage {
            dtype()
          }
          class IntTrueDiv {
            is_real : bool
            precedence : int
            eval(base, divisor)
          }
          class Integer {
          }
          class Intermediate {
            idx : int
            fake() bool
          }
          class IntermediateValueDebuggingLevel {
            name
          }
          class InternalError {
          }
          class InternalMatch {
            anchors : List[Node]
            name_node_map : Dict[str, Node]
            nodes_map : Dict[Node, Node]
            placeholder_nodes : List[Node]
            returning_nodes : List[Node]
          }
          class InternalTorchDynamoError {
          }
          class Interpreter {
            args_iter : Iterator[Any]
            env : Dict[Node, Any], NoneType, dict
            extra_traceback : bool
            garbage_collect_values : bool
            graph
            module
            name : str
            submodules : dict
            user_to_last_uses : Dict[Node, List[Node]]
            boxed_run(args_list)
            call_function(target: 'Target', args: Tuple[Argument, ...], kwargs: Dict[str, Any]) Any
            call_method(target: 'Target', args: Tuple[Argument, ...], kwargs: Dict[str, Any]) Any
            call_module(target: 'Target', args: Tuple[Argument, ...], kwargs: Dict[str, Any]) Any
            fetch_args_kwargs_from_env(n: Node) Tuple[Tuple, Dict]
            fetch_attr(target: str)
            get_attr(target: 'Target', args: Tuple[Argument, ...], kwargs: Dict[str, Any]) Any
            map_nodes_to_values(args: Argument, n: Node) Argument
            output(target: 'Target', args: Tuple[Argument, ...], kwargs: Dict[str, Any]) Any
            placeholder(target: 'Target', args: Tuple[Argument, ...], kwargs: Dict[str, Any]) Any
            run() Any
            run_node(n: Node) Any
          }
          class InterpreterModule {
            arg_names : list
            graph
            graph_module : Optional[torch.fx.GraphModule]
            finalize()
            forward()
            print_readable(print_output, include_stride, include_device, colored)
          }
          class InterpreterModuleDispatcher {
            call_modules()
            forward()
            print_readable(print_output, include_stride, include_device, colored)
          }
          class InterpreterShim {
            current_node : NoneType
            extra_traceback : bool
            fetch_attr
            graph
            module
            submodules
            run()
            run_node(n: torch.fx.Node) Any
          }
          class Interval {
            end
            start
            elapsed_us()
          }
          class Interval {
            end : int
            queue_depth : int
            start : int
          }
          class Invalid {
          }
          class InvalidBackend {
          }
          class InvalidCxxCompiler {
          }
          class InvalidExportOptionsError {
          }
          class InvalidNodeBase {
          }
          class InvalidVecISA {
          }
          class InvalidVersion {
          }
          class InverseGamma {
            arg_constraints : dict
            concentration
            has_rsample : bool
            mean
            mode
            rate
            support
            variance
            entropy()
            expand(batch_shape, _instance)
          }
          class Invocation {
          }
          class Invocation {
            account : Optional[str]
            arguments : Optional[List[str]]
            command_line : Optional[str]
            end_time_utc : Optional[str]
            environment_variables : Optional[Any]
            executable_location : Optional[_artifact_location.ArtifactLocation]
            execution_successful : bool
            exit_code : Optional[int]
            exit_code_description : Optional[str]
            exit_signal_name : Optional[str]
            exit_signal_number : Optional[int]
            machine : Optional[str]
            notification_configuration_overrides : Optional[List[_configuration_override.ConfigurationOverride]]
            process_id : Optional[int]
            process_start_failure_message : Optional[str]
            properties : Optional[_property_bag.PropertyBag]
            response_files : Optional[List[_artifact_location.ArtifactLocation]]
            rule_configuration_overrides : Optional[List[_configuration_override.ConfigurationOverride]]
            start_time_utc : Optional[str]
            stderr : Optional[_artifact_location.ArtifactLocation]
            stdin : Optional[_artifact_location.ArtifactLocation]
            stdout : Optional[_artifact_location.ArtifactLocation]
            stdout_stderr : Optional[_artifact_location.ArtifactLocation]
            tool_configuration_notifications : Optional[List[_notification.Notification]]
            tool_execution_notifications : Optional[List[_notification.Notification]]
            working_directory : Optional[_artifact_location.ArtifactLocation]
          }
          class InvokeSubgraph {
            name
            operands : Optional[List[TensorBox]]
            outputs : Optional[List[MultiOutput]]
            subgraph : Optional[Subgraph]
            codegen(wrapper) None
            create(subgraph: Subgraph, operands)
          }
          class InvokeSubgraphAutogradOp {
            backward(ctx)
            forward(ctx, fw_graph, bw_graph, identifier, num_fw_outs)
          }
          class InvokeSubgraphCache {
            autograd_cache : Dict[str, Callable]
            dynamo_identifiers : Dict[str, str]
            proxy_dispatch_cache : Dict[str, Callable]
            add_autograd_key_entry(identifier: str, key: Callable)
            add_dynamo_identifier(cache_key: str, identifier: str)
            add_proxy_dispatch_entry(identifier: str, key: Callable)
            get_autograd_key_entry(identifier: str)
            get_dynamo_identifier(cache_key: str) Optional[str]
            get_proxy_dispatch_entry(identifier: str)
          }
          class InvokeSubgraphHOP {
          }
          class InvokeSubgraphHigherOrderVariable {
            call_function(tx: 'InstructionTranslator', args: 'List[VariableTracker]', kwargs: 'Dict[str, VariableTracker]') 'VariableTracker'
            install_subgraph_in_output_graph(tx, fn_vt, fn_args_vt, kwargs, body_gmod, attr_name)
          }
          class IsInputHandler {
            base_idx
            unwrap_out
          }
          class IsNonOverlappingAndDenseIndicator {
            is_integer : bool
            eval()
          }
          class IterDataPipe {
            functions : Dict[str, Callable]
            getstate_hook : Optional[Callable]
            reduce_ex_hook : Optional[Callable]
            repr_hook : Optional[Callable]
            str_hook : Optional[Callable]
            register_datapipe_as_function(function_name, cls_to_register, enable_df_api_tracing)
            register_function(function_name, function)
            reset()* None
            set_getstate_hook(hook_fn)
            set_reduce_ex_hook(hook_fn)
          }
          class IterableDataset {
          }
          class IterableWrapperIterDataPipe {
            deepcopy : bool
            iterable
          }
          class IterationRanges {
            divisor
            is_reduction
            kernel
            length
            name : str
            numel : Expr
            prefix : str
            root
            symt
            var_list : List[sympy.Symbol]
            var_ranges : Dict[sympy.Symbol, sympy.Expr]
            symbol()
          }
          class IterationRangesEntry {
            codegen
            expr : Expr
            name
            parent
            cache_clear()
            precomputed_args()
            set_name(name)
          }
          class IterationRangesRoot {
            grid_dim
            has_zdim
            index : int
            is_loop
            nodes : Dict[sympy.Expr, IterationRangesEntry]
            pid_cache : Optional[Dict[str, str]]
            tensor_dim
            cache_clear()
            construct(lengths: List[sympy.Expr])
            construct_entries(lengths: List[sympy.Expr])
            index_sym()
            lookup(divisor, length)
            vars_and_sizes(index: sympy.Expr)
          }
          class IteratorDecorator {
            datapipe
            iterator
            iterator_id
            self_and_has_next_method
          }
          class IteratorVariable {
            force_unpack_var_sequence(tx) List[VariableTracker]
            has_force_unpack_var_sequence(tx) bool
            next_variable(tx)
          }
          class ItertoolsVariable {
            value
            as_python_constant()
            call_function(tx: 'InstructionTranslator', args: 'List[VariableTracker]', kwargs: 'Dict[str, VariableTracker]') 'VariableTracker'
          }
          class JitCommonTestCase {
            assertAutodiffNode(graph, should_autodiff_node, nonfusible_nodes, fusible_nodes)
            assertExportImport(trace, inputs)
            assertExportImportModule(m, inputs)
            autoDiffErrorMessage(should_autodiff_node, nodes_not_in_diff_graph, fusion_nodes_not_found, non_fusible_nodes_being_fused, fusion_nodes_found, nodes_in_diff_graph)
            checkShapeAnalysis(out_sizes: Union[List[int], List[List[int]]], traced_graph, assert_propagation, constant_prop)
            createFunctionFromGraph(trace)
            getExportImportCopy(m, also_test_file, map_location)
            runAndSaveRNG(func, inputs, kwargs)
          }
          class JitDistAutogradTest {
            test_dist_backward()
            test_get_gradients()
            test_jit_fork_within_context()
            test_restore_context_after_swtich_to_jit_thread()
          }
          class JitFaultyAgentRpcTest {
            test_remote_timeout_to_here_in_jit()
            test_rref_timeout_pickle_in_jit()
            test_rref_timeout_pickle_script_func()
            test_rref_to_here_timeout_in_jit()
            test_timeout_in_python()
            test_timeout_in_torchscript_function()
          }
          class JitRpcOpTest {
            test_all_kwargs_are_populated_by_defaults()
            test_args_and_kwargs_contain_different_types()
            test_args_kwargs_are_neither_passed()
            test_call_python_function_remotely_from_script_not_supported()
            test_call_script_function_that_not_exists_remotely_from_script()
            test_call_script_function_that_raises_remotely_from_script()
            test_kwargs_not_passed()
            test_less_than_needed_args_are_specified()
            test_more_than_needed_args_are_specified()
            test_no_kwargs_are_populated_by_defaults()
            test_some_kwargs_are_populated_by_defaults()
            test_unexepected_kwarg_is_specified()
          }
          class JitRpcTest {
            test_add_done_callback()
            test_async_function_remote()
            test_async_function_remote_multi()
            test_async_function_simple()
            test_async_function_wrong_decorator_order()
            test_async_function_wrong_return_type()
            test_async_function_wrong_return_type_remote()
            test_async_script_throw()
            test_async_script_udf()
            test_call_fork_in_jit_with_profiling()
            test_call_rpc_with_profiling()
            test_callback_chain()
            test_callback_simple()
            test_callback_with_exception()
            test_create_script_module_on_remote()
            test_load_script_module_with_pickled_rref()
            test_record_function_jit_end_callbacks_with_fork()
            test_record_function_on_caller_rpc_async()
            test_remote_script_module()
            test_remote_script_throw()
            test_remote_script_udf()
            test_rpc_async_jit_profiled()
            test_rpc_torchscript_record_function()
            test_rref_jit_pickle_not_supported()
            test_torchscript_function()
            test_torchscript_function_exception()
            test_torchscript_functions_not_supported()
          }
          class JitScalarType {
            name
            dtype() torch.dtype
            from_dtype(dtype: torch.dtype | None) JitScalarType
            from_onnx_type(onnx_type: int | _C_onnx.TensorProtoDataType | None) JitScalarType
            from_value(value: None | torch._C.Value | torch.Tensor, default) JitScalarType
            onnx_compatible() bool
            onnx_type() _C_onnx.TensorProtoDataType
            scalar_name() ScalarName
            torch_name() TorchName
          }
          class JitTestCase {
            assertAllFused(graph, except_for)
            assertExpectedGraph(trace)
            assertExpectedONNXGraph(g)
            assertGraphContains(graph, kind, consider_subgraphs)
            assertGraphContainsExactly(graph, kind, num_kind_nodes, consider_subgraphs)
            assertRaisesRegexWithHighlight(exception, regex, highlight)
            checkBailouts(model, inputs, expected)
            checkModule(nn_module, args)
            checkScript(script, inputs, name, optimize, inputs_requires_grad, capture_output, frames_up, profiling, atol, rtol)
            checkScriptRaisesRegex(script, inputs, exception, regex, name, outputs, capture_output, frames_up, profiling)
            checkTrace(func, reference_tensors, input_tensors, drop, allow_unused, verbose, inputs_require_grads, check_tolerance, export_import, _force_outplace, grad_atol, grad_rtol)
            clearHooks()
            emitFunctionHook(func)
            emitModuleHook(module)
            getExportImportCopyWithPacking(m, also_test_file, map_location)
            get_frame_vars(frames_up)
            run_pass(name, trace)
            setHooks()
            setUp()
            tearDown()
          }
          class JitTraceConvertStrategy {
          }
          class JitTypeTraceConfig {
            s
            code_filter() Optional[CodeFilter]
            trace_logger() JitTypeTraceStoreLogger
            trace_store() CallTraceStore
          }
          class JitTypeTraceStore {
            trace_records : Dict[str, list]
            add(traces: Iterable[CallTrace])
            analyze(qualified_name: str) Dict
            consolidate_types(qualified_name: str) Dict
            filter(qualified_name: str, qualname_prefix: Optional[str], limit: int) List[CallTraceThunk]
            get_args_types(qualified_name: str) Dict
          }
          class JitTypeTraceStoreLogger {
            log(trace: CallTrace) None
          }
          class Join {
            notify_join_context(joinable: Joinable)
          }
          class JoinHook {
            main_hook()* None
            post_hook(is_last_joiner: bool)* None
          }
          class Joinable {
            join_device
            join_process_group
            join_hook()* JoinHook
          }
          class JointOutputResult {
            captured_grads : List[Optional[TensorBox]]
            captured_grads_compute : List[ComputedBuffer]
            grad_input
            mutated_grads : List[TensorBox]
          }
          class JvpIncrementNestingCtxManagerVariable {
            create(tx: 'InstructionTranslator')
            enter(tx)
            exit(tx: 'InstructionTranslator')
          }
          class JvpInterpreter {
            get_state()
            lift(args, kwargs)
            lower()
            prev_fwd_grad_mode()
            process(op, args, kwargs)
          }
          class K {
            size : int
            stride : int
          }
          class KLDivLoss {
            log_target : bool
            forward(input: Tensor, target: Tensor) Tensor
          }
          class KeepModules {
            is_leaf_module(_: torch.nn.Module, __: str) bool
          }
          class Kernel {
            args
            assert_function
            compute : NoneType
            cse
            current_node : NoneType
            inplace_update_buffers : dict
            inplaced_to_remove
            kernel_name : NoneType
            load_format : NoneType
            loads : NoneType
            min_elem_per_thread : int
            must_keep_buffers
            name : str
            newvar_prefix : str
            node_to_bounds : Optional[Dict[torch.fx.Node, ValueRanges[Any]]]
            num_load : int
            num_reduction : int
            overrides : Optional[Callable[[OpsHandler[Any]], OpsHandler[Any]]]
            removed_buffers
            store_buffer_names
            store_format : NoneType
            stores : NoneType
            suffix : str
            bucketize(values: CSEVariable, boundaries: Tuple[str, sympy.Expr, sympy.Expr, sympy.Expr], boundary_indices: CSEVariable, indexing_dtype: torch.dtype, right: bool, sorter: Optional[Tuple[str, sympy.Expr]], sorter_indices: Optional[CSEVariable])* CSEVariable
            check_bounds(expr: sympy.Expr, size: sympy.Expr, lower: bool, upper: bool)*
            create_cse_var()
            index_to_str(index: sympy.Expr)* str
            indirect_assert(var: Union[CSEVariable, str], lower: Optional[str], upper: Optional[str], mask: Optional[Union[CSEVariable, str]]) str
            indirect_load(name: str, index: sympy.Expr)
            load(name: str, index: sympy.Expr)* CSEVariable
            reduction(dtype: torch.dtype, src_dtype: torch.dtype, reduction_type: ReductionType, value: Union[CSEVariable, Tuple[CSEVariable, ...]])* Union[CSEVariable, Tuple[CSEVariable, ...]]
            remove_buffer(name: str) None
            remove_inplace_buffer(name: str) None
            remove_kernel_local_buffers() None
            rename_indexing(index) sympy.Expr
            scan(dtypes: Tuple[torch.dtype, ...], combine_fn: Callable[[Tuple[CSEVariable, ...], Tuple[CSEVariable, ...]], Tuple[CSEVariable, ...]], values: Tuple[CSEVariable, ...])* Tuple[CSEVariable, ...]
            set_current_node(node)
            sort(dtypes: Tuple[torch.dtype, ...], values: Tuple[CSEVariable, ...], stable: bool, descending: bool)* Tuple[CSEVariable, ...]
            store(name: str, index: sympy.Expr, value: CSEVariable, mode: StoreMode)* None
            store_reduction(name: str, index: sympy.Expr, value: CSEVariable)*
            swap_buffers(lb, cb, sb)
            var_ranges()*
          }
          class Kernel {
            func : Callable
            source : str
          }
          class KernelArgs {
            inplace_buffers : dict
            input_buffers : dict
            output_buffers : dict
            sizevars : dict
            workspace_args : list
            aliases()
            call_names()
            cpp_argdefs()
            input(name)
            is_removed(name)
            live_output_buffers()
            make_inplace(input_name, output_name)
            output(name)
            python_argdefs()
            seed_offset(name, value)
            semaphores(min_size: sympy.Expr)
            size(name)
            workspace(nbytes: sympy.Expr, zero_fill: bool)
            wrap_ptr_arg(buf, dtype)
            wrap_size_arg(size)
          }
          class KernelFormatterHandler {
            output
            parent_handler
            var_counter : count
            getvalue(result)
            ir_to_string(ir_fn, index, rindex) str
            reduction(dtype: torch.dtype, src_dtype: torch.dtype, reduction_type: ReductionType, value: Union[str, Tuple[str, ...]]) Union[str, Tuple[str, ...]]
          }
          class KernelGroup {
            args
            loops_code
            scheduled_nodes : list
            stack : ExitStack
            ws
            call_kernel(wrapper, kernel_name)
            codegen_group(name) str
            finalize_kernel(new_kernel, nodes)
            get_num_args()
            new_kernel(cls)
          }
          class KernelNamespace {
          }
          class KernelSideTable {
            constant_args : Dict[int, Dict[str, Any]]
            id_to_kernel : Dict[int, 'TritonKernelType']
            kernel_to_id : Dict['TritonKernelType', int]
            lock : lock
            add_constant_args(args: Dict[str, Any]) int
            add_kernel(kernel: 'TritonKernelType') int
            get_constant_args(idx: int) Dict[str, Any]
            get_kernel(idx: int) 'TritonKernelType'
            reset_table() None
          }
          class KernelTemplate {
            name : str
            generate()* torch._inductor.ir.ChoiceCaller
            indent_except_first(source: str, num_indents: int, indents_spacing)
            maybe_append_choice(choices)
          }
          class Key {
            device
          }
          class KeyEntry {
            get(parent: Any) Any
          }
          class KeyErrorMessage {
          }
          class KeyErrorMsg {
            value
          }
          class KeyedJaggedTensorVariable {
            is_matching_object(obj)
            var_getattr(tx: 'InstructionTranslator', name)
          }
          class KeywordArg {
            name : str
            pattern_eq(other: Any) bool
          }
          class KinetoStepTracker {
            current_step() int
            erase_step_count(requester: str) bool
            increment_step(requester: str) int
            init_step_count(requester: str)
          }
          class Kumaraswamy {
            arg_constraints : dict
            concentration0
            concentration1
            has_rsample : bool
            mean
            mode
            support
            variance
            entropy()
            expand(batch_shape, _instance)
          }
          class L1Loss {
            forward(input: Tensor, target: Tensor) Tensor
          }
          class L1Unstructured {
            PRUNING_TYPE : str
            amount
            apply(module, name, amount, importance_scores)
            compute_mask(t, default_mask)
          }
          class LBFGS {
            step(closure)
          }
          class LKJCholesky {
            arg_constraints : dict
            concentration
            dim
            support
            expand(batch_shape, _instance)
            log_prob(value)
            sample(sample_shape)
          }
          class LOBPCG {
            A : Optional[Tensor]
            B : Optional[Tensor]
            E
            R
            S
            X
            bparams : Dict[str, bool]
            bvars : Dict[str, bool]
            fparams : Dict[str, float]
            fvars : Dict[str, float]
            iK : Optional[Tensor]
            iparams : Dict[str, int]
            ivars : Dict[str, int]
            method : str
            tracker : NoneType
            tvars : Dict[str, Tensor]
            call_tracker()*
            run()
            stop_iteration()
            update()
            update_converged_count()
            update_residual()
          }
          class LOBPCGAutogradFunction {
            backward(ctx, D_grad, U_grad)
            forward(ctx, A: Tensor, k: Optional[int], B: Optional[Tensor], X: Optional[Tensor], n: Optional[int], iK: Optional[Tensor], niter: Optional[int], tol: Optional[float], largest: Optional[bool], method: Optional[str], tracker: None, ortho_iparams: Optional[Dict[str, int]], ortho_fparams: Optional[Dict[str, float]], ortho_bparams: Optional[Dict[str, bool]]) Tuple[Tensor, Tensor]
          }
          class LPPool1d {
            kernel_size : Union
            stride : Union
            forward(input: Tensor) Tensor
          }
          class LPPool2d {
            kernel_size : Union
            stride : Union
            forward(input: Tensor) Tensor
          }
          class LPPool3d {
            kernel_size : Union
            stride : Union
            forward(input: Tensor) Tensor
          }
          class LRScheduler {
            base_lrs : List[float]
            last_epoch : int
            optimizer
            verbose : bool, str
            get_last_lr() List[float]
            get_lr()* List[float]
            load_state_dict(state_dict: Dict[str, Any])
            print_lr(is_verbose: bool, group: Dict[str, Any], lr: float, epoch: Optional[int])
            state_dict()
            step(epoch: Optional[int])
          }
          class LSTM {
            check_forward_args(input: Tensor, hidden: Tuple[Tensor, Tensor], batch_sizes: Optional[Tensor]) None
            forward(input, hx)
            forward_impl(input: Tensor, hx: Optional[Tuple[Tensor, Tensor]], batch_sizes: Optional[Tensor], max_batch_size: int, sorted_indices: Optional[Tensor]) Tuple[Tensor, Tuple[Tensor, Tensor]]
            forward_packed(input: PackedSequence, hx: Optional[Tuple[Tensor, Tensor]]) Tuple[PackedSequence, Tuple[Tensor, Tensor]]
            forward_tensor(input: Tensor, hx: Optional[Tuple[Tensor, Tensor]]) Tuple[Tensor, Tuple[Tensor, Tensor]]
            from_float(mod, use_precomputed_fake_quant)
            from_reference(ref_mod)
            permute_hidden(hx: Tuple[Tensor, Tensor], permutation: Optional[Tensor]) Tuple[Tensor, Tensor]
          }
          class LSTM {
            check_forward_args(input: Tensor, hidden: Tuple[Tensor, Tensor], batch_sizes: Optional[Tensor])
            forward(input, hx)
            from_float(mod, weight_qparams_dict)
            get_expected_cell_size(input: Tensor, batch_sizes: Optional[Tensor]) Tuple[int, int, int]
            get_flat_weights()
            get_quantized_weight_bias_dict()
            permute_hidden(hx: Tuple[Tensor, Tensor], permutation: Optional[Tensor]) Tuple[Tensor, Tensor]
          }
          class LSTM {
            from_float()*
            from_observed(other)
          }
          class LSTM {
            batch_first : bool
            bias : bool
            bidirectional : bool
            dropout : float
            hidden_size : int
            input_size : int
            layers
            num_layers : int
            qconfig
            training : bool
            forward(x: Tensor, hidden: Optional[Tuple[Tensor, Tensor]])
            from_float(other, qconfig, split_gates)
            from_observed(other)*
          }
          class LSTM {
            check_forward_args(input: Tensor, hidden: Tuple[Tensor, Tensor], batch_sizes: Optional[Tensor])
            forward(input: Tensor, hx: Optional[Tuple[Tensor, Tensor]])* Tuple[Tensor, Tuple[Tensor, Tensor]]
            get_expected_cell_size(input: Tensor, batch_sizes: Optional[Tensor]) Tuple[int, int, int]
            permute_hidden(hx: Tuple[Tensor, Tensor], permutation: Optional[Tensor]) Tuple[Tensor, Tensor]
          }
          class LSTMCell {
            forward(input: Tensor, hx: Optional[Tuple[Tensor, Tensor]]) Tuple[Tensor, Tensor]
            from_float(mod, use_precomputed_fake_quant)
          }
          class LSTMCell {
            bias_hh
            bias_ih
            weight_hh
            weight_ih
            forward(input: Tensor, hx: Optional[Tuple[Tensor, Tensor]]) Tuple[Tensor, Tensor]
            from_float(mod, weight_qparams_dict, use_precomputed_fake_quant)
          }
          class LSTMCell {
            bias : bool
            cell_gate
            cell_state_dtype
            fgate_cx
            fgate_cx_igate_cgate
            forget_gate
            gates
            hgates
            hidden_size : int
            hidden_state_dtype
            igate_cgate
            igates
            initial_cell_state_qparams : Tuple[float, int]
            initial_hidden_state_qparams : Tuple[float, int]
            input_gate
            input_size : int
            ogate_cy
            output_gate
            qconfig
            split_gates : bool
            forward(x: Tensor, hidden: Optional[Tuple[Tensor, Tensor]]) Tuple[Tensor, Tensor]
            from_float(other, use_precomputed_fake_quant, split_gates)
            from_params(wi, wh, bi, bh, split_gates)
            initialize_hidden(batch_size: int, is_quantized: bool) Tuple[Tensor, Tensor]
          }
          class LSTMCell {
            forward(input: Tensor, hx: Optional[Tuple[Tensor, Tensor]]) Tuple[Tensor, Tensor]
          }
          class LSTMLayerNormLinearModel {
            linear
            lstm
            norm
            forward(x: torch.Tensor) Tuple[torch.Tensor, torch.Tensor]
          }
          class LSTMLinearModel {
            linear
            lstm
            forward(input: torch.Tensor) Tuple[torch.Tensor, torch.Tensor]
          }
          class LSTMSaliencyPruner {
            update_mask(module, tensor_name)
          }
          class LSTMwithHiddenDynamicModel {
            lstm
            qconfig
            forward(x, hid)
          }
          class LShift {
            is_integer : bool
            eval(base, shift)
          }
          class LambdaFuture {
            result_fn : Callable[..., Any]
            result() Callable[..., Any]
          }
          class LambdaLR {
            lr_lambdas : List[Callable[[int], float]], list
            optimizer
            get_lr()
            load_state_dict(state_dict)
            state_dict()
          }
          class LambdaSL {
            sl_lambdas : list
            sparsifier
            get_sl()
          }
          class LambdaVariable {
            fn
            call_function(tx: 'InstructionTranslator', args: 'List[VariableTracker]', kwargs: 'Dict[str, VariableTracker]') 'VariableTracker'
          }
          class Language {
            name
          }
          class Laplace {
            arg_constraints : dict
            has_rsample : bool
            loc
            mean
            mode
            scale
            stddev
            support
            variance
            cdf(value)
            entropy()
            expand(batch_shape, _instance)
            icdf(value)
            log_prob(value)
            rsample(sample_shape: _size) torch.Tensor
          }
          class LargeNet {
            fc1
            fc2
            forward(x)
          }
          class LaunchConfig {
            local_addr : Optional[str]
            log_line_prefix_template : Optional[str]
            logs_specs : Optional[LogsSpecs]
            max_nodes : int
            max_restarts : int
            metrics_cfg : Dict[str, str]
            min_nodes : int
            monitor_interval : float
            nproc_per_node : int
            rdzv_backend : str
            rdzv_configs : Dict[str, Any]
            rdzv_endpoint : str
            rdzv_timeout : int
            role : str
            run_id : str
            start_method : str
          }
          class Layer {
            layer_dummy_buf
            layer_dummy_param
          }
          class LayerNorm {
            bias
            weight
            forward(input)
            from_float(mod, use_precomputed_fake_quant)
            from_reference(mod, scale, zero_point)
          }
          class LayerNorm {
            bias
            elementwise_affine : bool
            eps : float
            normalized_shape : Tuple[int, ...]
            weight
            extra_repr() str
            forward(input: Tensor) Tensor
            reset_parameters() None
          }
          class LayerNormPerSampleGrad {
            backward(ctx, grad_output)
            forward(ctx, kwarg_names, _)
          }
          class Layout {
            device
            dtype
            offset : Expr
            size : List[Expr]
            stride : NoneType, Optional[List[Expr]], list
            as_fixed()
            get_device() torch.device
            is_channels_last_contiguous(shape: Sequence[_IntLike], strides: Sequence[_IntLike]) bool
            is_channels_last_stride_ordered()
            is_contiguous() bool
            is_stride_ordered(order) bool
            is_transposed() bool
            make_indexer() Callable[[Sequence[Expr]], Expr]
            pad_strides()
            should_pad_strides()
            storage_size() sympy.Expr
          }
          class Layout {
            name
          }
          class LayoutArg {
            attr : Literal
            dim : int
            node
            symbol : Literal
            matches(node, attr, dim) bool
          }
          class LayoutType {
            name
          }
          class LazyBatchNorm1d {
            cls_to_become
          }
          class LazyBatchNorm2d {
            cls_to_become
          }
          class LazyBatchNorm3d {
            cls_to_become
          }
          class LazyCache {
            source : Any
            value : Any
            vt : Optional[VariableTracker]
            realize() None
          }
          class LazyConv1d {
            bias
            cls_to_become
            out_channels : int
            weight
          }
          class LazyConv2d {
            bias
            cls_to_become
            out_channels : int
            weight
          }
          class LazyConv3d {
            bias
            cls_to_become
            out_channels : int
            weight
          }
          class LazyConvTranspose1d {
            bias
            cls_to_become
            out_channels : int
            weight
          }
          class LazyConvTranspose2d {
            bias
            cls_to_become
            out_channels : int
            weight
          }
          class LazyConvTranspose3d {
            bias
            cls_to_become
            out_channels : int
            weight
          }
          class LazyInstanceNorm1d {
            cls_to_become
          }
          class LazyInstanceNorm2d {
            cls_to_become
          }
          class LazyInstanceNorm3d {
            cls_to_become
          }
          class LazyLinear {
            bias
            cls_to_become
            in_features
            out_features : int
            weight
            initialize_parameters(input) None
            reset_parameters() None
          }
          class LazyModuleMixin {
            cls_to_become : Optional[Type[Any]]
            has_uninitialized_params()
            initialize_parameters()*
          }
          class LazyProxy {
            args : tuple
            fn
            kwargs : dict
            tracer
          }
          class LazyString {
            args : tuple
            func
            kwargs : dict
          }
          class LazySymNodeFormatString {
            fmt_var
            sym_node_var
          }
          class LazyTestBase {
            device_type : str
            setUpClass()
          }
          class LazyTraceHandler {
            root_dir : Optional[str]
            stream : NoneType, _TemporaryFileWrapper
            close()
            emit(record)
          }
          class LazyVal {
          }
          class LazyVariableTracker {
            visit
            clone() VariableTracker
            create(value: Any, source: Any) 'LazyVariableTracker'
            is_realized() bool
            peek_type() type[Any]
            peek_value() Any
            realize() VariableTracker
            realize_all(value: Any, cache: Optional[Dict[int, Tuple[Any, Any]]]) Any
            unwrap() Union[VariableTracker, Self]
          }
          class LeafSpec {
          }
          class LeafSpec {
          }
          class LeafSpecMeta {
          }
          class LeakyReLU {
            forward(input)
            from_float(mod, use_precomputed_fake_quant)
            from_reference(mod, scale, zero_point)
          }
          class LeakyReLU {
            inplace : bool
            negative_slope : float
            extra_repr() str
            forward(input: Tensor) Tensor
          }
          class LearnedHeuristic {
            check_precondition(metadata: AHMetadata, context: AHContext) bool
            get_confidence_threshold() float
            get_decision(context: AHContext, choices: List[Choice]) Optional[Choice]
            get_decisions_ranked(context: AHContext) Optional[List[str]]
            get_name() str
          }
          class LearnedHeuristicController {
            context
            existing_heuristics : Dict[str, List[LearnedHeuristic]]
            heuristics_initialized : bool
            metadata
            get_decision() Optional[Choice]
            get_decisions_ranked(top_k: int) Optional[List[Choice]]
            get_heuristics(name: str) List[LearnedHeuristic]
          }
          class LearnedHeuristicDecision {
            get_best_choices(context: AHContext) Optional[List[Tuple[float, int]]]
            get_choice(idx: int) Optional[str]
            get_decision(context: AHContext, choices: List[Choice]) Optional[Choice]
            get_decisions_ranked(context: AHContext) Optional[List[str]]
          }
          class LearnedHeuristicRegression {
            get_decision(context: AHContext, choices: List[Choice]) Optional[Choice]
            get_feedback(context: AHContext, choice: Choice) float
          }
          class LegacyDynamoStrategy {
          }
          class Level {
            name
          }
          class Library {
            dispatch_key : str
            kind
            m : NoneType, Optional[Any]
            ns
            define(schema, alias_analysis)
            fallback(fn, dispatch_key)
            impl(op_name, fn, dispatch_key)
          }
          class LiftParametersAndBuffersIntoArgsInputStep {
            inputs : tuple[torch.Tensor, ...]
            apply(model_args: Sequence[Any], model_kwargs: Mapping[str, Any], model: torch.nn.Module | Callable | torch_export.ExportedProgram | None) tuple[Sequence[Any], Mapping[str, Any]]
          }
          class LightTracer {
            graph
            module_stack : dict
            node_name_to_scope : dict
            scope
          }
          class LinAlgError {
          }
          class LineContext {
            context : Any
          }
          class Linear {
            in_features
            out_features
            scale : float
            zero_point : int
            bias()
            extra_repr()
            forward(x: torch.Tensor) torch.Tensor
            from_float(mod, use_precomputed_fake_quant)
            set_weight_bias(w: torch.Tensor, b: Optional[torch.Tensor], row_block_size: Optional[int], col_block_size: Optional[int]) None
            weight()
          }
          class Linear {
            in_features
            out_features
            bias()
            extra_repr()
            forward(x: torch.Tensor) torch.Tensor
            from_float(mod, use_precomputed_fake_quant)
            set_weight_bias(w: torch.Tensor, b: Optional[torch.Tensor], row_block_size: Optional[int], col_block_size: Optional[int]) None
            weight()
          }
          class Linear {
            version : int
            extra_repr()
            forward(x)
            from_float(mod, use_precomputed_fake_quant)
            from_reference(ref_qlinear)
          }
          class Linear {
            bias
            weight
            forward(x: torch.Tensor) torch.Tensor
            from_float(float_linear, weight_qparams)
          }
          class Linear {
            in_features
            out_features
            scale : float
            zero_point : int
            bias()
            extra_repr()
            forward(x: torch.Tensor) torch.Tensor
            from_float(mod, use_precomputed_fake_quant)
            from_reference(ref_qlinear, output_scale, output_zero_point)
            set_weight_bias(w: torch.Tensor, b: Optional[torch.Tensor]) None
            weight()
          }
          class Linear {
          }
          class Linear {
            bias
            qconfig : NoneType
            weight
            weight_fake_quant
            forward(input)
            from_float(mod, use_precomputed_fake_quant)
            to_float()
          }
          class Linear {
            bias : NoneType
            buffer
            fc2
            in_features : int
            lin
            out_features : int
            qconfig : NoneType
            weight
            extra_repr() str
            forward(input: Tensor) Tensor
            reset_parameters() None
          }
          class LinearActivation {
            act1
            act2
            linear1
            linear2
            seq
            forward(x: torch.Tensor) torch.Tensor
          }
          class LinearActivationFunctional {
            act1
            linear1
            linear2
            linear3
            seq
            forward(x: torch.Tensor) torch.Tensor
          }
          class LinearAddModel {
            fc1
            fc2
            forward(x)
            get_example_inputs() Tuple[Any, ...]
          }
          class LinearBias {
            seq
            forward(x: torch.Tensor) torch.Tensor
          }
          class LinearBinary {
            kernel : str
            apply_constraint()*
            codegen(wrapper)
            create(x, y, w, B, attr)
          }
          class LinearBlockSparsePattern {
            col_block_size : int
            prev_col_block_size : int
            prev_row_block_size : int
            rlock : _RLock
            row_block_size : int
            block_size()
          }
          class LinearBn1d {
          }
          class LinearBn1d {
            bias
            bn
            freeze_bn : bool
            qconfig : NoneType
            training : bool
            weight
            weight_fake_quant
            forward(input)
            freeze_bn_stats()
            from_float(mod, use_precomputed_fake_quant)
            reset_bn_parameters()
            reset_parameters()
            reset_running_stats()
            to_float()
            train(mode)
            update_bn_stats()
          }
          class LinearBnLeakyReluModel {
            bn1d
            leaky_relu
            linear
            with_bn : bool
            forward(x)
            get_example_inputs() Tuple[Any, ...]
          }
          class LinearLR {
            end_factor : float
            start_factor : float
            total_iters : int
            get_lr()
          }
          class LinearLeakyReLU {
            negative_slope
            scale : float
            zero_point : int
            forward(x: torch.Tensor) torch.Tensor
            from_float(mod, use_precomputed_fake_quant)
            from_reference(ref_mod, output_scale, output_zero_point)
          }
          class LinearLeakyReLU {
          }
          class LinearModelWithSubmodule {
            fc
            subm
            forward(x)
            get_example_inputs() Tuple[Any, ...]
          }
          class LinearPackedParams {
            dtype
            training
            forward(x)
            set_weight_bias(weight: torch.Tensor, bias: Optional[torch.Tensor], row_block_size: Optional[int], col_block_size: Optional[int]) None
          }
          class LinearPackedParams {
            dtype
            forward(x)
            set_weight_bias(weight: torch.Tensor, bias: Optional[torch.Tensor]) None
          }
          class LinearPerSampleGrad {
            backward(ctx, grad_output)
            forward(ctx, _, __)
          }
          class LinearReLU {
            forward(x: torch.Tensor) torch.Tensor
            from_float(mod, use_precomputed_fake_quant)
            from_reference(ref_qlinear_relu)
          }
          class LinearReLU {
            forward(x: torch.Tensor) torch.Tensor
            from_float(mod, use_precomputed_fake_quant)
            from_reference(ref_linear_relu, output_scale, output_zero_point)
          }
          class LinearReLU {
          }
          class LinearReLU {
            forward(input)
            from_float(mod, use_precomputed_fake_quant)
            to_float()
          }
          class LinearReLUQuantizeHandler {
          }
          class LinearReluAddModel {
            fc1
            fc2
            relu
            forward(x)
            get_example_inputs() Tuple[Any, ...]
          }
          class LinearReluFunctional {
            b1
            child
            w1
            forward(x)
          }
          class LinearReluFunctionalChild {
            b1
            w1
            forward(x)
          }
          class LinearReluLinearModel {
            fc1
            fc2
            relu
            forward(x)
            get_example_inputs() Tuple[Any, ...]
          }
          class LinearReluModel {
            fc
            relu
            forward(x)
            get_example_inputs() Tuple[Any, ...]
          }
          class LinearReluModel {
            fc
            relu
            forward(x)
          }
          class LinearTanh {
            scale : float
            zero_point : int
            forward(x: torch.Tensor) torch.Tensor
            from_float(mod, use_precomputed_fake_quant)
            from_reference(ref_mod, output_scale, output_zero_point)
          }
          class LinearTanh {
          }
          class LinearTanhModel {
            linear
            tanh
            forward(x)
            get_example_inputs() Tuple[Any, ...]
          }
          class LinearUnary {
            apply_constraint()*
            codegen(wrapper)
            create(x, w, B, attr, scalars, algorithm)
          }
          class LintCode {
            name
          }
          class ListContains {
            forward(x)
          }
          class ListIteratorVariable {
            index : int
            items
            as_python_constant()
            call_method(tx, name, args: 'List[VariableTracker]', kwargs: 'Dict[str, VariableTracker]')
            force_unpack_var_sequence(tx) List[VariableTracker]
            next_variable(tx)
            python_type()
            reconstruct(codegen: 'PyCodegen') None
            unpack_var_sequence(tx)
          }
          class ListOf {
            partial : bool
            pattern
            pattern_eq(other: Any) bool
          }
          class ListUnpack {
            forward(args: List[torch.Tensor])
          }
          class ListVariable {
            class_type
            has_grad_fn : bool
            call_hasattr(tx: 'InstructionTranslator', name: str) 'VariableTracker'
            call_method(tx, name, args: List['VariableTracker'], kwargs: Dict[str, 'VariableTracker']) 'VariableTracker'
            debug_repr()
            python_type()
            reconstruct(codegen: 'PyCodegen') None
            var_getattr(tx, name)
          }
          class Lit {
            s
          }
          class Lit {
            s
          }
          class LiteScriptModule {
            find_method(method_name)
            forward()
            run_method(method_name)
          }
          class LiveRange {
            begin : float
            end : float
            contains(other: LiveRange)
            join(other: LiveRange)
          }
          class LiveRanges {
            begin
            end
            ranges
            overlaps(other: LiveRanges)
          }
          class LnStructured {
            PRUNING_TYPE : str
            amount
            dim : int
            n
            apply(module, name, amount, n, dim, importance_scores)
            compute_mask(t, default_mask)
          }
          class LoadEndianness {
            name
          }
          class LoadItemType {
            name
          }
          class LoadPlan {
            items : List[ReadItem]
            planner_data : Optional[Any]
            storage_data : Optional[Any]
          }
          class LoadPlanner {
            commit_tensor(read_item: ReadItem, tensor: torch.Tensor)* None
            create_global_plan(global_plan: List[LoadPlan])* List[LoadPlan]
            create_local_plan()* LoadPlan
            finish_plan(central_plan: LoadPlan)* LoadPlan
            load_bytes(read_item: ReadItem, value: io.BytesIO)* None
            resolve_bytes(read_item: ReadItem)* io.BytesIO
            resolve_tensor(read_item: ReadItem)* torch.Tensor
            set_up_planner(state_dict: STATE_DICT_TYPE, metadata: Optional[Metadata], is_coordinator: bool)* None
          }
          class LoadTensorMeta {
            device
            dtype
            size : List[int]
            stride : List[int]
          }
          class LocalAutotuneCache {
          }
          class LocalBufferContext {
            exit_stack : ExitStack
            global_buffers : Dict[str, ir.Buffer]
            global_to_local : Dict[str, ir.Buffer]
            kernel_args
            local_buffers : Dict[str, ir.Buffer]
            add_local_buffer(local_buffer: ir.Buffer, global_buffers: Optional[List[ir.Buffer]])
            localize_function(fn: Callable[..., Any], rewrite_index: Callable[['LocalizeBufferHandler', sympy.Expr, str], sympy.Expr])
            localize_nodes(nodes: List[ir.IRNode], rewrite_index: Callable[['LocalizeBufferHandler', sympy.Expr, str], sympy.Expr]) List[ir.IRNode]
          }
          class LocalCache {
            lookup() Optional[Dict[str, Any]]
            set_value() None
          }
          class LocalCellSource {
            local_name : str
            reconstruct(codegen)
          }
          class LocalElasticAgent {
          }
          class LocalFeedback {
            feedback_fn : Callable[[Choice], Feedback]
          }
          class LocalLRUCache {
            cache
            cache_info()
          }
          class LocalOptimStateDictConfig {
            offload_to_cpu : bool
          }
          class LocalRRefTest {
            test_create_local_script_class_rref_in_py()
            test_create_local_script_module_rref_in_py()
            test_return_local_script_class_rref_in_py_and_use_in_script()
            test_return_local_script_module_rref_in_py_and_use_in_script()
          }
          class LocalResponseNorm {
            alpha : float
            beta : float
            k : float
            size : int
            extra_repr()
            forward(input: Tensor) Tensor
          }
          class LocalShardsWrapper {
            device
            is_meta
            local_chunks
            handle_all_gather_into_tensor(args, kwargs)
            handle_clone(args, kwargs)
            handle_detach(args, kwargs)
            handle_equal(args, kwargs)
            handle_to_copy(args, kwargs)
            handle_view(args, kwargs)
            handle_wait_tensor(args, kwargs)
            is_pinned() bool
            local_offsets() List[torch.Size]
            local_shards() List[torch.Tensor]
            local_sizes() List[torch.Size]
            requires_grad_(requires_grad: bool) 'LocalShardsWrapper'
            storage_metadata() TensorStorageMetadata
          }
          class LocalSource {
            is_derefed_cell_contents : bool
            is_input : bool
            local_name : str
            guard_source()
            name()
            reconstruct(codegen)
          }
          class LocalState {
            automatic_dynamic : Dict[str, FrameStateSizeEntry]
            render() str
          }
          class LocalStateDictConfig {
          }
          class LocalTimerClient {
            acquire(scope_id, expiration_time)
            release(scope_id)
          }
          class LocalTimerServer {
            clear_timers(worker_ids: Set[int]) None
            get_expired_timers(deadline: float) Dict[Any, List[TimerRequest]]
            register_timers(timer_requests: List[TimerRequest]) None
          }
          class LocalizeBufferHandler {
            global_to_local : Dict[str, ir.Buffer]
            rewrite_index : Callable[['LocalizeBufferHandler', sympy.Expr, str], sympy.Expr]
            load(name: str, index: sympy.Expr)
            localize(name: str, index: sympy.Expr)
            store(name, index, value, mode)
            store_reduction(name, index, value)
          }
          class Location {
            end_column : int | None
            function : str | None
            line : int | None
            message : str | None
            snippet : str | None
            start_column : int | None
            uri : str | None
            sarif() sarif.Location
          }
          class Location {
            annotations : Optional[List[_region.Region]]
            id : int
            logical_locations : Optional[List[_logical_location.LogicalLocation]]
            message : Optional[_message.Message]
            physical_location : Optional[_physical_location.PhysicalLocation]
            properties : Optional[_property_bag.PropertyBag]
            relationships : Optional[List[_location_relationship.LocationRelationship]]
          }
          class LocationRelationship {
            description : Optional[_message.Message]
            kinds : List[str]
            properties : Optional[_property_bag.PropertyBag]
            target : int
          }
          class LogNormal {
            arg_constraints : dict
            has_rsample : bool
            loc
            mean
            mode
            scale
            support
            variance
            entropy()
            expand(batch_shape, _instance)
          }
          class LogRegistry {
            artifact_descriptions : Dict[str, str]
            artifact_log_formatters : Dict[str, logging.Formatter]
            artifact_log_qnames : Set[str]
            artifact_names : Set[str]
            child_log_qnames : Set[str]
            log_alias_to_log_qnames : Dict[str, List[str]]
            off_by_default_artifact_names : Set[str]
            visible_artifacts : Set[str]
            get_artifact_log_qnames()
            get_child_log_qnames()
            get_log_qnames() Set[str]
            is_artifact(name)
            is_log(alias)
            is_off_by_default(artifact_qname)
            register_artifact_log(artifact_log_qname)
            register_artifact_name(name, description, visible, off_by_default, log_format)
            register_child_log(log_qname)
            register_log(alias, log_qnames: Union[str, List[str]])
          }
          class LogSigmoid {
            forward(input: Tensor) Tensor
          }
          class LogSoftmax {
            dim : Optional[int]
            extra_repr()
            forward(input: Tensor) Tensor
          }
          class LogState {
            artifact_names : Set[str]
            log_qname_to_level : Dict[str, str]
            clear()
            enable_artifact(artifact_name)
            enable_log(log_qnames, log_level)
            get_log_level_pairs()
            is_artifact_enabled(name)
          }
          class Logger {
            dtype
            stats : dict
            forward(x)*
          }
          class LoggingLoggerVariable {
            value
            call_method(tx, name, args: 'List[VariableTracker]', kwargs: 'Dict[str, VariableTracker]') 'VariableTracker'
          }
          class LoggingShapeGuardPrinter {
          }
          class LoggingTensor {
            context : nullcontext
            elem
          }
          class LoggingTensorHandler {
            log_list : List[str]
            memo
            next_id : int
            tracebacks_list : Optional[List]
            use_shortid_for_all_tensors : bool
            with_type : bool
            emit(record)
          }
          class LoggingTensorMode {
          }
          class LoggingTensorReentrant {
            context
          }
          class LoggingTestCase {
            getRecord(records, m)
            hasRecord(records, m)
            setUpClass()
            tearDownClass()
          }
          class LogicalLocation {
            decorated_name : Optional[str]
            fully_qualified_name : Optional[str]
            index : int
            kind : Optional[str]
            name : Optional[str]
            parent_index : int
            properties : Optional[_property_bag.PropertyBag]
          }
          class LogisticNormal {
            arg_constraints : dict
            has_rsample : bool
            loc
            scale
            support
            expand(batch_shape, _instance)
          }
          class LogitRelaxedBernoulli {
            arg_constraints : dict
            logits
            param_shape
            probs
            support
            temperature
            expand(batch_shape, _instance)
            log_prob(value)
            logits()
            probs()
            rsample(sample_shape: _size) torch.Tensor
          }
          class LogsDest {
            error_files : Dict[int, str]
            stderrs : Dict[int, str]
            stdouts : Dict[int, str]
            tee_stderrs : Dict[int, str]
            tee_stdouts : Dict[int, str]
          }
          class LogsSpecs {
            root_log_dir
            reify(envs: Dict[int, Dict[str, str]])* LogsDest
          }
          class LongStorage {
            dtype()
          }
          class LongStorage {
            dtype()
          }
          class LoopBody {
            indexing : NoneType
            indexing_exprs : Dict[str, sympy.Expr]
            indexing_exprs_name : Dict[sympy.Expr, str]
            indirect_var_ranges : Dict[sympy.Symbol, sympy.Expr]
            indirect_vars : List[sympy.Symbol]
            iter_vars
            memory_usage : Dict[MemoryUsageType, List[MemoryEntry]]
            op_counts : collections.Counter[str]
            reduce_vars
            root_block
            sizes : tuple
            subblocks : Dict[str, LoopBodyBlock]
            submodules : Dict[str, Any]
            var_ranges
            vars
            add_index_expr(expr: sympy.Expr, mtype: MemoryUsageType, buffer_name: Optional[str], mode: Optional[str])
            add_indirect(size)
            add_submodule(block, prefix)
            bind_masked_shim(name)
            bind_scan_shim(combine_fn)
            bind_set_indirect_shim(var, size, check, wrap_neg)
            bounds()
            debug_str()
            get_index(name)
            get_nodes()
            get_read_expr(buffer_name)
            get_read_exprs()
            get_write_expr(buffer_name)
            get_write_exprs()
            has_op(name: str)
            indexing_from_args(indices)
            is_memory_copy() bool
            merge_loops() LoopBody
            reorder_iter_loops(new_order) LoopBody
            replace_indirect(old, new)
          }
          class LoopBodyBlock {
            body
            graph
            name : str
            clone(body: LoopBody)
            contains_only_ops(allowed_ops) bool
            debug_str(name)
          }
          class LoopLevel {
            collapsed : bool
            is_reduction : bool
            offset : Expr
            parallel : int
            simd_nelements : int
            simd_omp : bool
            simd_vec : bool
            size : Optional[sympy.Expr]
            steps : Expr
            tiled_size : Expr
            var : Optional[sympy.Expr]
            lines()
            tile(factor)
          }
          class LoopNest {
            kernel : Optional[CppKernel]
            loops : Optional[List[LoopLevel]]
            build(kernel: CppKernel)
            from_loop_level(level: int)
            get_kernel() CppKernel
            is_reduction_only()
            mark_parallel(par_depth)
            max_parallel_depth()
            set_kernel(kernel)
            tile(depth, factor)
          }
          class Loops {
            device
            dtype
            inner_fn : Callable[..., Any]
            ranges : Sequence[_IntLike]
            constant_to_device(device: torch.device)* IRNode
            create() TensorBox
            get_device() Optional[torch.device]
            get_origin_node() Optional[torch.fx.Node]
            get_pointwise_size() Sequence[Expr]
            get_read_names() OrderedSet[str]
            get_reads() OrderedSet[Dep]
            get_reduction_size()* Sequence[sympy.Expr]
            get_reduction_type()* Optional[str]
            get_size() Sequence[Expr]
            get_unbacked_symbol_uses() OrderedSet[Symbol]
            has_large_inner_fn(threshold: Optional[int]) bool
            inner_fn_args() Sequence[Sequence[_IntLike]]
            inner_fn_free_unbacked_symbols() OrderedSet[Symbol]
            inner_fn_opcount() OpCountResult
            inner_fn_str() str
            num_reads() int
          }
          class LossOutputSpec {
            arg : Annotated[TensorArgument, 10]
          }
          class LossWrapper {
            loss_fn
            module
            forward()*
          }
          class LowPrecisionState {
            parameter_type
          }
          class LowRankMultivariateNormal {
            arg_constraints : dict
            cov_diag
            cov_factor
            has_rsample : bool
            loc
            mean
            mode
            support
            covariance_matrix()
            entropy()
            expand(batch_shape, _instance)
            log_prob(value)
            precision_matrix()
            rsample(sample_shape: _size) torch.Tensor
            scale_tril()
            variance()
          }
          class LowerCholeskyTransform {
            codomain
            domain
          }
          class LoweringException {
          }
          class LoweringPatternEntry {
            handler : Callable[..., Any]
            apply(match: Match, graph: torch.fx.Graph, node: torch.fx.Node) None
          }
          class M {
            forward(x)*
          }
          class M {
            linear
            forward(x)
          }
          class MDNotImplementedError {
          }
          class MEM_FORMAT_ENCODING {
            name
          }
          class MKLPackedLinear {
            codegen(wrapper)
            create(x, packed_w, orig_w, B, batch_size)
          }
          class MLP {
            buffer : NoneType
            in_proj
            out_proj
            forward(x: torch.Tensor) torch.Tensor
            reset_parameters()
          }
          class MLPModule {
            net1
            net2
            relu
            forward(x)
            reset_parameters()
          }
          class MLPStack {
            with_seq_parallel : bool
            parallelize(tp_mesh: DeviceMesh, dp_mesh: DeviceMesh, use_activation_checkpointing: bool) 'MLPStack'
          }
          class MLPStacked {
            layers
            forward(x)
          }
          class MMRankingA100 {
            choices : List[Choice]
            check_precondition(metadata: AHMetadata, context: AHContext) bool
            fill_choices() None
            get_best_choices(context: AHContext) Optional[List[Tuple[float, int]]]
            get_choice(idx: int) Optional[str]
            get_confidence_threshold() float
            get_name() str
          }
          class MMRankingH100 {
            choices : List[Choice]
            check_precondition(metadata: AHMetadata, context: AHContext) bool
            fill_choices() None
            get_best_choices(context: AHContext) Optional[List[Tuple[float, int]]]
            get_choice(idx: int) Optional[str]
            get_confidence_threshold() float
            get_name() str
          }
          class MPSTestBase {
            device_type : str
            primary_device : ClassVar[str]
            get_all_devices()
            get_primary_device()
            setUpClass()
          }
          class MSELoss {
            forward(input: Tensor, target: Tensor) Tensor
          }
          class MSPS {
            func_names : Set[str]
            memory : int
            msps : float
            op_idx : int
            runtime : float
          }
          class ManagerWatchdog {
            kernel32 : WinDLL
            manager_dead : bool
            manager_handle
            manager_pid
            is_alive()
          }
          class ManualConvLinearQATModel {
            conv
            dequant
            fc1
            fc2
            qconfig
            quant
            forward(x)
          }
          class ManualConvLinearSymmQATModel {
          }
          class ManualDropoutQATModel {
            dequant
            dropout
            fc1
            qconfig
            quant
            forward(x)
          }
          class ManualEmbeddingBagLinear {
            dequant
            emb
            linear
            qconfig
            quant
            forward(input: torch.Tensor, offsets: Optional[torch.Tensor], per_sample_weights: Optional[torch.Tensor])
          }
          class ManualLinearDynamicQATModel {
            fc1
            fc2
            qconfig
            forward(x)
          }
          class ManualLinearQATModel {
            dequant
            fc1
            fc2
            qconfig
            quant
            forward(x)
          }
          class MapAutogradOp {
            backward(ctx)
            forward(ctx, fw_graph, joint_graph, num_mapped_args)
          }
          class MapDataPipe {
            functions : Dict[str, Callable]
            getstate_hook : Optional[Callable]
            reduce_ex_hook : Optional[Callable]
            repr_hook : Optional[Callable]
            str_hook : Optional[Callable]
            register_datapipe_as_function(function_name, cls_to_register)
            register_function(function_name, function)
            set_getstate_hook(hook_fn)
            set_reduce_ex_hook(hook_fn)
          }
          class MapHigherOrderVariable {
            call_function(tx: 'InstructionTranslator', args: List[VariableTracker], kwargs: Dict[str, VariableTracker]) VariableTracker
          }
          class MapImpl {
          }
          class MapVariable {
            fn
            has_unpack_var_sequence(tx) bool
            next_variable(tx)
            python_type()
            reconstruct(codegen)
          }
          class MapWrapper {
          }
          class MapperIterDataPipe {
            datapipe
            fn : Callable
            input_col : NoneType
            output_col : NoneType
          }
          class MapperMapDataPipe {
            datapipe
            fn : Callable
          }
          class MappingKey {
            key : K
            get(mapping: Mapping[K, T]) T
          }
          class MarginRankingLoss {
            margin : float
            forward(input1: Tensor, input2: Tensor, target: Tensor) Tensor
          }
          class MarkStepBox {
            mark_step_counter : int
          }
          class Marker {
            index : int
          }
          class MaskBuffer {
            data : Optional[torch.Tensor]
            refcount : int
            apply_mask(tensor)
            materialize_mask(mask)
            release_mask()
          }
          class MaskedTensor {
            is_sparse
            get_data()
            get_mask()
            is_sparse_coo()
            is_sparse_csr()
            to_tensor(value)
            unary(fn, data, mask)
          }
          class MatHandler {
            loadmat_kwargs : dict
            sio
          }
          class MatMulDimInFP16Pattern {
            description : str
            name : str
            skip
            url : str
            benchmark(events: List[_ProfilerEvent])
            match(event: _ProfilerEvent)
          }
          class Match {
            anchor
            nodes_map : Dict[Node, Node]
          }
          class Match {
            args : List[Any]
            ctx
            graph
            kwargs : Dict[str, Any]
            nodes : List[torch.fx.Node]
            pattern
            replacement_graph : Optional[torch.fx.GraphModule]
            targets : Dict[_TargetExpr, torch.fx.node.Target]
            bundle() Match
            erase_nodes() None
            extend(other: Match) None
            output_node() torch.fx.Node
            output_nodes() List[Optional[torch.fx.Node]]
            replace_by_example(replacement_fn: ReplaceFn, args: Sequence[Any], trace_fn: Optional[TraceFn], run_functional_passes: bool) None
            replace_with_graph(replacement_graph: torch.fx.Graph, args: Sequence[Any]) None
          }
          class MatchAllNode {
          }
          class MatchContext {
            exclusive_node_set : List[NodeOrConstant]
            graph
            outputs : List[Optional[PatternExpr]]
            pattern_to_node : Dict[PatternExpr, Optional[torch.fx.Node]]
            filter_multi_user_patterns() Dict[PatternExpr, torch.fx.Node]
            match(pattern: PatternExpr, node: NodeOrConstant) MatchResult
          }
          class Max {
            identity
            zero
          }
          class MaxPool1d {
            dilation : Union
            kernel_size : Union
            padding : Union
            stride : Union
            forward(input: Tensor)
          }
          class MaxPool2d {
            dilation : Union
            kernel_size : Union
            padding : Union
            stride : Union
            forward(input: Tensor)
          }
          class MaxPool3d {
            dilation : Union
            kernel_size : Union
            padding : Union
            stride : Union
            forward(input: Tensor)
          }
          class MaxUnpool1d {
            kernel_size : Union
            padding : Union
            stride : Union
            forward(input: Tensor, indices: Tensor, output_size: Optional[List[int]]) Tensor
          }
          class MaxUnpool2d {
            kernel_size : Union
            padding : Union
            stride : Union
            forward(input: Tensor, indices: Tensor, output_size: Optional[List[int]]) Tensor
          }
          class MaxUnpool3d {
            kernel_size : Union
            padding : Union
            stride : Union
            forward(input: Tensor, indices: Tensor, output_size: Optional[List[int]]) Tensor
          }
          class MeanShadowLogger {
            count : int
            float_sum : NoneType
            quant_sum : NoneType
            clear()
            forward(x, y)
          }
          class Measurement {
            as_row_name
            env
            has_warnings
            iqr
            mean
            median
            metadata : Optional[Dict[Any, Any]]
            number_per_run : int
            raw_times : List[float]
            significant_figures
            task_spec
            times
            title
            meets_confidence(threshold: float) bool
            merge(measurements: Iterable['Measurement']) List['Measurement']
          }
          class MemPool {
            allocator
            id
            snapshot()
            use_count() int
          }
          class MemPoolContext {
            active_pool() Optional[_MemPool]
          }
          class MemRecordsAcc {
            in_interval(start_us, end_us)
          }
          class MemTracker {
            memory_tracking
            display_modulewise_snapshots(depth: int, units: str, tabulate: bool) None
            display_snapshot(type: str, units: str, tabulate: bool) None
            get_tracker_snapshot(type: str) Dict[torch.device, Dict[str, int]]
            reset_mod_stats() None
            track_external() None
          }
          class MemoizeWithCycleCheck {
            cache : Dict[Tuple[str, int], Any]
            fn : Callable[..., Any]
            reset() None
          }
          class MemoryDep {
            index : Expr
            mode : Optional[str]
            name : str
            num_vars
            ranges
            size : Tuple[sympy.Expr, ...]
            var_names : Tuple[sympy.Symbol, ...]
            decide_loop_order_to_match(other)
            get_numel() sympy.Expr
            get_offset()
            has_unbacked_symbols()
            is_contiguous() bool
            is_indirect() bool
            is_scalar() bool
            normalize() 'MemoryDep'
            normalize_with_stride_order(prefix)
            numbytes_hint() int
            rename(renames: Dict[str, str]) 'MemoryDep'
            simplify_with_ranges()
            stride1_for_last_dim(result_for_complex_expression) bool
          }
          class MemoryEntry {
            buffer_name : Optional[str]
            index_name : str
            mode : Optional[str]
          }
          class MemoryFormat {
            name
          }
          class MemoryPlanner {
            buffer_groups : Optional[List[BufferGroup]]
            pools
            wrapper : Any
            allocate_groups()
            compute_buffer_groups(lines)
            compute_live_ranges(lines)
            convert_to_pool_lines(lines)
            drop_removed_buffers(lines)
            mark_first_last_usage(lines)
            plan(lines: List[Any]) List[Any]
          }
          class MemoryPlanningInfoForBuffer {
            size_alloc : int
            size_free : int
            succ_nodes : OrderedSet[BaseSchedulerNode]
          }
          class MemoryPlanningInfoForNode {
            index : int
            pred_buffers : OrderedSet[Union[SchedulerBuffer, FreeableInputBuffer]]
            pred_nodes : OrderedSet[BaseSchedulerNode]
            size : int
            succ_nodes : OrderedSet[BaseSchedulerNode]
          }
          class MemoryPlanningLine {
            wrapper
            codegen(code: IndentedBuffer)* None
            plan(state: MemoryPlanningState) MemoryPlanningLine
          }
          class MemoryPlanningState {
            reuse_pool : Dict[ReuseKey, List[FreeIfNotReusedLine]]
            total_allocated_buffer_size : int
            pop(key: ReuseKey) FreeIfNotReusedLine
            push(key: ReuseKey, item: FreeIfNotReusedLine) None
          }
          class MemoryProfile {
            timeline
          }
          class MemoryProfileDispatchMode {
            memory_tracker
          }
          class MemoryProfileTimeline {
            categories
            timeline
            export_memory_timeline(path, device_str) None
            export_memory_timeline_html(path, device_str, figsize, title) None
            export_memory_timeline_raw(path, device_str) None
          }
          class MemorySplitProtocol {
            get_live_ranges : CachedMethod[[], LiveRanges]
            get_size_hint : CachedMethod[[], int]
            get_symbolic_size : CachedMethod[[], sympy.Expr]
          }
          class MemoryTracker {
            memories_active : Dict[int, Dict[str, float]]
            memories_allocated : Dict[int, Dict[str, float]]
            memories_reserved : Dict[int, Dict[str, float]]
            profile_mode : NoneType
            load(path: str) None
            save_stats(path: str) None
            show_traces(path: str) None
            start_monitor(root_module: nn.Module) None
            stop() None
            summary(top: int) None
          }
          class MemoryUsageType {
            name
          }
          class MergeKwargsIntoArgsInputStep {
            apply(model_args: Sequence[Any], model_kwargs: Mapping[str, Any], model: torch.nn.Module | Callable | torch_export.ExportedProgram | None) tuple[Sequence[Any], Mapping[str, Any]]
          }
          class MeshTopoInfo {
            mesh
            mesh_dim_bandwidth : List[float]
            mesh_dim_devices : List[int]
            mesh_dim_latency : List[float]
            build_from_mesh(mesh: DeviceMesh) 'MeshTopoInfo'
          }
          class Message {
            arguments : Optional[List[str]]
            id : Optional[str]
            markdown : Optional[str]
            properties : Optional[_property_bag.PropertyBag]
            text : Optional[str]
          }
          class MetaAttribute {
            attr : str
            node
            root
            tracer
          }
          class MetaConverter {
            arg_cnt : int
            copy_data : bool
            del_hook : NoneType
            describer
            hit : int
            miss : int
            storage_memo : weakref.WeakValueDictionary[MetaStorageId, torch.UntypedStorage]
            tensor_memo : weakref.WeakValueDictionary[MetaTensorId, _TensorT]
            get_storage_memo(s: MetaStorageDesc) Optional[torch.UntypedStorage]
            get_tensor_memo(t: MetaTensorDesc) Optional[torch.Tensor]
            meta_storage(s: MetaStorageDesc, callback: Callable[[Callable[[], torch.Tensor]], _TensorT]) torch.UntypedStorage
            meta_tensor(t: MetaTensorDesc, shape_env: Optional[ShapeEnv], callback: _MetaTensorCallback[_TensorT], source: Optional[Source], symbolic_context: Optional[SymbolicContext]) _TensorT
            set_storage_memo(s: MetaStorageDesc, v: torch.UntypedStorage) None
            set_tensor_memo(t: MetaTensorDesc, v: _TensorT) None
            successful() bool
          }
          class MetaDeviceAttribute {
          }
          class MetaProxy {
            fake_mode : NoneType
          }
          class MetaProxy {
            device
            dtype
            shape
            dim()
            install_tensor_meta(tensor_meta)
            size(dim)
          }
          class MetaStorageDesc {
            data : Optional[torch.UntypedStorage]
            id : MetaStorageId
            size : int
            as_json(describer_id: _DescriberId) Dict[str, object]
          }
          class MetaTensorDesc {
            attrs : Optional[Dict[str, MetaTensorDesc]]
            autograd_meta_from : Optional[torch.Tensor]
            base : Optional[MetaTensorDesc]
            bdim : Optional[int]
            ccol_indices : Optional[MetaTensorDesc]
            col_indices : Optional[MetaTensorDesc]
            creation_meta : Optional[CreationMeta]
            crow_indices : Optional[MetaTensorDesc]
            ctx : Optional[object]
            current_level : Optional[int]
            data : Optional[torch.Tensor]
            dense_dim : Optional[int]
            device
            dtype
            dynamo_dynamic_indices : List[int]
            fake_mode : Optional[FakeTensorMode]
            functorch_stack : Optional[List[CInterpreter]]
            grad : Optional[MetaTensorDesc]
            id : MetaTensorId
            is_batchedtensor : bool
            is_coalesced : Optional[bool]
            is_conj : bool
            is_functional : bool
            is_functorch_wrapped : bool
            is_gradtrackingtensor : bool
            is_inference : bool
            is_leaf : bool
            is_legacy_batchedtensor : bool
            is_mkldnn : bool
            is_neg : bool
            is_nested : bool
            is_parameter : bool
            is_sparse : bool
            is_traceable_wrapper_subclass : bool
            is_view : bool
            layout
            level : Optional[int]
            ndim : int
            nested_int : Optional[int]
            requires_grad : bool
            row_indices : Optional[MetaTensorDesc]
            shape
            size : Tuple[int, ...]
            sparse_dim : Optional[int]
            storage : Optional[MetaStorageDesc]
            storage_offset : int
            stride : Optional[Tuple[int, ...]]
            type : Optional[Type]
            unwrapped : Optional[MetaTensorDesc]
            values : Optional[MetaTensorDesc]
            view_func : Optional[ViewFunc]
            as_json(describer_id: _DescriberId) Dict[str, object]
          }
          class MetaTensorDescriber {
            copy_data : bool
            id : _DescriberId
            lookup_storage
            lookup_tensor
            next_storage_id : MetaStorageId
            next_tensor_id : MetaTensorId
            traced_storages : Set[int]
            traced_tensors : Set[int]
            describe_storage(s: torch.UntypedStorage) MetaStorageDesc
            describe_tensor(t: torch.Tensor) MetaTensorDesc
            get_storage_id(s: torch.UntypedStorage) MetaStorageId
            get_tensor_id(t: torch.Tensor) MetaTensorId
          }
          class MetaTracer {
            allow_insert_stateless_mods : bool
            meta_args : Dict[str, torch.Tensor]
            orig_fns : set
            orig_forward
            patched_torch_methods
            prev_module : str
            call_module(m, forward, args, kwargs)
            create_proxy(kind, target, args, kwargs, name, type_expr, proxy_factory_fn)
            getattr(attr, attr_val, parameter_proxy_cache)
            path_of_module(mod: torch.nn.Module) str
            proxy(node)
            trace(root, meta_args: Dict[str, torch.Tensor], concrete_args)
          }
          class Metadata {
            planner_data : Optional[Any]
            state_dict_metadata : Dict[str, STORAGE_TYPES]
            storage_data : Optional[Any]
            storage_meta : Optional[StorageMeta]
          }
          class Metadata {
            keyset
            keyword_only_args : Dict[str, Any]
          }
          class Metadata {
            input_spec
            output_spec : Optional[spec_t]
            result_is_tuple : Optional[bool]
          }
          class MetadataIndex {
            fqn : str
            index : Optional[int]
            offset : Optional[torch.Size]
          }
          class MetadataKey {
            is_conj : bool
            is_neg : bool
            is_sparse : bool
            layout
            size : Tuple[SymIntEqByExpr, ...]
            storage_offset : Optional[SymIntEqByExpr]
            stride : Optional[Tuple[SymIntEqByExpr, ...]]
            make(t)
          }
          class MetadataMismatchError {
            reason : str
          }
          class MethodDispatcher {
            cls
            obj
            get_func_params(func)
          }
          class MethodWrapperVariable {
            method_wrapper
            as_python_constant()
            call_function(tx: 'InstructionTranslator', args: 'List[VariableTracker]', kwargs: 'Dict[str, VariableTracker]') 'VariableTracker'
            is_python_constant()
          }
          class MetricHandler {
            emit(metric_data: MetricData)*
          }
          class MetricStream {
            group_name : str
            handler
            add_value(metric_name: str, metric_value: int)
          }
          class MetricTable {
            column_names : List[str]
            num_rows_added : int
            table_name : str
            add_row(row_fn)
            output_filename()
            register_table(name, column_names)
            write_header()
          }
          class MetricsConfig {
            params : Optional[Dict[str, str]], dict
          }
          class MetricsContext {
            add_to_set(metric: str, value: Any) None
            in_progress() bool
            increment(metric: str, value: int) None
            set(metric: str, value: Any) None
            set_key_value(metric: str, key: str, value: Any) None
            update(values: Dict[str, Any]) None
            update_outer(values: Dict[str, Any]) None
          }
          class Min {
            identity
            zero
          }
          class MinCutOptions {
            ban_if_long_fusible_chains : bool
            ban_if_materialized_backward : bool
            ban_if_not_in_allowlist : bool
            ban_if_reduction : bool
            ban_if_used_far_apart : bool
          }
          class MinMaxBase {
          }
          class MinMaxObserver {
            max_val
            min_val
            calculate_qparams()
            extra_repr()
            forward(x_orig)
            reset_min_max_vals()
          }
          class MinifierTestBase {
            DEBUG_DIR : bytes, str
            setUpClass()
            tearDownClass()
          }
          class MinifierTestResult {
            minifier_code : str
            repro_code : str
            get_exported_program_path()
            minifier_module()
            repro_module()
          }
          class Mish {
            inplace : bool
            extra_repr() str
            forward(input: Tensor) Tensor
          }
          class MissingOperatorWithDecomp {
          }
          class MissingOperatorWithoutDecomp {
          }
          class MixedMMA100 {
            choices : List[Choice]
            check_precondition(metadata: AHMetadata, context: AHContext) bool
            fill_choices() None
            get_best_choices(context: AHContext) Optional[List[Tuple[float, int]]]
            get_choice(idx: int) Optional[str]
            get_confidence_threshold() float
            get_name() str
          }
          class MixedMMH100 {
            choices : List[Choice]
            check_precondition(metadata: AHMetadata, context: AHContext) bool
            fill_choices() None
            get_best_choices(context: AHContext) Optional[List[Tuple[float, int]]]
            get_choice(idx: int) Optional[str]
            get_confidence_threshold() float
            get_name() str
          }
          class MixedPrecision {
            buffer_dtype : Optional[torch.dtype]
            cast_forward_inputs : bool
            cast_root_forward_inputs : bool
            keep_low_precision_grads : bool
            param_dtype : Optional[torch.dtype]
            reduce_dtype : Optional[torch.dtype]
          }
          class MixedPrecisionPolicy {
            cast_forward_inputs : bool
            output_dtype : Optional[torch.dtype]
            param_dtype : Optional[torch.dtype]
            reduce_dtype : Optional[torch.dtype]
          }
          class MixtureOfExperts {
            delay_before_free_ms : int
            group
            module
            move_to_device
            num_expert_params
            wrap_fsdp : bool
            forward(x)
            init(group: dist.ProcessGroup, fsdp_init_mode: FSDPInitMode, device_init_mode: DEVICEInitMode, fsdp_kwargs: Optional[Dict[str, Any]], deterministic: bool, delay_before_free_ms: int)
            run_backward(loss)
          }
          class MixtureSameFamily {
            arg_constraints : Dict[str, constraints.Constraint]
            component_distribution
            has_rsample : bool
            mean
            mixture_distribution
            variance
            cdf(x)
            expand(batch_shape, _instance)
            log_prob(x)
            sample(sample_shape)
            support()
          }
          class MklSubgraph {
            end_nodes : List[fx.Node]
            fx_graph
            nodes : List[fx.Node]
            start_nodes : List[fx.Node]
          }
          class MklSupport {
            name
          }
          class MkldnnBatchNorm {
            bias
            eps
            exponential_average_factor : float
            running_mean
            running_var
            training
            weight
            forward(x)
          }
          class MkldnnConv1d {
            bias
            training
            weight
          }
          class MkldnnConv2d {
            bias
            training
            weight
          }
          class MkldnnConv3d {
            bias
            training
            weight
          }
          class MkldnnLinear {
            bias
            training
            weight
            forward(x)
          }
          class MkldnnModule {
            deterministic
            enabled
          }
          class MkldnnPrelu {
            training
            weight
            forward(x)
          }
          class MkldnnRnnLayer {
            outputs
            codegen(wrapper)
            create(x: 'TensorBox', w0: 'TensorBox', w1: 'TensorBox', w2: 'TensorBox', w3: 'TensorBox', hx: 'TensorBox', cx: 'TensorBox', reverse: bool, batch_sizes: List[int], mode: int, hidden_size: int, num_layers: int, has_biases: bool, bidirectional: bool, batch_first: bool, train: bool)
          }
          class MockBackend {
            with_name(name: str) Callable[[], MockBackend]
          }
          class MockFXGraphCacheOutput {
            gm : Optional[Any]
            post_compile(example_inputs: Sequence[InputType], cudagraphs: BoxedBool, constants: CompiledFxGraphConstants)* None
            set_triton_bundle(triton_bundle: Any)* None
          }
          class MockHandler {
            frexp(x)
            indirect_indexing(index_var, size, check, wrap_neg) sympy.Symbol
            masked(mask, body, other) str
            scan(dtypes, combine_fn, values)
            sort(dtypes, values, stable, descending)
          }
          class MockModule {
            l1
            forward(x)
          }
          class MockProcessGroup {
            getBackendName()
          }
          class MockSparseLinear {
            from_dense(mod: nn.Linear) 'MockSparseLinear'
          }
          class MockedObject {
          }
          class Mod {
            is_integer : bool
            is_nonnegative : bool
            nargs : tuple
            precedence : int
            eval(p, q)
          }
          class ModIndex {
            generate_vmap_rule : bool
            backward(ctx, gradOut)
            forward(x: Tensor, indices: List[Tensor]) Tensor
            setup_context(ctx: Any, inputs: Tuple[Any, ...], output: Any) None
          }
          class ModOrder {
            bw_post_order : List[str]
            bw_pre_order : List[str]
            fw_post_order : List[str]
            fw_pre_order : List[str]
          }
          class ModRuntime {
            bw : float
            fw : float
          }
          class ModStats {
            act_bw_per_module : int
            act_fw_per_module : int
            act_grad_per_module : int
            act_total : int
            breakpoints : List[float]
            bw_runtime_per_module : float
            fqn : str
            fw_runtime_per_module : float
            grad_per_module : int
            grad_total : int
            input_per_module : int
            intercepts : List[float]
            is_leaf : bool
            n_segments : int
            output_per_module : int
            param_per_module : int
            sac_memory : int
            sac_runtime : float
            slopes : List[float]
            tradeoff_curve : OrderedDict[float, float]
          }
          class ModTracker {
            is_bw
            parents : Set[str]
            clear_user_hooks()
            get_known_fqn(mod)
            register_user_hooks(pre_fw_hook: Optional[Callable], post_fw_hook: Optional[Callable], pre_bw_hook: Optional[Callable], post_bw_hook: Optional[Callable])
          }
          class Mode {
            name
          }
          class Model {
            p
            forward()
          }
          class ModelArgs {
            checkpoint_activations : bool
            dim : int
            dropout_p : float
            max_seq_len : int
            n_heads : int
            n_layers : int
            use_attn_mask : bool
            vocab_size : int
            weight_tying : bool
          }
          class ModelAttrMutation {
            attr_list : list
            forward(x)
            recreate_list()
          }
          class ModelAverager {
            process_group : NoneType
            step : int
            average_parameters(params)*
          }
          class ModelForConvTransposeBNFusion {
            bn1
            bn2
            bn3
            conv1
            conv2
            conv3
            forward(x)
          }
          class ModelForFusion {
            bn1
            bn2
            bn3
            conv1
            conv2
            conv3
            dequant
            fc
            qconfig
            quant
            relu1
            relu2
            relu3
            relu4
            sub1
            sub2
            forward(x)
          }
          class ModelForFusionWithBias {
            bn1
            bn2
            conv1
            conv2
            dequant
            quant
            relu1
            forward(x)
          }
          class ModelForLinearBNFusion {
            bn
            fc
            forward(x)
          }
          class ModelInfo {
            buffer_count : defaultdict[torch.dtype, int]
            dispatch_failures : list[tuple[torch.fx.Node, str]]
            fx_node_count : int
            fx_node_op_count : defaultdict[str, int]
            fx_node_target_count : defaultdict[str, int]
            inputs : dict[str, torch._export.serde.schema.TensorMeta]
            outputs : dict[str, torch._export.serde.schema.TensorMeta]
            parameter_count : defaultdict[torch.dtype, int]
          }
          class ModelMultipleOps {
            avgpool
            bn1
            cat
            conv1
            conv2
            downsample
            fc
            relu1
            relu2
            skip_add
            forward(x)
          }
          class ModelMultipleOpsNoAvgPool {
            bn1
            cat
            conv1
            conv2
            fc
            maxpool
            relu1
            relu2
            skip_add
            forward(x)
          }
          class ModelReport {
            generate_equalization_mapping() QConfigMapping
            generate_model_report(remove_inserted_observers: bool) Dict[str, Tuple[str, Dict]]
            generate_qconfig_mapping() QConfigMapping
            generate_visualizer() ModelReportVisualizer
            get_desired_reports_names() Set[str]
            get_observers_of_interest() Dict[str, Set[str]]
            prepare_detailed_calibration() GraphModule
          }
          class ModelReportObserver {
            average_batch_activation_range
            average_percentile_ratio
            ch_axis : int
            comp_percentile
            constant_channels
            epoch_activation_max
            epoch_activation_min
            max_val
            min_val
            num_batches_tracked : int
            percentile_batches_tracked
            calculate_qparams()
            forward(x)
            get_batch_to_epoch_ratio()
            reset_batch_and_epoch_values()
          }
          class ModelReportVisualizer {
            CHANNEL_NUM_INDEX : int
            NUM_NON_FEATURE_CHANNEL_HEADERS : int
            NUM_NON_FEATURE_TENSOR_HEADERS : int
            TABLE_CHANNEL_KEY : str
            TABLE_TENSOR_KEY : str
            generated_reports : OrderedDict[str, Any]
            generate_filtered_tables(feature_filter: str, module_fqn_filter: str) Dict[str, Tuple[List, List]]
            generate_histogram_visualization(feature_filter: str, module_fqn_filter: str, num_bins: int)
            generate_plot_visualization(feature_filter: str, module_fqn_filter: str)
            generate_table_visualization(feature_filter: str, module_fqn_filter: str)
            get_all_unique_feature_names(plottable_features_only: bool) Set[str]
            get_all_unique_module_fqns() Set[str]
          }
          class ModelWithComm {
            lin
            forward(x)
          }
          class ModelWithFunctionals {
            myadd
            myadd_relu
            mycat
            mymatmul
            forward(x)
          }
          class ModelWithSequentialFusion {
            classifier
            conv1
            dequant
            features
            quant
            relu1
            seq
            forward(x)
          }
          class ModificationWrapper {
            fixed_inputs : Dict[str, Any]
            kernel
            mask : Optional[str]
            name : str
            indirect_indexing(index_var: str, size, check, wrap_neg)
            load(name: str, index: sympy.Expr)
            store(name: str, index: sympy.Expr, value: CSEVariable, mode: StoreMode) str
          }
          class ModularIndexing {
            is_integer : bool
            nargs : Tuple[int, ...]
            precedence : int
            eval(base: sympy.Integer, divisor: sympy.Integer, modulus: sympy.Integer) Optional[sympy.Basic]
          }
          class Modularize {
            is_exported_program : bool
            module
          }
          class Module {
            members
            name
          }
          class Module {
            T_destination : T_destination
            call_super_init : bool
            dump_patches : bool
            forward : Callable[..., Any]
            training : bool
            add_module(name: str, module: Optional['Module']) None
            apply(fn: Callable[['Module'], None]) T
            bfloat16() T
            buffers(recurse: bool) Iterator[Tensor]
            children() Iterator['Module']
            compile()
            cpu() T
            cuda(device: Optional[Union[int, device]]) T
            double() T
            eval() T
            extra_repr() str
            float() T
            get_buffer(target: str) 'Tensor'
            get_extra_state() Any
            get_parameter(target: str) 'Parameter'
            get_submodule(target: str) 'Module'
            half() T
            ipu(device: Optional[Union[int, device]]) T
            load_state_dict(state_dict: Mapping[str, Any], strict: bool, assign: bool)
            modules() Iterator['Module']
            mtia(device: Optional[Union[int, device]]) T
            named_buffers(prefix: str, recurse: bool, remove_duplicate: bool) Iterator[Tuple[str, Tensor]]
            named_children() Iterator[Tuple[str, 'Module']]
            named_modules(memo: Optional[Set['Module']], prefix: str, remove_duplicate: bool)
            named_parameters(prefix: str, recurse: bool, remove_duplicate: bool) Iterator[Tuple[str, Parameter]]
            parameters(recurse: bool) Iterator[Parameter]
            register_backward_hook(hook: Callable[['Module', _grad_t, _grad_t], Union[None, _grad_t]]) RemovableHandle
            register_buffer(name: str, tensor: Optional[Tensor], persistent: bool) None
            register_forward_hook(hook: Union[Callable[[T, Tuple[Any, ...], Any], Optional[Any]], Callable[[T, Tuple[Any, ...], Dict[str, Any], Any], Optional[Any]]]) RemovableHandle
            register_forward_pre_hook(hook: Union[Callable[[T, Tuple[Any, ...]], Optional[Any]], Callable[[T, Tuple[Any, ...], Dict[str, Any]], Optional[Tuple[Any, Dict[str, Any]]]]]) RemovableHandle
            register_full_backward_hook(hook: Callable[['Module', _grad_t, _grad_t], Union[None, _grad_t]], prepend: bool) RemovableHandle
            register_full_backward_pre_hook(hook: Callable[['Module', _grad_t], Union[None, _grad_t]], prepend: bool) RemovableHandle
            register_load_state_dict_post_hook(hook)
            register_load_state_dict_pre_hook(hook)
            register_module(name: str, module: Optional['Module']) None
            register_parameter(name: str, param: Optional[Parameter]) None
            register_state_dict_post_hook(hook)
            register_state_dict_pre_hook(hook)
            requires_grad_(requires_grad: bool) T
            set_extra_state(state: Any) None
            set_submodule(target: str, module: 'Module') None
            share_memory() T
            state_dict() T_destination
            to(device: Optional[DeviceLikeType], dtype: Optional[dtype], non_blocking: bool) Self
            to_empty() T
            train(mode: bool) T
            type(dst_type: Union[dtype, str]) T
            xpu(device: Optional[Union[int, device]]) T
            zero_grad(set_to_none: bool) None
          }
          class ModuleCallEntry {
            fqn : Annotated[str, 10]
            signature : Optional[Annotated[Optional[ModuleCallSignature], 30]]
          }
          class ModuleCallEntry {
            fqn : str
            signature : Optional[ModuleCallSignature]
          }
          class ModuleCallSignature {
            forward_arg_names : Optional[Annotated[Optional[List[str]], 50]]
            in_spec : Annotated[str, 30]
            inputs : Annotated[List[Argument], 10]
            out_spec : Annotated[str, 40]
            outputs : Annotated[List[Argument], 20]
          }
          class ModuleCallSignature {
            forward_arg_names : Optional[List[str]]
            in_spec
            inputs : List[ArgumentSpec]
            out_spec
            outputs : List[ArgumentSpec]
            replace_all_uses_with(original_node, new_node)
          }
          class ModuleContext {
            nn_modules : Dict[str, Any]
            copy_graphstate()
            restore_graphstate(state)
          }
          class ModuleContextCheckpointState {
            nn_modules : Dict[str, torch.nn.Module]
            diff(other)
          }
          class ModuleCreationMode {
            name
          }
          class ModuleDict {
            bias
            qconfig
            weight
            clear() None
            items() Iterable[Tuple[str, Module]]
            keys() Iterable[str]
            pop(key: str) Module
            update(modules: Mapping[str, Module]) None
            values() Iterable[Module]
          }
          class ModuleErrorEnum {
            name
          }
          class ModuleExpansionTracer {
            is_leaf_module(module: torch.nn.Module, module_qualified_name: str) bool
            to_bool(obj: torch.fx.Proxy) bool
          }
          class ModuleInfo {
            mod_order
            mod_stats : List[ModStats]
          }
          class ModuleInfo {
            decorators : tuple
            dtypes
            dtypesIfHpu : tuple
            dtypesIfMPS : tuple
            formatted_name
            gradcheck_nondet_tol : float
            is_lazy
            module_cls
            module_error_inputs_func : NoneType
            module_inputs_func
            module_memformat_affects_out : bool
            name
            supports_gradgrad : bool
            train_and_eval_differ : bool
            get_decorators(test_class, test_name, device, dtype, param_kwargs)
            supported_dtypes(device_type)
          }
          class ModuleInput {
            constructor_input
            desc : str
            forward_input : NoneType
            reference_fn : NoneType
          }
          class ModuleList {
            append(module: Module) 'ModuleList'
            extend(modules: Iterable[Module]) Self
            insert(index: int, module: Module) None
            pop(key: Union[int, slice]) Module
          }
          class ModuleRecord {
            accessed_attrs : Dict[str, Any]
            module : module
          }
          class ModuleTest {
            FIXME_no_cuda_gradgrad_comparison
            check_forward_only
            check_gradgrad
            default_dtype
            jacobian_input
            precision
            should_test_cuda
            should_test_pickle
            noncontiguize(obj)
            test_cuda(test_case)
            test_noncontig(test_case, module, input)
          }
          class ModuleTracker {
            is_bw
            parents : Set[str]
          }
          class ModuleWithDelay {
            delay_after_loss_ms : int
            delay_before_reduction_ms : int
            module
            forward(x)
            get_input(device)
            get_loss(input, output)
            init(module_class: Type[FSDPTestModel])
            run_backward(loss)
          }
          class ModuleWrapPolicy {
          }
          class ModuleWrapper {
            cpp_module
            training
          }
          class MovePlaceholderToFront {
          }
          class MovingAverageMinMaxObserver {
            averaging_constant : float
            forward(x_orig)
          }
          class MovingAveragePerChannelMinMaxObserver {
            averaging_constant : float
            forward(x_orig)
          }
          class MulGenVmap {
            generate_vmap_rule : bool
            backward(ctx, grad_output)
            forward(x, y)
            jvp(ctx, x_tangent, y_tangent)
            setup_context(ctx, inputs, outputs)
          }
          class MulInplaceMul {
            forward(x, y)
          }
          class MultiKernel {
            args : object
            inplace_update_buffers
            inplaced_to_remove
            kernel_name
            kernels
            removed_buffers
            call_kernel(kernel_name)
            codegen_nan_check()
            get_grid_fn()
            merge_workspaces_inplace(kernels)
            warn_mix_layout(kernel_name: str)*
          }
          class MultiKernelCall {
            disable_cache
            kernels
            multi_kernel_name
            picked_kernel : NoneType, int
            run
            benchmark_sub_kernels()
            cache_file_path()
            load_cache()
            lookup_choice(multi_kernel_name: str) str
            record_choice(multi_kernel_name: str, picked_kernel_name: str)
            run()
            store_cache()
          }
          class MultiKernelState {
            subkernel_to_kernel_name : dict
            define_kernel(kernels)
          }
          class MultiLabelMarginLoss {
            forward(input: Tensor, target: Tensor) Tensor
          }
          class MultiLabelSoftMarginLoss {
            forward(input: Tensor, target: Tensor) Tensor
          }
          class MultiMarginLoss {
            margin : float
            p : int
            forward(input: Tensor, target: Tensor) Tensor
          }
          class MultiOutput {
            indices : List[Tuple[Any, ...]]
            name
            codegen(wrapper) None
            codegen_list_tuple_access(basename, indices)
            get_inputs_that_alias_output() Sequence[str]
            get_unbacked_symbol_uses() OrderedSet[sympy.Symbol]
            should_allocate() bool
          }
          class MultiOutputLayout {
            device
            get_device() Optional[torch.device]
          }
          class MultiOutputPattern {
            fns
            op
            outputs : List[Optional[PatternExpr]]
            match(node: torch.fx.Node) MatchResult
            pattern_eq(other: Any) bool
            pretty_print(pp: PatternPrettyPrinter) str
          }
          class MultiProcContinousTest {
            rank : int
            rdvz_file : Optional[str]
            world_size : int
            backend_str()* str
            opts(high_priority_stream)
            run_rank(rank: int, world_size: int, rdvz_file: Optional[str])
            setUpClass()
            tearDownClass()
          }
          class MultiProcessTestCase {
            MAIN_PROCESS_RANK : int
            TEST_ERROR_EXIT_CODE : int
            destroy_pg_upon_exit
            file_name : str
            is_master
            pid_to_pipe : dict
            processes : list
            rank : int
            skip_return_code_checks : list
            world_size
            join_or_run(fn)
            run_test(test_name: str, parent_pipe) None
            setUp() None
            tearDown() None
          }
          class MultiStepLR {
            gamma : float
            milestones : Counter
            get_lr()
          }
          class MultiTemplateBuffer {
            choice_timings
            make_kernel_render
            original_inputs : List[IRNode]
            output_plannable
            finalize_as_triton_caller(caller: TritonTemplateCallerBase) None
            get_min_choice() Tuple[ChoiceCaller, float]
            swap_as_triton_caller(caller: TritonTemplateCallerBase)
          }
          class MultiThreadedTestCase {
            MAIN_THREAD_RANK : int
            exception_queue : Queue
            rank : int
            threads : list
            world_size
            assertEqualOnRank(x, y, msg)
            assertNotEqualOnRank(x, y, msg)
            join_or_run(fn)
            perThreadSetUp()*
            perThreadTearDown()*
            run_test_with_threaded_pg(test_name, rank, world_size)
            setUp() None
            tearDown()
          }
          class MultiUseParameterConfig {
            name
          }
          class MultiformatMessageString {
            markdown : Optional[str]
            properties : Optional[_property_bag.PropertyBag]
            text : str
          }
          class MultiheadAttention {
            from_float(other)*
            from_observed(other)
          }
          class MultiheadAttention {
            bias_k
            bias_v
            dequant_k
            dequant_q
            dequant_v
            linear_K
            linear_Q
            linear_V
            out_proj
            q_scaling_product
            qconfig
            quant_attn_output
            quant_attn_output_weights
            dequantize()
            forward(query: Tensor, key: Tensor, value: Tensor, key_padding_mask: Optional[Tensor], need_weights: bool, attn_mask: Optional[Tensor], average_attn_weights: bool, is_causal: bool) Tuple[Tensor, Optional[Tensor]]
            from_float(other)
            from_observed(other)*
          }
          class MultiheadAttention {
            add_zero_attn : bool
            batch_first : bool
            bias_k : Optional[torch.Tensor]
            bias_v : Optional[torch.Tensor]
            dropout : float
            embed_dim
            head_dim
            in_proj_bias
            in_proj_weight
            k_proj_weight
            kdim : NoneType
            num_heads
            out_proj
            q_proj_weight
            v_proj_weight
            vdim : NoneType
            forward(query: Tensor, key: Tensor, value: Tensor, key_padding_mask: Optional[Tensor], need_weights: bool, attn_mask: Optional[Tensor], average_attn_weights: bool, is_causal: bool) Tuple[Tensor, Optional[Tensor]]
            merge_masks(attn_mask: Optional[Tensor], key_padding_mask: Optional[Tensor], query: Tensor) Tuple[Optional[Tensor], Optional[int]]
          }
          class Multinomial {
            arg_constraints : dict
            logits
            mean
            param_shape
            probs
            total_count : int
            variance
            entropy()
            expand(batch_shape, _instance)
            log_prob(value)
            sample(sample_shape)
            support()
          }
          class Multiple {
          }
          class MultiplexerIterDataPipe {
            buffer : List, list
            datapipes : tuple
            reset() None
          }
          class MultiplicativeLR {
            lr_lambdas : List[Callable[[int], float]], list
            optimizer
            get_lr()
            load_state_dict(state_dict)
            state_dict()
          }
          class MultiprocessContext {
            start_method : str
            pids() Dict[int, int]
          }
          class MultiprocessingRequestQueue {
            get(size, timeout: float) List[TimerRequest]
            size() int
          }
          class MultivariateNormal {
            arg_constraints : dict
            covariance_matrix
            has_rsample : bool
            loc
            mean
            mode
            precision_matrix
            scale_tril
            support
            variance
            covariance_matrix()
            entropy()
            expand(batch_shape, _instance)
            log_prob(value)
            precision_matrix()
            rsample(sample_shape: _size) torch.Tensor
            scale_tril()
          }
          class MutableBox {
            data
            dtype
            layout
            codegen_reference(writer: Optional[IndentedBuffer]) str
            constant_to_device(device: torch.device) IRNode
            freeze_layout() None
            freeze_layout_with_exact_strides(exact_strides: List[_IntLike], allow_padding: bool) None
            freeze_layout_with_fill_order(order: List[int]) None
            freeze_layout_with_same_order(stride: List[_IntLike]) None
            freeze_layout_with_stride_order(order: List[int], allow_padding: bool) None
            get_defining_op() Optional[Operation]
            get_device() Optional[torch.device]
            get_inputs_that_alias_output() Sequence[str]
            get_layout() Layout
            get_mutation_names() Sequence[str]
            get_name() str
            get_operation_name() str
            get_output_spec() OutputSpec
            get_read_names() OrderedSet[str]
            get_read_writes() dependencies.ReadWrites
            get_reads() OrderedSet[Dep]
            get_reduction_size() Sequence[sympy.Expr]
            get_reduction_type() Optional[str]
            get_size() Sequence[Expr]
            get_storage_numel() _IntLike
            get_stride() Sequence[_IntLike]
            get_unbacked_symbol_uses() OrderedSet[sympy.Symbol]
            has_exceeded_max_reads() bool
            has_large_inner_fn(threshold: Optional[int]) bool
            is_extern() bool
            is_no_op() bool
            make_indexer() Callable[[Sequence[Expr]], Expr]
            make_loader() Callable[[Sequence[Expr]], OpsValue]
            mark_reuse(users: int) None
            num_reads() int
            realize() Optional[str]
            realize_hint() None
            unwrap_view() IRNode
          }
          class MutableMappingVariable {
            generic_dict_vt
            mutation_type
            var_getattr(tx: 'InstructionTranslator', name: str) 'VariableTracker'
          }
          class MutatingFirstArgExternKernel {
            codegen(wrapper) None
            get_mutation_names()
            get_unbacked_symbol_defs() OrderedSet[sympy.Symbol]
            has_side_effects() bool
            should_allocate() bool
          }
          class MutationChecker {
            args_spec
            flat_args
            op
            real_pre_hashes
            check()
          }
          class MutationLayoutSHOULDREMOVE {
            stride
            target
            as_fixed()
            get_buffer() Buffer
            make_indexer() Callable[[Sequence[Expr]], Expr]
            real_layout()
            realize_into(src, dst, unsafe_alias)
            storage_size() sympy.Expr
          }
          class MutationOutput {
            mutating_node
            mutation_names : list
            name
            get_defining_op() Operation
            get_mutation_names()
            should_allocate() bool
          }
          class MutationTracker {
            db
            mutation_count : int
            watchers : List[weakref.ReferenceType[Any]], list
            on_mutation(name: str) None
            track(guarded_code: Any) None
          }
          class MutationType {
            scope : bool, int
          }
          class MutationType {
            name
          }
          class MyAutogradFunction {
            backward(ctx, grad_output)
            forward(ctx, x)
          }
          class MyBackwardFunc {
            backward(ctx, input)
            forward(ctx, input)
          }
          class MyClass {
            obj
          }
          class MyClass {
            a
            other
            rref
            get_value()
            increment_value(increment)
            my_class_method(d, e)
            my_instance_method(b)
            my_slow_method(my_tensor_arg)
            my_static_method(f)
          }
          class MyConvNetForMNIST {
            device
            net
            forward(x, is_rref)
          }
          class MyEmbeddingBagModel {
            eb
            forward(x)
          }
          class MyFunc {
            static_grad_ptr : NoneType
            backward(ctx, grad)
            forward(ctx, inp1, inp2)
          }
          class MyFunc {
            static_grad_ptr : NoneType
            backward(ctx, grad)
            forward(ctx, inp)
          }
          class MyFunc {
            static_grad_indices_ref : NoneType
            static_grad_ptr : NoneType
            static_grad_values_ref : NoneType
            backward(ctx, grad)
            forward(ctx, inp)
          }
          class MyFuncSingleGrad {
            static_grad_ptr : NoneType
            backward(ctx, grad)
            forward(ctx, inp)
          }
          class MyLocalCompute {
            next_stage
            forward(input)
          }
          class MyModel {
            m
            p
            forward(x)
          }
          class MyModel {
            sub_module
            forward(x)
          }
          class MyModel {
            fc1
            fc2
            forward(x)
          }
          class MyModel {
            device
            fc1
            fc2
            forward(x, opt_1, opt_2, opt_nested)
          }
          class MyModel {
            error : bool
            fc1
            forward(inp)
          }
          class MyModel {
            fc1
            fc2
            rank
            forward(inp)
          }
          class MyModel {
            fc
            forward(input)
          }
          class MyModel {
            lin
            forward(x)
          }
          class MyModule {
            lock : lock
            w
            forward(t1)
            get_w()
          }
          class MyModule {
            param1
            forward(tensor: Tensor, number: int, word: str) Tuple[str, int, Tensor]
          }
          class MyModuleInterface {
            forward()* Tensor
          }
          class MyModuleInterface {
            forward(tensor: Tensor, number: int, word: str)* Tuple[str, int, Tensor]
          }
          class MyParameterServer {
            futures : list
            gradient : NoneType
            iteration : int
            lock : lock
            total : NoneType
            trainers
            updates : int
            average(rref, riteration, tensor)
            get_gradient(rref)
          }
          class MyPickleClass {
            t : NoneType
            set(val)
          }
          class MyRemoteCompute {
            forward(input)
          }
          class MyScriptClass {
            a : int
            get_value() int
          }
          class MyScriptModule {
            a
            custom_func() Tensor
            forward() Tensor
          }
          class MyScriptModuleWithRRefs {
            rrefs : list
            forward() Tensor
          }
          class MyShardedModel1 {
            random_tensor1
            sharded_tensor1 : NoneType
            submodule
          }
          class MyShardedModel2 {
            random_tensor2
            sharded_tensor2 : NoneType
          }
          class MySubModule {
            foo(x)
            forward(x)
          }
          class NAdam {
            step(closure)
          }
          class NCCL_ALGO {
            name
          }
          class NCCL_COLL {
            name
          }
          class NCCL_HW {
            name
          }
          class NCCL_PROTO {
            name
          }
          class NLLLoss {
            ignore_index : int
            forward(input: Tensor, target: Tensor) Tensor
          }
          class NLLLoss2d {
          }
          class NNAPI_FuseCode {
            FUSED_NONE : int
            FUSED_RELU : int
            FUSED_RELU1 : int
            FUSED_RELU6 : int
          }
          class NNAPI_OperandCode {
            BOOL : int
            FLOAT16 : int
            FLOAT32 : int
            INT32 : int
            TENSOR_BOOL8 : int
            TENSOR_FLOAT16 : int
            TENSOR_FLOAT32 : int
            TENSOR_INT32 : int
            TENSOR_QUANT16_ASYMM : int
            TENSOR_QUANT16_SYMM : int
            TENSOR_QUANT8_ASYMM : int
            TENSOR_QUANT8_SYMM_PER_CHANNEL : int
            UINT32 : int
          }
          class NNAPI_OperationCode {
            ABS : int
            ADD : int
            ARGMAX : int
            ARGMIN : int
            AVERAGE_POOL_2D : int
            AXIS_ALIGNED_BBOX_TRANSFORM : int
            BATCH_TO_SPACE_ND : int
            BIDIRECTIONAL_SEQUENCE_LSTM : int
            BIDIRECTIONAL_SEQUENCE_RNN : int
            BOX_WITH_NMS_LIMIT : int
            CAST : int
            CHANNEL_SHUFFLE : int
            CONCATENATION : int
            CONV_2D : int
            DEPTHWISE_CONV_2D : int
            DEPTH_TO_SPACE : int
            DEQUANTIZE : int
            DETECTION_POSTPROCESSING : int
            DIV : int
            EMBEDDING_LOOKUP : int
            EQUAL : int
            EXP : int
            EXPAND_DIMS : int
            FLOOR : int
            FULLY_CONNECTED : int
            GATHER : int
            GENERATE_PROPOSALS : int
            GREATER : int
            GREATER_EQUAL : int
            GROUPED_CONV_2D : int
            HASHTABLE_LOOKUP : int
            HEATMAP_MAX_KEYPOINT : int
            INSTANCE_NORMALIZATION : int
            L2_NORMALIZATION : int
            L2_POOL_2D : int
            LESS : int
            LESS_EQUAL : int
            LOCAL_RESPONSE_NORMALIZATION : int
            LOG : int
            LOGICAL_AND : int
            LOGICAL_NOT : int
            LOGICAL_OR : int
            LOGISTIC : int
            LOG_SOFTMAX : int
            LSH_PROJECTION : int
            LSTM : int
            MAXIMUM : int
            MAX_POOL_2D : int
            MEAN : int
            MINIMUM : int
            MUL : int
            NEG : int
            NOT_EQUAL : int
            PAD : int
            PAD_V2 : int
            POW : int
            PRELU : int
            QUANTIZE : int
            QUANTIZED_16BIT_LSTM : int
            RANDOM_MULTINOMIAL : int
            REDUCE_ALL : int
            REDUCE_ANY : int
            REDUCE_MAX : int
            REDUCE_MIN : int
            REDUCE_PROD : int
            REDUCE_SUM : int
            RELU : int
            RELU1 : int
            RELU6 : int
            RESHAPE : int
            RESIZE_BILINEAR : int
            RESIZE_NEAREST_NEIGHBOR : int
            RNN : int
            ROI_ALIGN : int
            ROI_POOLING : int
            RSQRT : int
            SELECT : int
            SIN : int
            SLICE : int
            SOFTMAX : int
            SPACE_TO_BATCH_ND : int
            SPACE_TO_DEPTH : int
            SPLIT : int
            SQRT : int
            SQUEEZE : int
            STRIDED_SLICE : int
            SUB : int
            SVDF : int
            TANH : int
            TILE : int
            TOPK_V2 : int
            TRANSPOSE : int
            TRANSPOSE_CONV_2D : int
            UNIDIRECTIONAL_SEQUENCE_LSTM : int
            UNIDIRECTIONAL_SEQUENCE_RNN : int
          }
          class NNModuleAttrAccessorInfo {
            l1_key : Optional[str]
            l2_key : Optional[str]
            present_in_generic_dict : bool
          }
          class NNModuleSource {
            guard_source()
            name()
            reconstruct(codegen)
          }
          class NNModuleToString {
            safe_reprs : list
            can_convert_to_string(gm)
            convert(gm)
          }
          class NNModuleVariable {
            module_key : str
            module_type : type
            nn_module_stack_source : NoneType
            value
            call_function(tx, args: 'List[VariableTracker]', kwargs: 'Dict[str, VariableTracker]') 'VariableTracker'
            call_hasattr(tx: 'InstructionTranslator', name: str) 'VariableTracker'
            call_method(tx, name, args: 'List[VariableTracker]', kwargs: 'Dict[str, VariableTracker]', constant) 'VariableTracker'
            convert_to_unspecialized(tx)
            get_nn_module_stack_source()
            has_key_in_generic_dict(tx: 'InstructionTranslator', key)
            is_training(tx)
            python_type()
            set_nn_module_stack_source(source)
            unpack_var_sequence(tx)
            var_getattr(tx: 'InstructionTranslator', name)
          }
          class NNTestCase {
            check_jacobian(module, input: _TensorOrTensors, jacobian_input)
          }
          class NO_SUCH_SUBOBJ {
          }
          class NO_SUCH_SUBOBJ {
          }
          class NSSingleResultValuesType {
            name
          }
          class NSSubgraph {
            base_op_node
            end_node
            start_node
          }
          class NSTracer {
            is_leaf_module(m: torch.nn.Module, module_qualified_name: str) bool
          }
          class NVIDIA_GPU_TYPE {
            name
          }
          class NamePattern {
            description : str
            name : str
            match(event: _ProfilerEvent)
          }
          class NamedArgument {
            arg : Annotated[Argument, 20]
            name : Annotated[str, 10]
          }
          class NamedMemberAccessor {
            memo : Dict[str, torch.nn.Module]
            module : str
            check_keys(keys: Iterable[str]) Tuple[List[str], List[str]]
            del_tensor(name: str) None
            del_tensors(names: Iterable[str]) None
            get_submodule(name: str) 'torch.nn.Module'
            get_tensor(name: str) torch.Tensor
            get_tensors(names: Iterable[str]) List[torch.Tensor]
            named_buffers(remove_duplicate: bool) Iterable[Tuple[str, torch.Tensor]]
            named_modules(remove_duplicate: bool) Iterable[Tuple[str, 'torch.nn.Module']]
            named_parameters(remove_duplicate: bool) Iterable[Tuple[str, torch.Tensor]]
            named_tensors(remove_duplicate: bool) Iterable[Tuple[str, torch.Tensor]]
            set_tensor(name: str, value: torch.Tensor) None
            set_tensors(names: Iterable[str], values: Iterable[torch.Tensor]) None
            set_tensors_dict(named_tensors: Dict[str, torch.Tensor]) None
            swap_submodule(path: str, value: 'torch.nn.Module') 'torch.nn.Module'
            swap_tensor(name: str, value: torch.Tensor, allow_missing: bool) torch.Tensor
            swap_tensors(names: Iterable[str], values: Iterable[torch.Tensor], allow_missing: bool) List[torch.Tensor]
            swap_tensors_dict(named_tensors: Dict[str, torch.Tensor], allow_missing: bool) Tuple[Dict[str, torch.Tensor], List[str]]
          }
          class NamedTupleModule {
            lin
            forward(input, expected_type)
          }
          class NamedTupleVariable {
            dynamic_attributes : dict
            tuple_cls
            as_proxy()
            as_python_constant()
            call_hasattr(tx: 'InstructionTranslator', name: str) 'VariableTracker'
            call_method(tx, name, args: List[VariableTracker], kwargs: Dict[str, VariableTracker]) VariableTracker
            debug_repr()
            fields()
            is_namedtuple()
            is_structseq()
            python_type()
            reconstruct(codegen: 'PyCodegen') None
            var_getattr(tx: 'InstructionTranslator', name)
          }
          class NearlyDiagonalSparsifier {
            update_mask(module, tensor_name, nearliness)
          }
          class NegateSource {
            guard_source()
            name()
            reconstruct(codegen)*
          }
          class NegativeBinomial {
            arg_constraints : dict
            logits
            mean
            mode
            param_shape
            probs
            support
            total_count
            variance
            expand(batch_shape, _instance)
            log_prob(value)
            logits()
            probs()
            sample(sample_shape)
          }
          class NegativeInfinityType {
          }
          class NegativeIntInfinity {
            is_commutative : bool
            is_comparable : bool
            is_extended_negative : bool
            is_extended_real : bool
            is_integer : bool
            is_number : bool
            is_prime : bool
            as_powers_dict()
            ceiling()
            floor()
          }
          class NestedFunction {
            forward(a, b)
          }
          class NestedIOFunction {
            dirty_tensors : tuple
            non_differentiable : tuple
            retain_variables
            saved_tensors
            to_save : tuple
            backward() Any
            backward_extended()* None
            forward() Any
            forward_extended()* None
            mark_dirty() None
            mark_non_differentiable() None
            save_for_backward() None
          }
          class NestedIntNode {
            coeff : int
            t_id : int
            clone() 'NestedIntNode'
            eq(other: Any) Any
            ge(other: Any) Any
            gt(other: Any) Any
            is_bool() bool
            is_constant() bool
            is_float() bool
            is_int() bool
            is_nested_int() bool
            is_symbolic() bool
            le(other: Any) Any
            lt(other: Any) Any
            maybe_as_int() Optional[int]
            mul(other: Any) 'NestedIntNode'
            ne(other: Any) Any
            nested_int() int
            nested_int_coeff() int
            str() Any
            wrap_int(num: int) ConstantIntNode
          }
          class NestedLinear {
            nested_linear
            forward(x)
          }
          class NestedModel {
            fc3
            sub1
            sub2
            forward(x)
          }
          class NestedOutputModule {
            lin
            forward(inp, output_type)
          }
          class NestedSequentialModel {
            lin
            seq1
            seq2
          }
          class NestedTensor {
            lengths()
            offsets()
            values()
          }
          class NestedTensorTestCase {
            assertEqualIgnoringNestedInts(a, b)
            assertEqualNoncontigAware(a, b)
            branch_nested_state()
          }
          class NestedUserFunctionVariable {
            annotations
            closure
            code
            defaults
            f_globals
            fn_name
            kwdefaults
            wrapped_fn : Optional[VariableTracker]
            bind_args(parent, args, kwargs)
            get_code()
            get_function()
            get_globals()
            has_closure()
            has_self()
            reconstruct(codegen)
            self_args()
          }
          class NestedWrappedModule {
            module
            rank
            world_size
            forward(x)
            get_input(device)
            get_loss(input, output)
            init(group: dist.ProcessGroup, fsdp_init_mode: FSDPInitMode, device_init_mode: DEVICEInitMode, fsdp_kwargs: Optional[Dict[str, Any]], deterministic: bool) nn.Module
            run_backward(loss)
          }
          class NestedWrappedModuleWithDelay {
            init(group: dist.ProcessGroup, fsdp_init_mode: FSDPInitMode, device_init_mode: DEVICEInitMode, fsdp_kwargs: Optional[Dict[str, Any]], deterministic: bool, delay_after_loss_ms: int, delay_before_reduction_ms: int)
          }
          class Net {
            dummy_buf
            dummy_param
            l1
          }
          class Net {
            fc1
            fc2
            fc3
            no_grad_param
            relu
            forward(x)
          }
          class Net {
            lin
            forward(x)
          }
          class Net {
            lin
            relu
            forward(x)
          }
          class Net {
            linear
          }
          class Net {
            linear
            relu
          }
          class NetWithBuffers {
            a
            b
            forward(x)
          }
          class NetWithBuffers {
            a
            b
            forward(x)
          }
          class NewDim {
            size : int
            new(size: int) DimSpec
          }
          class NewGlobalVariable {
          }
          class NewModuleTest {
            check_batched_grad
            check_gradgrad
            check_inplace
            constructor_args
            cudnn
            gradcheck_fast_mode
            has_sparse_gradients
            skip_double
            skip_half
            supports_forward_ad
            supports_fwgrad_bwgrad
            test_cpu
            tf32_precision
            with_tf32
          }
          class NnapiInterfaceWrapper {
            mod
          }
          class NnapiModule {
            comp : Optional[torch.classes._nnapi.Compilation]
            compilation_preference : int
            inp_mem_fmts : List[int]
            out_mem_fmts : List[int]
            out_templates : List[torch.Tensor]
            relax_f32_to_f16 : bool
            ser_model
            shape_compute_module
            weights : List[torch.Tensor]
            forward(args: List[torch.Tensor]) List[torch.Tensor]
            init(args: List[torch.Tensor])
          }
          class NoEnterTorchFunctionMode {
          }
          class NoTest {
          }
          class NoTracerWarnContextManager {
            prev
          }
          class NoValidChoicesError {
          }
          class Node {
            next_functions
            metadata()* dict
            name()* str
            register_hook(fn: Callable[..., Any])* RemovableHandle
            register_prehook(fn: Callable[..., Any])* RemovableHandle
          }
          class Node {
            context : Optional[str]
            label : str
            referrents : List[Tuple[str, int]]
            root : bool
          }
          class Node {
            all_input_nodes
            args
            graph : str
            kwargs
            meta : Dict[str, Any]
            name : str
            next
            op : str
            prev
            stack_trace
            target : str
            type : Optional[Any]
            users : Dict['Node', None]
            append(x: 'Node') None
            format_node(placeholder_names: Optional[List[str]], maybe_return_typename: Optional[List[str]]) Optional[str]
            insert_arg(idx: int, arg: Argument) None
            is_impure() bool
            normalized_arguments(root: torch.nn.Module, arg_types: Optional[Tuple[Any]], kwarg_types: Optional[Dict[str, Any]], normalize_to_only_use_kwargs: bool) Optional[ArgsKwargsPair]
            prepend(x: 'Node') None
            replace_all_uses_with(replace_with: 'Node', delete_user_cb: Callable[['Node'], bool]) List['Node']
            replace_input_with(old_input: 'Node', new_input: 'Node') None
            update_arg(idx: int, arg: Argument) None
            update_kwarg(key: str, arg: Argument) None
          }
          class Node {
            children : Optional[List[_node.Node]]
            id : str
            label : Optional[_message.Message]
            location : Optional[_location.Location]
            properties : Optional[_property_bag.PropertyBag]
          }
          class Node {
            index : int
            pos_fw_post_order : int
          }
          class Node {
            inputs : Annotated[List[NamedArgument], 20]
            metadata : Annotated[Dict[str, str], 40]
            outputs : Annotated[List[Argument], 30]
            target : Annotated[str, 10]
          }
          class NodeAccuracySummary {
            actual_module_stack : str
            actual_node_name : str
            handle : int
            ref_module_stack : str
            ref_node_name : str
            results : Sequence[QuantizationComparisonResult]
          }
          class NodeBase {
            attributes : str
            debugName : NoneType
            inputs : NoneType
            kind : str
            scope : NoneType
            tensor_size : NoneType
          }
          class NodeDef {
            flatten_fn : Callable
            flatten_with_keys_fn : Optional[FlattenWithKeysFunc]
            type : Type[Any]
            unflatten_fn : Callable
          }
          class NodeHashException {
          }
          class NodeInfo {
            graph_id : int
            name : str
            target : str
          }
          class NodeInfo {
            indegree : int
            memory_to_free : int
          }
          class NodeInfo {
            indegree : int
            order : int
          }
          class NodeInfo {
            fw_order : Dict[fx.Node, int]
            inputs : List[fx.Node]
            required_bw_nodes : Set[fx.Node]
            required_fw_nodes
            unclaimed_nodes : Set[fx.Node]
            get_fw_order(n: fx.Node) int
            is_required_bw(n: fx.Node) bool
            is_required_fw(n: fx.Node) bool
            is_unclaimed(n: fx.Node) bool
          }
          class NodeInputOrOutputType {
            name
          }
          class NodeLatency {
            computer_latency_sec : float
            mem_latency_sec : float
          }
          class NodeMetadata {
            data : Dict[str, Any]
            copy() 'NodeMetadata'
          }
          class NodePy {
            inputs : list
          }
          class NodePyIO {
            debugName : str
            input_or_output : NoneType
            inputs : list
            kind : str
            tensor_size : list
          }
          class NodePyOP {
            attributes : str
            kind
            scopeName
          }
          class NodeScheduleMarker {
            is_reduction() bool
            only_nodes(it: Iterable[NodeScheduleEntry]) Iterable[SchedulerNode]
          }
          class NodeSource {
            action : Optional['NodeSourceAction']
            from_node : List['NodeSource']
            graph_id
            name
            node_info : Optional['NodeInfo']
            pass_name : str
            target
            print_readable(indent)
            to_dict() dict
          }
          class NodeSourceAction {
            name
          }
          class NodeSpec {
            op
            target
            call_function(target)
            call_method(target)
            call_module(target)
          }
          class NodeState {
            name
          }
          class NodeUser {
            can_inplace : bool
            is_weak : bool
            node : Union[BaseSchedulerNode, OutputNode]
            get_name() str
            merge(other: NodeUser) NodeUser
          }
          class NodeWithPriority {
            node
            priority : List[int]
          }
          class NonContGradFunc {
            backward(ctx, grad)
            forward(ctx, inp1)
          }
          class NonContGradFunc {
            static_grad_ptr : NoneType
            backward(ctx, grad)
            forward(ctx, inp1, inp2)
          }
          class NonDynamicallyQuantizableLinear {
            bias
            weight
          }
          class NonOwningLayout {
            view : Union[BaseView, TensorBox]
            make_indexer() Callable[[Sequence[Expr]], Expr]
            maybe_guard_aligned()
          }
          class NonUniformReqGradNWM {
            module
            rank
            world_size
            init(group: dist.ProcessGroup, fsdp_init_mode: FSDPInitMode, device_init_mode: DEVICEInitMode, fsdp_kwargs: Optional[Dict[str, Any]], deterministic: bool)
          }
          class NonWrapperTensor {
            new_empty(shape)
          }
          class NoneAsConstantBuffer {
            codegen_reference(writer: Optional[IndentedBuffer]) str
            get_output_spec() OutputSpec
            get_unbacked_symbol_uses() OrderedSet[sympy.Symbol]
            has_tensor_output() bool
          }
          class NoneLayout {
            device : Optional[torch.device]
            size : List[int]
            stride : List[int]
            as_fixed()
            get_device() Optional[torch.device]
            storage_size() int
          }
          class NonePair {
            compare() None
          }
          class NonzeroWorkspaceNotSupportedError {
          }
          class NoopAliasHandler {
          }
          class NoopHandler {
            frexp(x) Tuple[None, None]
            indirect_indexing(index_var, size, check, wrap_neg) sympy.Symbol
            masked(mask, body, other) None
            scan(dtypes, combine_fn, values) Tuple[None, ...]
            sort(dtypes, values, stable, descending) Tuple[None, ...]
          }
          class NoopObserver {
            custom_op : str
            dtype
            calculate_qparams()
            forward(x)
          }
          class NopInputReader {
            total : int
            storage(storage_hash, nbytes)
            symint()*
            tensor()*
          }
          class NopKernel {
            get_reads() OrderedSet[Dep]
            is_no_op() bool
          }
          class NopKernelSchedulerNode {
            last_usage
            max_order
            min_order
          }
          class NormReduction {
            norm_type : Union[int, float, str]
          }
          class Normal {
            arg_constraints : dict
            has_rsample : bool
            loc
            mean
            mode
            scale
            stddev
            support
            variance
            cdf(value)
            entropy()
            expand(batch_shape, _instance)
            icdf(value)
            log_prob(value)
            rsample(sample_shape: _size) torch.Tensor
            sample(sample_shape)
          }
          class NormalizationTestModel {
            fc1
            group_norm
            instance_norm1d
            instance_norm2d
            instance_norm3d
            layer_norm
            quant
            forward(x)
          }
          class NormalizeArgs {
            node_map : Dict[Proxy, Node]
            normalize_to_only_use_kwargs : bool
            call_function(target: Target, args: Tuple[Argument, ...], kwargs: Dict[str, Any], arg_types: Optional[Tuple[Any, ...]], kwarg_types: Optional[Dict[str, Any]])
            call_module(target: Target, args: Tuple[Argument, ...], kwargs: Dict[str, Any])
            run_node(n: Node) Any
          }
          class NormalizeOperators {
            binary_magic_method_remap : Dict[Callable[[Any, Any], Any], Callable[[Any, Any], Any]]
            call_function(target: Target, args: Tuple[Argument, ...], kwargs: Dict[str, Any])
          }
          class NormalizedLinearNode {
            node
            get_bias() torch.fx.Node
            get_input() torch.fx.Node
            get_weight() torch.fx.Node
          }
          class NormalizedMatmulNode {
            node
            get_input() torch.fx.Node
            get_other() torch.fx.Node
          }
          class NotEqualError {
          }
          class NotSupportedError {
          }
          class NotView {
            regenerate_view(bases_list: List[Tensor])
          }
          class Notification {
            associated_rule : Optional[_reporting_descriptor_reference.ReportingDescriptorReference]
            descriptor : Optional[_reporting_descriptor_reference.ReportingDescriptorReference]
            exception : Optional[_exception.Exception]
            level : Literal['none', 'note', 'warning', 'error']
            locations : Optional[List[_location.Location]]
            message
            properties : Optional[_property_bag.PropertyBag]
            thread_id : Optional[int]
            time_utc : Optional[str]
          }
          class NullContextManager {
            forward(x)
          }
          class NullContextVariable {
            enter(tx)
            exit(tx: 'InstructionTranslator')
            fn_name()
            module_name()
          }
          class NullHandler {
          }
          class NullKernelHandler {
            index_dtype : str
            inplaced_to_remove
            removed_buffers
          }
          class NullLine {
          }
          class NullMetricHandler {
            emit(metric_data: MetricData)*
          }
          class NullVariable {
            reconstruct(codegen)
          }
          class NumberPair {
            atol
            check_dtype : bool
            equal_nan : bool
            rtol
            compare() None
            extra_repr() Sequence[str]
          }
          class NumpyCompatNormalization {
            cache : Dict['torch.fx.graph.Target', OrderedSet[str]]
            inverse_mapping : Dict[str, str]
            numpy_compat : Dict[str, Tuple[str, ...]]
          }
          class NumpyCube {
            backward(ctx, grad_output, grad_saved)
            forward(input)
            jvp(ctx, input_tangent)
            setup_context(ctx, inputs, output)
            vmap(info, in_dims, input)
          }
          class NumpyCubeNotComposable {
            backward(ctx, grad_output, grad_saved)
            forward(input)
            setup_context(ctx, inputs, output)
          }
          class NumpyDTypeVariable {
            as_proxy()
          }
          class NumpyExp_ {
            backward(ctx, grad_output)
            forward(x)
            jvp(ctx, x_tangent)
            setup_context(ctx, inputs, output)
            vmap(info, in_dims, x)
          }
          class NumpyMul {
            backward(ctx, grad_output)
            forward(x, y)
            jvp(ctx, x_tangent, y_tangent)
            setup_context(ctx, inputs, output)
            vmap(info, in_dims, x, y)
          }
          class NumpyNdarrayVariable {
            call_method(tx, name, args: 'List[VariableTracker]', kwargs: 'Dict[str, VariableTracker]') 'VariableTracker'
            create(tx: 'InstructionTranslator', proxy)
            patch_args(name, args, kwargs)
            python_type()
            var_getattr(tx: 'InstructionTranslator', name)
          }
          class NumpySort {
            backward(ctx, grad_output, _0, _1)
            forward(x, dim)
            jvp(ctx, x_tangent, _)
            setup_context(ctx, inputs, output)
            vmap(info, in_dims, x, dim)
          }
          class NumpyTake {
            backward(ctx, grad_output)
            forward(x, ind, ind_inv, dim)
            jvp(ctx, x_tangent, ind_tangent, ind_inv_tangent, _)
            setup_context(ctx, inputs, output)
            vmap(info, in_dims, x, ind, ind_inv, dim)
          }
          class NumpyTensorSource {
            guard_source()
            name() str
            reconstruct(codegen)
          }
          class NumpyTypeInfoVariable {
          }
          class NumpyVariable {
            constant_fold_functions : tuple
            value
            as_proxy()
            as_python_constant()
            call_function(tx: 'InstructionTranslator', args: 'List[VariableTracker]', kwargs: 'Dict[str, VariableTracker]') 'VariableTracker'
            call_method(tx, name, args: 'List[VariableTracker]', kwargs: 'Dict[str, VariableTracker]') 'VariableTracker'
            can_constant_fold_through(fn)
            get_constant_collection_for_func(fn)
          }
          class ODictGetItemSource {
            index : Any
            guard_source()
            name()
            reconstruct(codegen)
          }
          class ONNXFakeContext {
            fake_mode
            state_dict_paths : tuple[str | io.BytesIO | dict[str, Any]] | None
          }
          class ONNXFunction {
            is_complex : bool
            is_custom : bool
            onnx_function : onnxscript.OnnxFunction | onnxscript.TracedOnnxFunction
            op_full_name : str
          }
          class ONNXProgram {
            exported_program : torch.export.ExportedProgram | None
            model
            model_proto
            apply_weights(state_dict: dict[str, torch.Tensor]) None
            initialize_inference_session(initializer: Callable[[str | bytes], ort.InferenceSession]) None
            optimize() None
            release() None
            save(destination: str | os.PathLike)
          }
          class ONNXRegistry {
            functions : dict[TorchOp | str, list[OnnxDecompMeta]]
            opset_version
            from_torchlib() ONNXRegistry
            get_decomps(target: TorchOp) list[OnnxDecompMeta]
            is_registered(target: TorchOp) bool
            register_op(target: TorchOp, function: Callable, is_complex: bool) None
          }
          class ONNXRuntimeOptions {
            execution_provider_options : Sequence[dict[Any, Any]] | None
            execution_providers : Sequence[str | tuple[str, dict[Any, Any]]] | None
            session_options : Sequence[onnxruntime.SessionOptions] | None
          }
          class ONNXTorchPatcher {
            paths : List[Union[str, io.BufferedIOBase]]
            safetensors_torch_load_file
            safetensors_torch_load_file_wrapper
            torch_fx__symbolic_trace__wrapped_methods_to_patch : list
            torch_load
            torch_load_wrapper
            transformers_modeling_utils_safe_load_file
          }
          class ONNXTracedModule {
            inner
            strict : bool
            forward()
          }
          class ObjMismatchError {
          }
          class ObjNotFoundError {
          }
          class ObjectPair {
            compare() None
          }
          class ObjectPair {
            CLS : object
          }
          class ObservationType {
            name
          }
          class ObservedAttributeError {
          }
          class ObservedException {
          }
          class ObservedGraphModule {
            preserved_attr_names
          }
          class ObservedGraphModuleAttrs {
            equalization_node_name_to_qconfig : Dict[str, Any]
            is_observed_standalone_module : bool
            is_qat : bool
            node_name_to_qconfig : Dict[str, QConfigAny]
            node_name_to_scope : Dict[str, Tuple[str, type]]
            observed_node_names : Set[str]
            prepare_custom_config
            qconfig_mapping
            standalone_module_input_quantized_idxs : Optional[List[int]]
            standalone_module_output_quantized_idxs : Optional[List[int]]
          }
          class ObservedKeyError {
          }
          class ObservedStandaloneGraphModule {
          }
          class ObservedUserStopIteration {
            value : Optional[Any]
          }
          class Observer {
            env
            id
            run_episode(agent_rref, n_steps)
          }
          class ObserverBase {
            dtype
            is_dynamic : bool
            with_args : classmethod
            with_callable_args : classmethod
            calculate_qparams()*
            forward(x)*
          }
          class OffloadPolicy {
          }
          class OffloadWrapper {
            forward()
          }
          class OffsetBasedRNGTracker {
            get_offset(name: str) int
            set_offset(name: str, offset: int) None
          }
          class OneCycleLR {
            cycle_momentum : bool
            optimizer
            total_steps
            use_beta1
            get_lr()
          }
          class OneHotCategorical {
            arg_constraints : dict
            has_enumerate_support : bool
            logits
            mean
            mode
            param_shape
            probs
            support
            variance
            entropy()
            enumerate_support(expand)
            expand(batch_shape, _instance)
            log_prob(value)
            sample(sample_shape)
          }
          class OneHotCategoricalStraightThrough {
            has_rsample : bool
            rsample(sample_shape: _size) torch.Tensor
          }
          class OnnxBackend {
            name
          }
          class OnnxDecompMeta {
            device : Literal['cuda', 'cpu'] | str | None
            fx_target : Union
            is_complex : bool
            is_custom : bool
            onnx_function : Callable
          }
          class OnnxExporterError {
          }
          class OnnxExporterWarning {
          }
          class OnnxFunctionDispatcher {
            diagnostic_context
            onnx_registry
            dispatch(node: torch.fx.Node, onnx_args: Sequence[fx_type_utils.TensorLike | str | int | float | bool | list | complex | None], onnx_kwargs: dict[str, fx_type_utils.Argument], diagnostic_context: diagnostics.DiagnosticContext) onnxscript.OnnxFunction | onnxscript.TracedOnnxFunction
            get_function_overloads(node: torch.fx.Node, diagnostic_context: diagnostics.DiagnosticContext) list[registration.ONNXFunction]
          }
          class OnnxRegistry {
            opset_version
            get_op_functions(namespace: str, op_name: str, overload: str | None) list[registration.ONNXFunction] | None
            is_registered_op(namespace: str, op_name: str, overload: str | None) bool
            register_op(function: onnxscript.OnnxFunction | onnxscript.TracedOnnxFunction, namespace: str, op_name: str, overload: str | None, is_complex: bool) None
          }
          class OnnxTestCaseRepro {
            inputs : dict
            outputs : dict
            proto
            repro_dir
            create_test_case_repro(proto: bytes, inputs, outputs, dir: str, name: str | None)
            validate(options: VerificationOptions)
          }
          class Op {
            args : List[Union[Param, Intermediate]]
            fn_call_name : Optional[str]
            name : str
            ret
          }
          class OpCheckError {
          }
          class OpCheckMode {
            failures_dict : str
            failures_dict_path : str
            namespaces : List[str]
            prev_dynamo_disable : str
            prev_is_opcheck_mode : int
            seen_ops_to_errors : dict
            test_name : str
            test_util : Callable
            test_util_name : str
            maybe_raise_errors_on_exit() None
            run_test_util(op, args, kwargs)
          }
          class OpCountResult {
            nontrivial_read_count : int
            num_ops : int
            read_buffers : List[str]
            used_ops : OrderedSet[str]
          }
          class OpCounterCSE {
            op_count : int
            parent_handler
            var_names : dict
            bucketize(values: T, boundaries: Tuple[str, sympy.Expr, sympy.Expr, sympy.Expr], boundary_indices: T, indexing_dtype: torch.dtype, right: bool, sorter: Optional[Tuple[str, sympy.Expr]], sorter_indices: Optional[T]) T
            getvalue()
            indirect_indexing()
            load(name: str, index: sympy.Expr) str
            load_seed(name: str, offset: T)
          }
          class OpDTypes {
            name
          }
          class OpDecompositions {
            ceil_to_int(a, dtype)
            erfc(x)
            erfcx(x)
            exp2(x)
            expm1(x)
            floor_to_int(a, dtype)
            fma(x, y, z)
            identity(value)
            log10(x)
            log1p(x)
            log2(x)
            reciprocal(x)
            relu(x)
            remainder(a, b)
            round_to_int(a, dtype)
            sigmoid(x)
            square(x)
            trunc_to_int(a, dtype)
          }
          class OpDispatcher {
            sharding_propagator
            dispatch(op_call: torch._ops.OpOverload, args: Tuple[object, ...], kwargs: Dict[str, object]) object
            redistribute_local_args(op_info: OpInfo, suggested_input_schema: OpSchema) None
            unwrap_to_op_info(op_call: torch._ops.OpOverload, args: Tuple[object, ...], kwargs: Dict[str, object]) OpInfo
            wrap(res: object, spec: OutputSpecType) object
          }
          class OpDtypeRule {
            override_return_dtype : Optional[torch.dtype]
            type_promotion_kind : ELEMENTWISE_TYPE_PROMOTION_KIND
          }
          class OpDtypeSupport {
            convert_outputs : Dict[str, bool]
            supported_dtypes : Dict[str, OrderedSet[torch.dtype]]
            register_upcast(func: Callable[..., str], convert_output: bool)
          }
          class OpInfo {
            args_tree_spec : Optional[TreeSpec]
            flat_args_schema : List[object]
            local_args : Sequence[object]
            local_kwargs : Dict[str, object]
            mesh
            output_sharding : Optional[OutputSharding]
            schema
          }
          class OpInfo {
            aliases : Optional[Iterable]
            allow_cow_input_materialize_backward : Optional[List[Union[int, str]]]
            allow_cow_input_materialize_forward : Optional[List[Union[int, str]]]
            assert_autodiffed : bool
            assert_jit_shape_analysis : bool
            aten_backward_name : Optional[str]
            aten_name : Optional[str]
            autodiff_fusible_nodes : Optional[List[str]]
            autodiff_nonfusible_nodes : Optional[List[str]]
            backward_dtypes : Optional[_dispatch_dtypes]
            backward_dtypesIfCUDA : Optional[_dispatch_dtypes]
            backward_dtypesIfHpu : Optional[_dispatch_dtypes]
            backward_dtypesIfROCM : Optional[_dispatch_dtypes]
            check_batched_forward_grad : Optional[bool]
            check_batched_grad : Optional[bool]
            check_batched_gradgrad : Optional[bool]
            check_inplace_batched_forward_grad : Optional[bool]
            decomp_aten_name : Optional[str]
            decorators : Tuple
            dtypes : Optional[_dispatch_dtypes]
            dtypesIfCUDA : Optional[_dispatch_dtypes]
            dtypesIfHpu : Optional[_dispatch_dtypes]
            dtypesIfROCM : Optional[_dispatch_dtypes]
            dtypesIfXPU : Optional[_dispatch_dtypes]
            dynamic_dtypes
            error_inputs_func : Optional[Callable]
            error_inputs_sparse_func : Optional[Callable]
            formatted_name
            full_name
            gradcheck_fast_mode : Optional[bool]
            gradcheck_nondet_tol : float
            gradcheck_wrapper : Callable
            inplace_operator_variant : Callable
            inplace_variant : Callable
            is_factory_function : bool
            method_variant : Callable
            name : str
            op : Optional[Callable]
            operator_variant : Callable
            promotes_int_to_float : bool
            ref : Optional[Callable]
            reference_inputs_func : Optional[Callable]
            sample_inputs_func : Optional[Callable]
            sample_inputs_sparse_bsc_func : Optional[Callable]
            sample_inputs_sparse_bsr_func : Optional[Callable]
            sample_inputs_sparse_coo_func : Optional[Callable]
            sample_inputs_sparse_csc_func : Optional[Callable]
            sample_inputs_sparse_csr_func : Optional[Callable]
            skip_correctness_check_compile_vs_eager : bool
            skip_cow_input_backward : bool
            skips : Tuple
            supports_autograd : bool
            supports_cow_input_no_materialize_backward : bool
            supports_cow_input_no_materialize_forward : bool
            supports_expanded_weight : bool
            supports_forward_ad : bool
            supports_fwgrad_bwgrad : bool
            supports_gradgrad : Optional[bool]
            supports_inplace_autograd : Optional[bool]
            supports_njt : Optional[bool]
            supports_out : bool
            supports_scripting : bool
            supports_sparse : Optional[bool]
            supports_sparse_bsc : Optional[bool]
            supports_sparse_bsr : Optional[bool]
            supports_sparse_csc : Optional[bool]
            supports_sparse_csr : Optional[bool]
            supports_tracing : bool
            supports_varargs : bool
            test_conjugated_samples : bool
            test_neg_view : bool
            variant_test_name : str
            conjugate_sample_inputs(device, dtype, requires_grad)
            error_inputs(device)
            error_inputs_sparse(device, layout)
            get_decorators(test_class, test_name, device, dtype, param_kwargs)
            get_inplace()
            get_inplace_operator()
            get_method()
            get_op()
            get_operator()
            reference_inputs(device, dtype, requires_grad)
            sample_inputs(device, dtype, requires_grad)
            sample_inputs_sparse(layout, device, dtype, requires_grad)
            sample_inputs_sparse_bsc(device, dtype, requires_grad)
            sample_inputs_sparse_bsr(device, dtype, requires_grad)
            sample_inputs_sparse_coo(device, dtype, requires_grad)
            sample_inputs_sparse_csc(device, dtype, requires_grad)
            sample_inputs_sparse_csr(device, dtype, requires_grad)
            supported_backward_dtypes(device_type)
            supported_dtypes(device_type)
            supports_dtype(dtype, device_type) bool
            supports_sparse_layout(layout)
          }
          class OpName {
            namespace : str
            op_name : str
            overload : str
            from_builtin_function(builtin_function: types.BuiltinFunctionType) OpName
            from_name_parts(namespace: str, op_name: str, overload: str | None) OpName
            from_op_overload(op_overload: torch._ops.OpOverload) OpName
            from_qualified_name(qualified_name: str) OpName
            qualified_name() str
          }
          class OpOverload {
            is_view
            namespace
            op
            overloadpacket
            tags
            decompose()
            has_kernel_for_any_dispatch_key(ks)
            has_kernel_for_dispatch_key(k)
            name()
            redispatch()
          }
          class OpOverloadPacket {
            op
            overloads()
          }
          class OpOverrides {
            bitwise_and(x, y)
            bitwise_left_shift(x, y)
            bitwise_not(x)
            bitwise_or(x, y)
            bitwise_right_shift(x, y)
            bitwise_xor(x, y)
            constant(value, dtype)
            int_truediv(a, b)
            libdevice_abs(x)
            libdevice_cos(x)
            libdevice_exp(x)
            libdevice_log(x)
            libdevice_sigmoid(x)
            libdevice_sin(x)
            libdevice_sqrt(x)
            load_seed(name, offset)
            logical_not(a)
            paren(string: str) str
          }
          class OpPatcher {
          }
          class OpRecorder {
            constant_farm : dict[Any, ir.Value]
            functions : dict[ir.OperatorIdentifier, onnxscript.OnnxFunction | ir.Function]
            nodes : list[ir.Node]
            opset
            eval(schema: onnx.defs.OpSchema, args: Sequence[AllowedArgType], kwargs: Mapping[str, AllowedArgType]) _tensors.SymbolicTensor | Sequence[_tensors.SymbolicTensor]
            eval_function(function: onnxscript.OnnxFunction, args: Sequence[AllowedArgType], kwargs: Mapping[str, AllowedArgType]) _tensors.SymbolicTensor | Sequence[_tensors.SymbolicTensor] | bool | int
          }
          class OpSchema {
            args_schema : Tuple
            args_spec
            args_strategy
            has_symints : bool
            kwargs_schema : Dict
            op
            schema_info : Optional[RuntimeSchemaInfo]
            arg_type_tensor_or_tensor_list_like(arg_idx: int) bool
            gen_fake_args() ArgsType
            gen_fake_kwargs() KwargsType
            return_type_tensor() bool
            return_type_tuple_tensor_like() bool
          }
          class OpSignature {
            domain : str
            name : str
            opset_version : int | None
            outputs : Sequence[Parameter]
            overload : str
            params : Sequence[Parameter | AttributeParameter]
            params_map : Mapping[str, Parameter | AttributeParameter]
            from_function(func, domain: str, name: str | None, overload: str) OpSignature
            from_opschema(opschema: onnx.defs.OpSchema) OpSignature
            get(name: str) Parameter | AttributeParameter
          }
          class OpStrategy {
            mesh_shape
            ndim
            shape
            strategies : List[PlacementStrategy], list
            max_num_shards() int
          }
          class OpSupports {
            decline_if_input_dtype(dtype: torch.dtype) OperatorSupportBase
            decline_if_node_in_names(disallow_set: t.Set[str]) OperatorSupportBase
          }
          class OpTree {
            sorted_nodes
            dfs() Iterator[_ProfilerEvent]
          }
          class OpTypes {
            compute_intensive_ops : Set[Callable]
            fusible_ops : Set[Callable]
            random_ops : Set[Callable]
            recomputable_ops : Set[Callable]
            view_ops : Set[Callable]
            is_compute_intensive(node: fx.Node)
            is_fusible(node: fx.Node)
            is_random(node: fx.Node)
            is_recomputable(node: fx.Node)
            is_view(node: fx.Node)
          }
          class OpaqueUnaryFn {
            eval(a)
          }
          class Operand {
            dim_order
            op_type : int
            scale : float
            shape : Tuple[int, ...]
            zero_point : int
            use_nchw()
          }
          class OperandValueSourceType {
            IMMEDIATE : int
            NUMBERED_BUFFER : int
            NUMBERED_MEMORY : int
          }
          class Operation {
            operation_name : Optional[str]
            get_device()* Optional[torch.device]
            get_operation_name() str
            get_origin_node() Optional[torch.fx.Node]
            get_origins() OrderedSet[Any]
            get_outputs()* List[Buffer]
            get_read_names() OrderedSet[str]
            get_read_writes()* dependencies.ReadWrites
            get_reads() OrderedSet[Dep]
            get_unbacked_symbol_defs() OrderedSet[sympy.Symbol]
            get_unbacked_symbol_uses() OrderedSet[sympy.Symbol]
            get_workspace_size() int
            is_extern() bool
            is_no_op() bool
            is_user_of(name: str) bool
          }
          class OperationBuffer {
            get_operation_name
            get_defining_op() Operation
            get_outputs() List[Buffer]
          }
          class OperatorBase {
            functorch_table : dict
            py_kernels : Dict[DispatchKey, Callable[..., Any]]
            python_key_table : Dict[Union[Type[TorchDispatchMode], Type[torch.Tensor]], Callable[..., Any]]
            has_kernel_for_any_dispatch_key(ks)
            has_kernel_for_dispatch_key(k)
            name()*
            py_functionalize_impl(fn: _F) _F
            py_impl(k: Any) Callable[[_F], _F]
          }
          class OperatorConfig {
            config
            operators : List[OperatorPatternType]
          }
          class OperatorIssue {
            operator_str(target: Any, args: List[Any], kwargs: dict[str, Any]) str
          }
          class OperatorSupport {
            is_node_supported(submodules: t.Mapping[str, torch.nn.Module], node: torch.fx.Node) bool
          }
          class OperatorSupportBase {
            is_node_supported(submodules: t.Mapping[str, torch.nn.Module], node: torch.fx.Node)* bool
          }
          class OpsHandler {
            abs(x0: T) T
            acos(x0: T) T
            acosh(x0: T) T
            add(x0: T, x1: T) T
            airy_ai(x: T) T
            and_(x0: T, x1: T) T
            asin(x0: T) T
            asinh(x0: T) T
            atan(x0: T) T
            atan2(x0: T, x1: T) T
            atanh(x0: T) T
            bessel_j0(x: T) T
            bessel_j1(x: T) T
            bessel_y0(x: T) T
            bessel_y1(x: T) T
            bitwise_and(x0: T, x1: T) T
            bitwise_left_shift(x0: T, x1: T) T
            bitwise_not(x0: T) T
            bitwise_or(x0: T, x1: T) T
            bitwise_right_shift(x0: T, x1: T) T
            bitwise_xor(x0: T, x1: T) T
            bucketize(values: T, boundaries: Tuple[str, sympy.Expr, sympy.Expr, sympy.Expr], boundary_indices: T, indexing_dtype: torch.dtype, right: bool, sorter: Optional[Tuple[str, sympy.Expr]], sorter_indices: Optional[T]) T
            ceil(x0: T) T
            ceil_to_int(x: T, dtype: torch.dtype) T
            chebyshev_polynomial_t(x: T, y: T) T
            chebyshev_polynomial_u(x: T, y: T) T
            chebyshev_polynomial_v(x: T, y: T) T
            chebyshev_polynomial_w(x: T, y: T) T
            constant(value: Union[bool, float, int], dtype: torch.dtype) T
            copysign(x0: T, x1: T) T
            cos(x0: T) T
            cosh(x0: T) T
            digamma(x: T) T
            div(x0: T, x1: T) T
            eq(x0: T, x1: T) T
            erf(x0: T) T
            erfc(x0: T) T
            erfcx(x: T) T
            erfinv(x0: T) T
            exp(x0: T) T
            exp2(x0: T) T
            expm1(x0: T) T
            floor(x0: T) T
            floor_to_int(x: T, dtype: torch.dtype) T
            floordiv(x0: T, x1: T) T
            fma(x: T, y: T, z: T) T
            fmod(x0: T, x1: T) T
            frexp(x0: T)
            gammainc(x: T, y: T) T
            gammaincc(x: T, y: T) T
            ge(x0: T, x1: T) T
            getitem(x0: T, x1: T) T
            gt(x0: T, x1: T) T
            hermite_polynomial_h(x: T, y: T) T
            hermite_polynomial_he(x: T, y: T) T
            hypot(x0: T, x1: T) T
            i0(x: T) T
            i0e(x: T) T
            i1(x: T) T
            i1e(x: T) T
            identity(x: T) T
            igamma(x: T, y: T) T
            igammac(x: T, y: T) T
            index_expr(expr: sympy.Expr, dtype: torch.dtype) T
            indirect_indexing(x: T, size: sympy.Expr, check: bool, wrap_neg) sympy.Expr
            int_truediv(x0: T, x1: T) T
            invert(x0: T) T
            isinf(x0: T) T
            isnan(x0: T) T
            laguerre_polynomial_l(x: T, y: T) T
            le(x0: T, x1: T) T
            legendre_polynomial_p(x: T, y: T) T
            lgamma(x0: T) T
            libdevice_abs(x0: T) T
            libdevice_cos(x0: T) T
            libdevice_exp(x0: T) T
            libdevice_log(x0: T) T
            libdevice_sigmoid(x0: T) T
            libdevice_sin(x0: T) T
            libdevice_sqrt(x0: T) T
            load(name: str, index: sympy.Expr) T
            load_seed(name: str, offset: T)
            log(x0: T) T
            log10(x0: T) T
            log1p(x0: T) T
            log2(x0: T) T
            log_ndtr(x: T) T
            logical_and(x0: T, x1: T) T
            logical_not(x0: T) T
            logical_or(x0: T, x1: T) T
            logical_xor(x0: T, x1: T) T
            lshift(x0: T, x1: T) T
            lt(x0: T, x1: T) T
            masked(mask: T, body: Callable[[], T], other: T) T
            matmul(x0: T, x1: T) T
            maximum(x0: T, x1: T) T
            minimum(x0: T, x1: T) T
            mod(x0: T, x1: T) T
            modified_bessel_i0(x: T) T
            modified_bessel_i1(x: T) T
            modified_bessel_k0(x: T) T
            modified_bessel_k1(x: T) T
            mul(x0: T, x1: T) T
            ndtr(x: T) T
            ndtri(x: T) T
            ne(x0: T, x1: T) T
            neg(x0: T) T
            nextafter(x0: T, x1: T) T
            or_(x0: T, x1: T) T
            polygamma(x: T, y: T) T
            pow(x0: T, x1: T) T
            rand(seed: T, offset: T) T
            randint64(seed: T, offset: T, low: T, high: T) T
            randn(seed: T, offset: T) T
            reciprocal(x0: T) T
            reduction(dtype: torch.dtype, src_dtype: torch.dtype, reduction_type: ReductionType, value: T) Union[T, Tuple[T, ...]]
            relu(x0: T) T
            remainder(x0: T, x1: T) T
            round(x0: T) T
            round_decimal(x0: T, x1: T) T
            round_to_int(x: T, dtype: torch.dtype) T
            rshift(x0: T, x1: T) T
            rsqrt(x0: T) T
            scaled_modified_bessel_k0(x: T) T
            scaled_modified_bessel_k1(x: T) T
            scan(dtypes: Tuple[torch.dtype, ...], combine_fn: Callable[[Tuple[T, ...], Tuple[T, ...]], Tuple[T, ...]], values: Tuple[T, ...]) Tuple[T, ...]
            shifted_chebyshev_polynomial_t(x: T, y: T) T
            shifted_chebyshev_polynomial_u(x: T, y: T) T
            shifted_chebyshev_polynomial_v(x: T, y: T) T
            shifted_chebyshev_polynomial_w(x: T, y: T) T
            sigmoid(x0: T) T
            sign(x0: T) T
            signbit(x0: T) T
            sin(x0: T) T
            sinh(x0: T) T
            sort(dtypes: Tuple[torch.dtype, ...], values: Tuple[T, ...], stable: bool, descending: bool) Tuple[T, ...]
            spherical_bessel_j0(x: T) T
            sqrt(x0: T) T
            store(name: str, index: sympy.Expr, value: T, mode: StoreMode) None
            store_reduction(name: str, index: sympy.Expr, value: T) T
            sub(x0: T, x1: T) T
            tan(x0: T) T
            tanh(x0: T) T
            to_dtype(x: T, dtype: torch.dtype, src_dtype: Optional[torch.dtype], use_compute_types) T
            to_dtype_bitcast(x: T, dtype: torch.dtype, src_dtype: torch.dtype) T
            truediv(x0: T, x1: T) T
            trunc(x0: T) T
            trunc_to_int(x: T, dtype: torch.dtype) T
            truncdiv(x0: T, x1: T) T
            where(condition: T, input: T, other: T) T
            xor(x0: T, x1: T) T
            zeta(x: T, y: T) T
          }
          class OpsValue {
            value : Any
          }
          class OpsWrapper {
            indirect_indexing(index, size, check, wrap_neg)
          }
          class OptEinsumModule {
            enabled
            strategy : NoneType
          }
          class OptState {
            name
          }
          class OptimStateDictConfig {
            offload_to_cpu : bool
          }
          class OptimStateKeyType {
            name
          }
          class OptimizationContext {
            dtype : Optional[torch.dtype]
            key : ClassVar[str]
            ops_name : str
          }
          class OptimizeContext {
          }
          class OptimizedModule {
            dynamo_ctx
            forward
            get_compiler_config : Callable[[], Any]
            training
          }
          class OptimizedPythonReferenceAnalysis {
            sym_sum(args)
          }
          class Optimizer {
            OptimizerPostHook : TypeAlias, _ExtensionsSpecialForm
            OptimizerPreHook : TypeAlias, _ExtensionsSpecialForm
            defaults : Dict[str, Any]
            param_groups : List[Dict[str, Any]]
            state : DefaultDict[torch.Tensor, Any]
            add_param_group(param_group: Dict[str, Any]) None
            load_state_dict(state_dict: StateDict) None
            profile_hook_step(func: Callable[_P, R]) Callable[_P, R]
            register_load_state_dict_post_hook(hook: Callable[['Optimizer'], None], prepend: bool) RemovableHandle
            register_load_state_dict_pre_hook(hook: Callable[['Optimizer', StateDict], Optional[StateDict]], prepend: bool) RemovableHandle
            register_state_dict_post_hook(hook: Callable[['Optimizer', StateDict], Optional[StateDict]], prepend: bool) RemovableHandle
            register_state_dict_pre_hook(hook: Callable[['Optimizer'], None], prepend: bool) RemovableHandle
            register_step_post_hook(hook: OptimizerPostHook) RemovableHandle
            register_step_pre_hook(hook: OptimizerPreHook) RemovableHandle
            state_dict() StateDict
            step(closure: None) None
            zero_grad(set_to_none: bool) None
          }
          class OptimizerErrorEnum {
            name
          }
          class OptimizerFailingOnConstructor {
            step(closure)*
          }
          class OptimizerInfo {
            decorators : tuple
            has_capturable_arg : bool
            metadata_for_sparse : tuple
            name
            not_og_supported_flags : tuple
            only_supports_sparse_grads : bool
            optim_cls
            optim_error_inputs_func : NoneType
            optim_inputs_func
            scheduler_inputs : tuple
            step_requires_closure : bool
            supported_impls : tuple
            supports_complex : bool
            supports_fused_on : tuple
            supports_multiple_devices : bool
            supports_param_groups : bool
            supports_sparse : bool
            get_decorators(test_class, test_name, device, dtype, param_kwargs)
          }
          class OptimizerInput {
            desc : str
            kwargs : Dict[str, Any]
            params : Union[List[Parameter], List[Tensor], Dict[Any, Any], List[Dict[str, Any]]]
          }
          class OptimizerSingleTensorPattern {
            description : str
            name : str
            optimizers_with_foreach : list
            url : str
            match(event: _ProfilerEvent)
          }
          class OptimizerSource {
            guard_source()
            name()
            reconstruct(codegen)
          }
          class OptimizerVariable {
            grad_to_source : dict
            static_tensor_names : set
            tensor_to_source : dict
            call_method(tx, name, args: 'List[VariableTracker]', kwargs: 'Dict[str, VariableTracker]') 'VariableTracker'
            create_finalizer(tx)
            get_python_args()
            graph_break_if_pending_mutation(tx)
            map_sources_and_install_guards(tx)
            move_step_if_cpu()
            update_list_args(tx: 'InstructionTranslator', args, kwargs, py_args, py_kwargs)
            var_getattr(tx: 'InstructionTranslator', name)
            wrap_tensor(tx: 'InstructionTranslator', tensor_value)
          }
          class OptionalInput {
            forward(x, y)
          }
          class OptionalTensorArgument {
            as_none : Annotated[bool, 10]
            as_tensor : Annotated[TensorArgument, 20]
          }
          class OrderedDictWrapper {
            items()
            keys()
            values()
          }
          class OrderedDictWrapper {
            attr
            cpp_dict
            cpp_module
            items()
            keys()
            values()
          }
          class OrderedImporter {
            import_module(module_name: str) ModuleType
            whichmodule(obj: Any, name: str) str
          }
          class OrderedModuleDict {
            items()
          }
          class OrderedSet {
            add(elem: T) None
            clear() None
            copy() OrderedSet[T]
            difference() OrderedSet[T]
            difference_update() None
            discard(elem: T) None
            intersection() OrderedSet[T]
            intersection_update() None
            issubset(other: Iterable[T]) bool
            issuperset(other: Iterable[T]) bool
            pop() T
            symmetric_difference(other: Iterable[T]) OrderedSet[T]
            symmetric_difference_update(other: Iterable[T]) None
            union() OrderedSet[T]
            update() None
          }
          class OrderedSetHolder {
            items : List[Any]
          }
          class OrtBackend {
            execution_count : int
            preallocate_output : bool
            run
            clear_cached_instances()
            compile(graph_module: torch.fx.GraphModule, args) torch.fx.GraphModule
            get_cached_instance_for_options(options: Optional[Union[OrtBackendOptions, Mapping[str, Any]]]) 'OrtBackend'
            get_cached_instances()
          }
          class OrtBackendOptions {
            default_execution_providers : Optional[Sequence[OrtExecutionProvider]]
            export_options : Optional['torch.onnx.ExportOptions']
            infer_execution_providers : bool
            ort_session_options : Optional['onnxruntime.SessionOptions']
            pre_ort_model_transforms : Optional[Sequence[Callable[['onnx.ModelProto'], None]]]
            preallocate_output : bool
            preferred_execution_providers : Optional[Sequence[OrtExecutionProvider]]
            use_aot_autograd : bool
          }
          class OrtExecutionInfoForAllGraphModules {
            execution_info_per_graph_module : Dict[torch.fx.GraphModule, List[OrtExecutionInfoPerSession]]
            cache_session_execution_info(graph_module: torch.fx.GraphModule, info: OrtExecutionInfoPerSession)
            search_reusable_session_execution_info(graph_module: torch.fx.GraphModule)
          }
          class OrtExecutionInfoPerSession {
            example_outputs : Union[Tuple[torch.Tensor, ...], torch.Tensor]
            input_devices : Tuple[ORTC.OrtDevice, ...]
            input_names : Tuple[str, ...]
            input_value_infos : Tuple[onnx.ValueInfoProto, ...]
            output_devices : Tuple[ORTC.OrtDevice, ...]
            output_names : Tuple[str, ...]
            output_value_infos : Tuple[onnx.ValueInfoProto, ...]
            session
            is_supported()
          }
          class OrtOperatorSupport {
            is_node_supported(submodules: Mapping[str, torch.nn.Module], node: torch.fx.Node) bool
          }
          class Out {
          }
          class OutDtypeHigherOrderVariable {
            call_function(tx: 'InstructionTranslator', args: 'List[VariableTracker]', kwargs: 'Dict[str, VariableTracker]') 'VariableTracker'
          }
          class OutDtypeOperator {
          }
          class OuterLoopFusedKernel {
            inner : List[LoopNest]
            decide_parallel_depth(max_parallel_depth, threads) int
          }
          class OuterLoopFusedSchedulerNode {
            outer_fused_nodes : List[Union[FusedSchedulerNode, SchedulerNode]]
            outer_loop_fusion_depth
            check_outer_fusion_loop_level_attr(cpp_kernel_proxy_list, outer_loop_fusion_depth)
            fuse(node1: BaseSchedulerNode, node2: BaseSchedulerNode, outer_loop_fusion_depth)
            get_outer_nodes()
            merge_outer_fusion_kernels(cpp_kernel_proxy_list)
          }
          class OutlierDetector {
            CHANNEL_AXIS_KEY : str
            COMP_METRIC_KEY : str
            CONSTANT_COUNTS_KEY : str
            DEFAULT_PRE_OBSERVER_NAME : str
            INPUT_ACTIVATION_PREFIX : str
            IS_SUFFICIENT_BATCHES_KEY : str
            MAX_VALS_KEY : str
            NUM_BATCHES_KEY : str
            OUTLIER_KEY : str
            RATIO_THRES_KEY : str
            REF_PERCENTILE_KEY : str
            ch_axis : int
            fraction_batches_used_threshold : float
            ratio_threshold : float
            reference_percentile : float
            determine_observer_insert_points(prepared_fx_model: GraphModule) Dict[str, Dict[str, Any]]
            generate_detector_report(model: GraphModule) Tuple[str, Dict[str, Any]]
            get_detector_name() str
            get_qconfig_info(model) Dict[str, DetectorQConfigInfo]
          }
          class OutputAdaptStep {
            apply(model_outputs: Any, model: torch.nn.Module | Callable | torch_export.ExportedProgram | None) Any
          }
          class OutputAdapter {
            append_step(step: OutputAdaptStep) None
            apply(model_outputs: Any, model: torch.nn.Module | Callable | torch_export.ExportedProgram | None) Sequence[torch.Tensor | int | float | bool | str]
          }
          class OutputAliasInfo {
          }
          class OutputAliasInfo {
            base_idx : Optional[int]
            dynamic_dims : Optional[Set[int]]
            functional_tensor : Optional[FunctionalTensorMetadataEq]
            output_type : OutputType
            raw_type : type
            requires_grad : bool
          }
          class OutputCode {
            post_compile(example_inputs: Sequence[InputType], cudagraphs: BoxedBool, constants: CompiledFxGraphConstants)* None
            set_triton_bundle(triton_bundle: Any)* None
          }
          class OutputComparisonLogger {
            comparison_fn
            comparison_fn_name : str
            comparisons : list
            forward(x, x_ref)
          }
          class OutputGraph {
            backward_state : Dict[str, VariableTracker]
            backward_state_proxy : Optional[torch.fx.Proxy]
            backward_state_var : Optional[str]
            bound_symbols
            cleanup_hooks : List[Callable[[], Any]]
            cleanups : List[CleanupHook]
            co_fields : dict
            code_options : dict
            compile_id : int
            compile_subgraph_reason : Optional[GraphCompileReason]
            compiler_fn : Optional[CompilerFn]
            compliant_custom_ops : Set[torch._ops.OpOverload]
            current_tracer
            current_tx
            dynamo_compile_id : Optional[CompileId]
            dynamo_flat_name_to_original_fqn : Dict[str, str]
            export : bool
            export_constraints
            fake_mode
            frame_state
            global_scope : Dict
            graph
            graphargs
            guard_on_key_order : Set[str]
            guards
            has_user_defined_allowed_in_graph : bool
            input_name_to_proxy
            input_source_to_sizes_strides : Dict[Source, Dict[str, Any]]
            input_source_to_var : Dict[Source, VariableTracker]
            installed_globals : Set[str]
            local_scope : Dict
            name_of_builtins_dict_key_in_fglobals : str
            nn_modules
            non_compliant_ops : Set[torch._ops.OpOverload]
            output
            output_instructions : List[Instruction]
            param_name_to_source : NoneType, Optional[Dict[str, Source]]
            partial_convert : bool
            placeholders
            pregraph_bytecode : List[Instruction]
            random_calls : List[Tuple[Callable[..., object], Tuple[object, ...], Dict[str, object]]]
            random_values_var : NoneType
            real_value_cache
            region_tracker
            register_finalizer_fns : List[Callable[[fx.GraphModule], None]]
            root_tracer
            root_tx : NoneType
            shape_env
            should_exit : bool
            side_effects
            source_to_user_stacks : Dict[Source, List[traceback.StackSummary]]
            timestamp : int
            torch_function_enabled : bool
            torch_function_mode_enabled
            torch_function_mode_stack
            tracers : list
            tracing_context
            tracked_fakes : List[TrackedFake]
            tracked_fakes_id_to_source : Dict[int, List[Source]]
            unique_var_id : count
            unspec_variable_map : Dict[str, UnspecializedPythonVariable]
            variable_tracker_cache
            add_backward_state_hook(hook: VariableTracker, prefix)
            add_cleanup_hook(fn: Callable[[], Any])
            add_graph_finalizer(register_finalizer: Callable[[fx.GraphModule], None]) None
            add_output_instructions(prefix: List[Instruction]) None
            call_cleanup_hooks()
            call_user_compiler(gm: fx.GraphModule) CompiledFn
            cleanup() None
            cleanup_graph()
            codegen_suffix(tx, stack_values, cg)
            compile_and_call_fx_graph(tx, rv, root, replaced_outputs)
            compile_subgraph(tx, partial_convert, reason: Optional[GraphCompileReason])
            count_calls()
            create_node()
            create_proxy()
            dedup_pass()
            example_inputs() List[torch.Tensor]
            example_value_from_input_node(node: torch.fx.Node)
            get_backward_state_proxy()
            get_graph_sizes(name: str)
            get_graph_sizes_structured()
            get_submodule(keys)
            handle_aliases_for_stolen_lists(tx)
            init_ambient_guards()
            install_builtins_dict_in_fglobals()
            install_global(prefix, value) str
            install_global_by_id(prefix, value) str
            install_global_unsafe(name, value) None
            install_subgraph(name, sub_gm)
            is_empty_graph()
            is_root_tracer()
            module_key_name()
            new_var(name)
            pop_tx()
            push_tx(tx)
            register_attr_or_module(target: Union[torch.nn.Module, torch.Tensor, Any])
            remove_node()
            remove_unused_graphargs() None
            restore_global_state()
            run_compiler_collective(tx)
            save_global_state(out)
            set_torch_function_state(enabled: bool) None
            subtracer(source_target, prior_tracer)
            synthetic_graph_input(fn, args)
            update_co_names(name)
          }
          class OutputKind {
            name
          }
          class OutputLogger {
            debug_handle : int
            nn_module_stack : Optional[object]
            node_name : Optional[str]
            stats : List[torch.Tensor]
            forward(x: object) object
          }
          class OutputLogger {
            enabled : bool
            fqn : Optional[str]
            index_of_arg : int
            index_within_arg : int
            model_name : str
            prev_node_name : str
            prev_node_target_type : str
            qconfig_str : Optional[str]
            ref_name : str
            ref_node_name : str
            ref_node_target_type : str
            results_type : str
            save_activations : bool
            stats : List[torch.Tensor]
            stats_rnn : List[RNNReturnType]
            forward(x)
          }
          class OutputLogger {
            forward(x)
          }
          class OutputNode {
            unmet_dependencies
            get_inputs_that_alias_output() Sequence[str]
            get_name() str
            is_reduction() bool
          }
          class OutputProp {
            graph
            mod
            modules : dict
            propagate()
          }
          class OutputSharding {
            needs_redistribute : bool
            output_spec : Optional
            redistribute_schema : Optional[OpSchema]
          }
          class OutputSpec {
            get_device()* Optional[torch.device]
            storage_size()* int
          }
          class OutputSpec {
            buffer_mutation : Annotated[BufferMutationSpec, 30]
            gradient_to_parameter : Annotated[GradientToParameterSpec, 40]
            gradient_to_user_input : Annotated[GradientToUserInputSpec, 50]
            loss_output : Annotated[LossOutputSpec, 20]
            token : Annotated[OutputTokenSpec, 70]
            user_input_mutation : Annotated[UserInputMutationSpec, 60]
            user_output : Annotated[UserOutputSpec, 10]
          }
          class OutputSpec {
            arg : Union
            kind
            target : Optional[str]
          }
          class OutputTokenSpec {
            arg : Annotated[TokenArgument, 10]
          }
          class OverlappedOptimizer {
            optim_cls : Type
            register_ddp(ddp: DistributedDataParallel)* None
            register_fsdp(fsdp: FullyShardedDataParallel)* None
          }
          class OverrideDict {
            get(key: _K, default: Optional[_V])
            in_base(key: _K) bool
            overridden(key: _K) bool
            override(key: _K, value: _V) None
            remove_override(key: _K) None
            set_base(key: _K, value: _V) None
          }
          class OverridesData {
            cpp : Callable[..., str]
            cppvec : Optional[Callable[..., str]]
            name : str
            triton : Optional[Callable[..., str]]
            type_promotion_kind : ELEMENTWISE_TYPE_PROMOTION_KIND
          }
          class P2POp {
            group : NoneType
            group_peer : NoneType
            op : Callable
            peer : NoneType
            tag : int
            tensor
          }
          class PContext {
            args : Dict[int, Tuple]
            entrypoint : Union[Callable, str]
            envs : Dict[int, Dict[str, str]]
            error_files
            name : str
            nprocs
            stderrs
            stdouts
            close(death_sig: Optional[signal.Signals], timeout: int) None
            pids()* Dict[int, int]
            start() None
            wait(timeout: float, period: float) Optional[RunProcsResult]
          }
          class PHBase {
          }
          class PHWithMeta {
            ph_key : Optional[str]
          }
          class PReLU {
            num_parameters : int
            scale : float
            weight
            zero_point : int
            forward(input: torch.Tensor) torch.Tensor
            from_float(mod, use_precomputed_fake_quant)
            from_reference(mod, scale, zero_point)
            set_weight(w: torch.Tensor) None
          }
          class PReLU {
            init : float
            num_parameters : int
            weight
            extra_repr() str
            forward(input: Tensor) Tensor
            reset_parameters()
          }
          class PT2ArchiveReader {
            archive_file : Optional[zipfile.ZipFile]
            archive_path : str
            extract_to_path(member: str, path: str) str
            extractall(path: str) None
            get_file_names() List[str]
            read(name: str) bytes
          }
          class PT2ArchiveWriter {
            archive_file : NoneType, Optional[zipfile.ZipFile]
            archive_path : Union[str, io.BytesIO]
            write_file(name: str, file_path: str) None
            writestr(name: str, data: Union[bytes, str]) None
          }
          class PT2EQuantizationTestCase {
          }
          class PackageExporter {
            buffer : Optional[BinaryIO]
            debug : bool
            dependency_graph
            importer
            patterns : Dict[GlobGroup, _PatternInfo]
            script_module_serializer
            serialized_reduces : Dict[int, Any]
            storage_context
            zip_file
            add_dependency(module_name: str, dependencies)
            all_paths(src: str, dst: str) str
            close()
            denied_modules() List[str]
            deny(include: 'GlobPattern')
            dependency_graph_string() str
            extern(include: 'GlobPattern')
            externed_modules() List[str]
            get_rdeps(module_name: str) List[str]
            get_unique_id() str
            intern(include: 'GlobPattern')
            interned_modules() List[str]
            mock(include: 'GlobPattern')
            mocked_modules() List[str]
            register_extern_hook(hook: ActionHook) RemovableHandle
            register_intern_hook(hook: ActionHook) RemovableHandle
            register_mock_hook(hook: ActionHook) RemovableHandle
            save_binary(package, resource, binary: bytes)
            save_module(module_name: str, dependencies)
            save_pickle(package: str, resource: str, obj: Any, dependencies: bool, pickle_protocol: int)
            save_source_file(module_name: str, file_or_directory: str, dependencies)
            save_source_string(module_name: str, src: str, is_package: bool, dependencies: bool)
            save_text(package: str, resource: str, text: str)
          }
          class PackageImporter {
            Unpickler
            extern_modules
            filename : str
            last_map_location : NoneType
            modules : Dict[str, types.ModuleType]
            patched_builtins : dict
            root
            storage_context : NoneType, Optional[Any]
            zip_reader : Any
            file_structure() Directory
            get_resource_reader(fullname)
            get_source(module_name) str
            id()
            import_module(name: str, package)
            load_binary(package: str, resource: str) bytes
            load_pickle(package: str, resource: str, map_location) Any
            load_text(package: str, resource: str, encoding: str, errors: str) str
            python_version()
          }
          class PackageInfo {
            commit_hash : str | None
            package_name : str
            version : str | None
            from_python_class(python_class_name: type | str) PackageInfo
            to_onnx_domain_string() str
          }
          class PackageMangler {
            demangle(mangled: str) str
            mangle(name) str
            parent_name()
          }
          class PackagePickler {
            dispatch : dict
            importer
            persistent_id
            save_global(obj, name)
          }
          class PackageUnpickler {
            persistent_load
            find_class(module, name)
          }
          class PackagingError {
            dependency_graph
          }
          class PackagingErrorReason {
            name
          }
          class PackedParameter {
            param
          }
          class PackedSequence {
            is_cuda
            byte() Self
            char() Self
            cpu() Self
            cuda() Self
            double() Self
            float() Self
            half() Self
            int() Self
            is_pinned() bool
            long() Self
            pin_memory() Self
            short() Self
            to(dtype: torch.dtype, non_blocking: bool, copy: bool) Self
          }
          class PackedSequence_ {
            batch_sizes
            data
            sorted_indices : Optional[torch.Tensor]
            unsorted_indices : Optional[torch.Tensor]
          }
          class PadMMA100 {
            check_precondition(metadata: AHMetadata, context: AHContext) bool
            get_confidence_threshold() float
            get_feedback(context: AHContext, choice: Choice) float
            get_name() str
            predict(context: AHContext) float
          }
          class PagedAttention {
            capacity
            empty_pages : list
            n_pages : int
            page_size : int
            page_table
            physical_to_logical
            assign(batch_idx: torch.Tensor, input_pos: torch.Tensor, k_val: torch.Tensor, v_val: torch.Tensor, k_cache: torch.Tensor, v_cache: torch.Tensor) None
            convert_logical_block_mask(block_mask: BlockMask, batch_idx: Optional[torch.Tensor]) BlockMask
            erase(batch_idx: torch.Tensor) None
            get_mask_mod(mask_mod: Optional[_mask_mod_signature]) _mask_mod_signature
            get_score_mod(score_mod: Optional[_score_mod_signature]) _score_mod_signature
            reserve(batch_idx: torch.Tensor, seq_len: torch.Tensor) None
          }
          class Pair {
            actual : Any
            expected : Any
            id : tuple
            compare()* None
            extra_repr() Sequence[Union[str, Tuple[str, Any]]]
          }
          class PairwiseDistance {
            eps : float
            keepdim : bool
            norm : float
            forward(x1: Tensor, x2: Tensor) Tensor
          }
          class PandasWrapper {
            concat(buffer)
            create_dataframe(data, columns)
            get_columns(df)
            get_item(data, idx)
            get_len(df)
            is_column(data)
            is_dataframe(data)
            iterate(data)
          }
          class ParallelStyle {
          }
          class Param {
            idx : int
          }
          class ParamBufferSource {
            guard_source()
          }
          class ParamInfo {
            module
            module_name : str
            param_name : str
          }
          class ParamModuleInfo {
            module
            param_name : str
            shared_modules : List[nn.Module]
            shared_param_names : List[str]
          }
          class Parameter {
            default : Any
            name : str
            required : bool
            type_constraint
            variadic : bool
            has_default() bool
          }
          class Parameter {
            data
            grad : NoneType
            requires_grad : bool
          }
          class ParameterAlias {
            alias_to
          }
          class ParameterDict {
            clear() None
            copy() 'ParameterDict'
            extra_repr() str
            fromkeys(keys: Iterable[str], default: Optional[Any]) 'ParameterDict'
            get(key: str, default: Optional[Any]) Any
            items() Iterable[Tuple[str, Any]]
            keys() Iterable[str]
            pop(key: str) Any
            popitem() Tuple[str, Any]
            setdefault(key: str, default: Optional[Any]) Any
            update(parameters: Union[Mapping[str, Any], 'ParameterDict']) None
            values() Iterable[Any]
          }
          class ParameterList {
            append(value: Any) 'ParameterList'
            extend(values: Iterable[Any]) Self
            extra_repr() str
          }
          class ParameterProxy {
            name
            ndim
            param
            shape
            dim()
            nelement()
            numel()
            size()
          }
          class ParameterServerTest {
            test_batch_updating_parameter_server()
          }
          class ParametrizationList {
            is_tensor
            ntensors : int
            original
            unsafe : bool
            forward() Tensor
            right_inverse(value: Tensor) None
          }
          class Pareto {
            alpha
            arg_constraints : dict
            mean
            mode
            scale
            variance
            entropy()
            expand(batch_shape, _instance)
            support()
          }
          class ParsedDef {
            ast
            ctx
            file_lineno : int
            filename : Optional[str]
            source : str
          }
          class Partial {
            reduce_op : str
          }
          class PartialRender {
            code
            replacement_hooks
            finalize_all() str
            finalize_hook(hook_key: str, strict) None
          }
          class Partition {
            dependencies : Dict[str, None]
            dependents : Dict[str, None]
            environment : Dict[Node, Node]
            graph
            inputs : Dict[str, None]
            name : str
            node_names : List[str]
            outputs : Dict[str, None]
            submod_name : str
            targets : Dict[str, Any]
          }
          class Partition {
            id : Optional[int]
            nodes : dict
            add_node(node: Node)
            remove_node(node: Node)
            size()
          }
          class Partition {
            bfs_level : int
            children : Set[Partition]
            left_mem_bytes
            logical_device_ids : List[int], list
            nodes : Set[Node]
            parents : Set[Partition]
            partition_id : int
            used_mem_bytes : int
            add_node(node)
            recalculate_mem_size()
            remove_node(node)
          }
          class PartitionLatency {
            computer_latency_sec : float
            mem_latency_sec : float
            overall_latency_sec : float
          }
          class PartitionMode {
            name
          }
          class PartitionResult {
            dag
            module_with_submodules
          }
          class PartitionState {
            cur_count : int
            cur_partition : List[BaseSchedulerNode]
            partitions : List[List[BaseSchedulerNode]]
            finalize() None
          }
          class Partitioner {
            devices : List[Device]
            graph_module
            node_to_partition : Dict[Node, int], dict
            partitions : List[Partition]
            torch_module
            aot_based_partition(node_to_partition_mapping, partition_to_logical_device_mapping)
            cost_aware_partition(transfer_rate_bytes_per_sec: float, node_to_latency_mapping: Dict[Node, NodeLatency]) None
            create_partition() Partition
            create_single_node_partition(node)
            do_partition() GraphModule
            dump_dag(module_with_submodules: GraphModule) DAG
            find_single_partition(total_size_of_graph, logical_device_id: int) None
            kl_based_partition(transfer_rate_bytes_per_sec: float, node_to_latency_mapping: Dict[Node, NodeLatency]) None
            partition_graph(fx_module: GraphModule, torch_module: torch.nn.Module, partitioner_config: PartitionerConfig) PartitionResult
            saturate_host() None
            size_based_partition() None
            sparse_nn_partition(available_mem_bytes: int) None
          }
          class PartitionerConfig {
            devices : List[Device]
            mode
            node_to_latency_mapping : Dict[Node, NodeLatency]
            node_to_partition_mapping : Dict[Node, int]
            partition_to_logical_device_mapping : Dict[int, List[int]]
            saturate_host : bool
            transfer_rate_bytes_per_sec : float
          }
          class PassBase {
            call(graph_module: GraphModule)* Optional[PassResult]
            ensures(graph_module: GraphModule)* None
            requires(graph_module: GraphModule)* None
          }
          class PassManager {
            constraints : List[Callable]
            passes : List[Callable]
            add_constraint(constraint)
            add_pass(_pass: Callable)
            build_from_passlist(passes)
            remove_pass(_passes: List[str])
            replace_pass(_target, _replacement)
            validate()
          }
          class PassManager {
            constraints : List[Callable[[Callable, Callable], bool]]
            passes : List[Callable[[nn.Module], PassResult]]
            run_checks_after_each_pass : bool
            steps : int
            suppress_check_failures : bool
            add_checks(check: Callable) None
            add_constraint(constraint: Callable)
            add_pass(_pass: Callable)
            check(module: nn.Module)* None
            solve_constraints()
            validate_constraints()
          }
          class PassResult {
          }
          class PatchCaches {
            setUp()
            tearDown()
          }
          class PatchedPropertyBag {
          }
          class Path {
          }
          class Pattern {
            description : str
            event_tree
            name : str
            prof
            should_benchmark : bool
            skip
            tid_root : Dict[int, List[_ProfilerEvent]]
            url : str
            benchmark_summary(events: List[_ProfilerEvent])
            eventTreeTraversal()
            go_up_until(event: _ProfilerEvent, predicate)
            match(event: _ProfilerEvent)*
            matched_events()
            next_of(event: _ProfilerEvent)
            prev_of(event: _ProfilerEvent)
            report(event: _ProfilerEvent)
            root_of(event: _ProfilerEvent)
            siblings_of(event: _ProfilerEvent)
            summary(events: List[_ProfilerEvent])
          }
          class PatternEntry {
            extra_check : Callable[[Match], bool]
            pattern
            apply(match: Match, graph: torch.fx.Graph, node: torch.fx.Node)* None
            register(pass_dicts: Union[_PassDictsType, Sequence[_PassDictsType]], target: Union[torch.fx.node.Target, None], prepend: bool) None
          }
          class PatternExpr {
            find_anchor_nodes(ctx: MatchContext, searched: OrderedSet[torch.fx.Node]) Generator[Optional[torch.fx.Node], None, None]
            has_multiple_users() bool
            match(node: torch.fx.Node) MatchResult
            pattern_eq(other: Any) bool
          }
          class PatternMatcherPass {
            pass_name : Optional[str]
            patterns : DefaultDict[Tuple[str, torch.fx.node.Target], List[PatternEntry]]
            apply(gm: Union[torch.fx.GraphModule, torch.fx.Graph]) int
            clear() None
          }
          class PatternPrettyPrinter {
            memoized_objs_names : Dict[PatternExpr, str]
            memoized_objs_pp : Dict[PatternExpr, str]
            namespace
            memoize(obj: _TargetArgsExpr) str
            pretty_print(obj: Any) str
            run(obj: PatternExpr, output_name: str) str
          }
          class PeakMemoryResult {
            method : str
            order : List[BaseSchedulerNode]
            peak_memory : int
          }
          class PendingUnbackedSymbolNotFound {
          }
          class PerChannelDetector {
            BACKEND_KEY : str
            DEFAULT_BACKEND_PER_CHANNEL_SUPPORTED_MODULES : Dict[str, Set[Any]]
            PER_CHAN_SUPPORTED_KEY : str
            PER_CHAN_USED_KEY : str
            backend_chosen : str
            supported_modules : set
            determine_observer_insert_points(model: nn.Module) Dict
            generate_detector_report(model: nn.Module) Tuple[str, Dict[str, Any]]
            get_detector_name() str
            get_qconfig_info(model) Dict[str, DetectorQConfigInfo]
          }
          class PerChannelMinMaxObserver {
            ch_axis : int
            max_val
            min_val
            calculate_qparams()
            extra_repr()
            forward(x_orig)
            reset_min_max_vals()
          }
          class PerRowDataFramesPipe {
            source_datapipe
          }
          class PeriodicModelAverager {
            period
            step
            warmup_steps : int
            average_parameters(params: Union[Iterable[torch.nn.Parameter], Iterable[Dict[str, torch.nn.Parameter]]])
          }
          class PermuteView {
            dims : List[Expr]
            create(x, dims)
            get_size() Sequence[Expr]
            make_reindexer()
          }
          class PersistentCache {
            get_global_cache() Dict[str, Any]
            lookup(choices: List[ChoiceCaller], op: str, inputs: str, benchmark: Optional[Callable[[Any], Dict[ChoiceCaller, float]]]) Dict[ChoiceCaller, float]
          }
          class PhiloxState {
            base_offset
            offset_advanced_alteast_once : bool
            relative_offset : int
            seed
            advance_offset(consumed_offset)
            get_state_as_tensor()
            get_state_as_tuple()
            reset()
            set_state(seed, base_offset, relative_offset)
            set_state_from_tensor(state)
            validate_state()
          }
          class PhiloxStateTracker {
            bwd_state
            fwd_state
            running_state
            advance_offset(consumed_offset)
            get_current_relative_offset()
            get_state_as_tensor()
            get_state_as_tuple()
            get_updated_bwd_offset()
            get_updated_fwd_offset()
            mark_beginning_of_backward()
            mark_beginning_of_forward()
            multiple_of_4(offset)
            record_state(seed, offset, mode)
            reset()
            set_state_from_tensor(x)
          }
          class PhysicalLocation {
            address : Optional[_address.Address]
            artifact_location : Optional[_artifact_location.ArtifactLocation]
            context_region : Optional[_region.Region]
            properties : Optional[_property_bag.PropertyBag]
            region : Optional[_region.Region]
          }
          class Ping {
          }
          class Pipe {
            executor
            has_loss_and_backward : bool
            loss_spec
            num_stages : int
            replicated_params : List[Dict[str, str]]
            split_gm
            build_stage(stage_index: int, device: torch.device, group: Optional[ProcessGroup]) _PipelineStage
            forward()
            from_tracing(mod: torch.nn.Module, example_args: Tuple[Any, ...], example_kwargs: Optional[Dict[str, Any]], split_policy: Optional[Callable[[fx.GraphModule], fx.GraphModule]])
            get_stage_module(stage_idx: int) torch.nn.Module
            info() PipeInfo
            print_readable()
          }
          class PipeInfo {
            graph
            has_loss_and_backward : bool
            num_stages : int
          }
          class PipeSequential {
            forward(input)
            from_sequential(sequential_instance: torch.nn.Sequential)
          }
          class PipeSplitWrapper {
            SplitPoint
          }
          class PipelineScheduleMulti {
            pipeline_order : Dict[int, List[Optional[_Action]]]
            pp_group_size
            rank
            stage_index_to_group_rank
            step()
          }
          class PipelineScheduleSingle {
            step()
          }
          class PipelineStage {
            act_send_info : Dict[int, List]
            inputs : Optional[List[torch.Tensor]]
            inputs_meta : Optional[Tuple[torch.Tensor, ...]], Tuple[Any, ...], tuple
            next_rank
            outputs_grad : List[torch.Tensor]
            prev_rank
          }
          class PipeliningShapeError {
          }
          class PixelShuffle {
            upscale_factor : int
            extra_repr() str
            forward(input: Tensor) Tensor
          }
          class PixelUnshuffle {
            downscale_factor : int
            extra_repr() str
            forward(input: Tensor) Tensor
          }
          class Placeholder {
            name
          }
          class PlaceholderInfo {
            mutating_use_stack_trace : Optional[str]
            name : str
            stack_trace : Optional[str]
            users : List[PlaceholderInfo]
          }
          class PlaceholderObserver {
            custom_op : str
            dtype
            eps : NoneType
            qscheme : NoneType
            quant_max : NoneType
            quant_min : NoneType
            calculate_qparams()
            extra_repr()
            forward(x)
          }
          class Placement {
            is_partial() bool
            is_replicate() bool
            is_shard(dim: Optional[int]) bool
          }
          class PlacementClassVariable {
            as_python_constant()
            call_function(tx: 'InstructionTranslator', args: 'List[VariableTracker]', kwargs: 'Dict[str, VariableTracker]') 'VariableTracker'
            is_placement_type(value)
          }
          class PlacementSpec {
          }
          class PlacementStrategy {
            input_specs : Optional[Sequence[DTensorSpec]]
            output_spec
            output_specs : Union[DTensorSpec, Tuple[Optional[DTensorSpec], ...]]
            redistribute_cost : Optional[List[List[float]]]
            input_spec(index: int) DTensorSpec
          }
          class PlacementVariable {
            as_python_constant()
            call_method(tx, name, args: 'List[VariableTracker]', kwargs: 'Dict[str, VariableTracker]') 'VariableTracker'
            is_placement(value)
            var_getattr(tx: 'InstructionTranslator', name: str) VariableTracker
          }
          class PlainTensorMeta {
            memory_format : Optional[torch.memory_format]
            unwrapped_idx : int
          }
          class Pointwise {
            constant_to_device(device: torch.device) IRNode
            get_reduction_size() Sequence[sympy.Expr]
            get_reduction_type() Optional[str]
            make_loader() Callable[[Sequence[Expr]], OpsValue]
            store_output(output_name: Optional[str], indexer: Callable[[Sequence[Expr]], Never], vars: Sequence[Expr]) OpsValue
          }
          class PointwiseSubgraphLowering {
            additional_lowerings : Optional[LoweringDict]
            allowed_mutations : Optional[OrderedSet[OpOverload]]
            buffers : List[ir.Buffer]
            graph_outputs : Optional[List[ir.IRNode]]
            mutated_buffers : OrderedSet[str]
            root_graph
            call_function(target: TargetType, args: Any, kwargs: Dict[str, Any]) Any
            mark_buffer_mutated(name: str) None
            output(target: str, args: Tuple[Any], kwargs: Dict[str, Any]) None
            register_buffer(buffer: ir.Buffer) str
          }
          class Poisson {
            arg_constraints : dict
            mean
            mode
            rate
            support
            variance
            expand(batch_shape, _instance)
            log_prob(value)
            sample(sample_shape)
          }
          class PoissonNLLLoss {
            eps : float
            full : bool
            log_input : bool
            forward(log_input: Tensor, target: Tensor) Tensor
          }
          class Policy {
            affine1
            affine2
            dropout
            rewards : list
            saved_log_probs : list
            forward(x)
          }
          class PolyfilledFunctionVariable {
            fn : _F
            polyfill_fn
            traceable_fn : _F
            wrapped_fn : _F
            as_python_constant()
            call_function(tx: 'InstructionTranslator', args: 'List[VariableTracker]', kwargs: 'Dict[str, VariableTracker]') 'VariableTracker'
            call_method(tx, name, args: 'List[VariableTracker]', kwargs: 'Dict[str, VariableTracker]') 'VariableTracker'
            can_constant_fold_through()
            create_with_source(value, source)
            get_function()
          }
          class PolynomialLR {
            power : float
            total_iters : int
            get_lr()
          }
          class Pong {
          }
          class Pool {
          }
          class PoolMemoryPlanningLine {
            group
            node
            timestep : Optional[int]
          }
          class PopulateValidator {
            validator : str
            call_function(target: Target, args: Tuple[Argument, ...], kwargs: Dict[str, Any]) Any
            placeholder(target: Target, args: Tuple[Argument, ...], kwargs: Dict[str, Any]) Any
          }
          class PortNodeMetaForQDQ {
            call(graph_module: torch.fx.GraphModule) PassResult
          }
          class PositiveDefiniteTransform {
            codomain
            domain
          }
          class PostGradBatchLinearFusion {
            fuse(graph: torch.fx.GraphModule, subset: List[torch.fx.Node])
            match(node: torch.fx.Node) Optional[Tuple[str, int, int, int, bool, str]]
          }
          class PostLocalSGDOptimizer {
            averager
            optim
            param_groups
            state
            add_param_group(param_group)
            load_state_dict(state_dict)
            state_dict()
            step()
            zero_grad(set_to_none: bool)
          }
          class PostLocalSGDState {
            iter : int
            post_local_gradient_allreduce : bool
            process_group
            start_localSGD_iter
            subgroup
            maybe_increase_iter(bucket)
          }
          class PostTrainingDataSparsity {
            data_sparsifier : Optional[Any]
            data_sparsifier_args
            data_sparsifier_class
            sparsified : Optional[torch.nn.Module]
            on_fit_end(trainer, pl_module) None
          }
          class PowByNatural {
            is_integer : bool
            precedence : int
            eval(base, exp)
          }
          class PowerSGDState {
            batch_tensors_with_same_shape : bool
            compression_stats_logging_frequency
            error_dict : Dict[int, torch.Tensor]
            iter : int
            matrix_approximation_rank : int
            min_compression_rate : int
            next_stats_report : int
            orthogonalization_epsilon : int
            p_memory_dict : Dict[int, torch.Tensor]
            process_group
            q_memory_dict : Dict[int, torch.Tensor]
            rng
            start_powerSGD_iter : int
            total_numel_after_compression : int
            total_numel_before_compression : int
            use_error_feedback : bool
            warm_start : bool
            compression_stats()
            maybe_increase_iter(bucket)
          }
          class PowerTransform {
            bijective : bool
            codomain
            domain
            exponent
            forward_shape(shape)
            inverse_shape(shape)
            log_abs_det_jacobian(x, y)
            sign()
            with_cache(cache_size)
          }
          class PreDispatchTorchFunctionMode {
            enter_autocast_nodes : List[torch.fx.Node]
            tracer : Union
          }
          class PreGradBatchLinearFusion {
            fuse(graph: torch.fx.GraphModule, subset: List[torch.fx.Node])
            match(node: torch.fx.Node)
          }
          class PrepareCustomConfig {
            float_to_observed_mapping : Dict[QuantType, Dict[Type, Type]]
            input_quantized_indexes : List[int]
            non_traceable_module_classes : List[Type]
            non_traceable_module_names : List[str]
            output_quantized_indexes : List[int]
            preserved_attributes : List[str]
            standalone_module_classes : Dict[Type, StandaloneModuleConfigEntry]
            standalone_module_names : Dict[str, StandaloneModuleConfigEntry]
            from_dict(prepare_custom_config_dict: Dict[str, Any]) PrepareCustomConfig
            set_float_to_observed_mapping(float_class: Type, observed_class: Type, quant_type: QuantType) PrepareCustomConfig
            set_input_quantized_indexes(indexes: List[int]) PrepareCustomConfig
            set_non_traceable_module_classes(module_classes: List[Type]) PrepareCustomConfig
            set_non_traceable_module_names(module_names: List[str]) PrepareCustomConfig
            set_output_quantized_indexes(indexes: List[int]) PrepareCustomConfig
            set_preserved_attributes(attributes: List[str]) PrepareCustomConfig
            set_standalone_module_class(module_class: Type, qconfig_mapping: Optional[QConfigMapping], example_inputs: Tuple[Any, ...], prepare_custom_config: Optional[PrepareCustomConfig], backend_config: Optional[BackendConfig]) PrepareCustomConfig
            set_standalone_module_name(module_name: str, qconfig_mapping: Optional[QConfigMapping], example_inputs: Tuple[Any, ...], prepare_custom_config: Optional[PrepareCustomConfig], backend_config: Optional[BackendConfig]) PrepareCustomConfig
            to_dict() Dict[str, Any]
          }
          class PrepareModuleInput {
            desired_input_kwarg_layouts : dict
            desired_input_layouts : NoneType, tuple
            input_kwarg_layouts : dict
            input_layouts : NoneType, tuple
            use_local_output : bool
            with_kwargs
          }
          class PrepareModuleOutput {
            desired_output_layouts : tuple
            output_layouts : tuple
            use_local_output : bool
          }
          class PrependParamsAndBuffersAotAutogradOutputStep {
            apply(model_outputs: Any, model: torch.nn.Module | Callable | torch_export.ExportedProgram | None) Sequence[Any]
          }
          class PrependParamsBuffersConstantAotAutogradInputStep {
            apply(model_args: Sequence[Any], model_kwargs: Mapping[str, Any], model: torch.nn.Module | Callable | torch_export.ExportedProgram | None) tuple[Sequence[Any], Mapping[str, Any]]
          }
          class PreserveVersionContextVariable {
            prev_version
            tensor
            constructor(tx)
            enter(tx)*
            exit(tx: 'InstructionTranslator')
            reconstruct(codegen)
          }
          class PrimHOPBase {
          }
          class PrimHOPBaseFunction {
            backward(ctx)
            forward(ctx, hop, subgraph, kwargs)
          }
          class PrimHOPBaseVariable {
            call_function(tx: 'InstructionTranslator', args: 'List[VariableTracker]', kwargs: 'Dict[str, VariableTracker]') 'VariableTracker'
          }
          class PrivateUse1TestBase {
            device_mod : NoneType
            device_type : str
            primary_device : ClassVar[str]
            get_all_devices()
            get_primary_device()
            setUpClass()
          }
          class ProcessContext {
            error_files
            processes
            sentinels
            join(timeout: Optional[float], grace_period: Optional[float])
            pids()
          }
          class ProcessException {
            error_index : int
            msg : str
            pid : int
          }
          class ProcessExitedException {
            exit_code : int
            signal_name : Optional[str]
          }
          class ProcessFailure {
            error_file : str
            error_file_data : Dict
            exitcode : int
            local_rank : int
            message : str
            pid : int
            timestamp : int
            signal_name() str
            timestamp_isoformat()
          }
          class ProcessGroupState {
            global_rank : int
            global_world_size : int
            local_rank : int
            local_world_size : int
          }
          class ProcessGroupVariable {
            as_python_constant()
            call_method(tx, name, args: 'List[VariableTracker]', kwargs: 'Dict[str, VariableTracker]') 'VariableTracker'
            is_process_group(value)
            var_getattr(tx: 'InstructionTranslator', name)
          }
          class ProcessLocalGroup {
            group_name
            pg_name
            allgather(output_tensors, input_tensor, opts)
            allgather_into_tensor_coalesced(output_tensor_list, input_tensor_list, opts)
            allreduce(tensor_list, opts)
            allreduce_coalesced(tensor_list, opts)
            alltoall(output_tensor_list, input_tensor_list, opts)
            alltoall_base(output_buffer: torch.Tensor, input_buffer: torch.Tensor, output_split_sizes: Optional[List[int]], input_split_sizes: Optional[List[int]], opts) torch.Tensor
            barrier(opts)
            broadcast(tensor_list, opts)
            exception_handle(exc)
            gather(output_tensors, input_tensors, opts)
            getBackendName()
            reduce_scatter(output_tensor, scatter_list, opts)
            reduce_scatter_tensor_coalesced(output_tensors, input_tensors, opts)
            reset()
            scatter(output_tensors, input_tensors, opts)
            size()
          }
          class ProcessRaisedException {
          }
          class Prod {
            products
          }
          class ProfileEvent {
            category : str
            count : float
            key : str
            self_device_time_ms : float
          }
          class ProfileMetrics {
            fusions : int
            graphs : int
            microseconds : float
            operators : int
            tocsv() List[float]
          }
          class ProfileResult {
            captured
            total
            unique_graphs : int
            percent() ProfileMetrics
            tocsv() List[Any]
          }
          class Profiler {
            prof
            unique_graphs : int
            results() ProfileResult
          }
          class ProfilerAction {
            name
          }
          class ProfilerEndHook {
          }
          class ProfilerStartHook {
          }
          class ProfilingMode {
            name
          }
          class PropModule {
            m
          }
          class PropagateUnbackedSymInts {
            run_node(n: torch.fx.Node) Result
          }
          class PropertyBag {
            tags : Optional[List[str]]
          }
          class Proxy {
            node
            tracer : str
            keys()
          }
          class ProxyTorchDispatchMode {
            decomp_layers : int
            emulate_precision_casts : bool
            enable_tracing
            enter_stack : List[Optional[ProxyTorchDispatchMode]]
            pre_dispatch : bool
            tracer : Union
            tracing_mode : str
            is_infra_mode() bool
          }
          class ProxyValue {
            data
            node
            proxy
            proxy_or_node : Union[torch.fx.Proxy, torch.fx.Node]
            is_tensor() bool
            to_tensor() torch.Tensor
          }
          class ProxyableClassMeta {
          }
          class PruningContainer {
            add_pruning_method(method)
            compute_mask(t, default_mask)
          }
          class PyCodeCache {
            linemaps : Dict[str, List[Tuple[Any, ...]]]
            modules : List[ModuleType]
            cache_clear(purge: bool) None
            load(source_code: str, extra: str, linemap: Optional[List[Tuple[int, str]]], attrs: Optional[Dict[str, Any]]) ModuleType
            load_by_key_path(key: str, path: str, linemap: Optional[List[Tuple[int, str]]], attrs: Optional[Dict[str, Any]]) ModuleType
            stack_frames_for_code(path: str, lineno: int) Optional[List[Dict[str, Any]]]
            write(source_code: str, extra: str) Tuple[str, str]
          }
          class PyCodegen {
            cell_and_freevars
            code_options
            graph_output_var : Optional[str]
            graph_outputs : Dict[int, GraphOutputEntry]
            new_var
            overridden_sources : Dict[Source, Source]
            root : Optional[torch.nn.Module]
            tempvars : dict
            top_of_stack : NoneType, Optional[VariableTracker]
            tx : NoneType
            uses : Counter[VariableTracker]
            value_from_source : bool
            add_cache(value)
            add_graph_output(value)
            add_push_null(gen_fn, call_function_ex)
            append_output(inst)
            call_function(nargs: int, push_null: bool)
            call_method(nargs)
            call_reconstruct(value)
            clear_tos()
            create_call_function_kw(nargs, kw_names, push_null) List[Instruction]
            create_delete(value) Instruction
            create_load(name) Instruction
            create_load_attr(name) Instruction
            create_load_attrs(names)
            create_load_closure(name) Instruction
            create_load_const(value) Instruction
            create_load_const_unchecked(value) Instruction
            create_load_deref(name) Instruction
            create_load_global(name, add) Instruction
            create_load_python_module(mod) Instruction
            create_store(name) Instruction
            create_store_attr(name) Instruction
            create_store_deref(name) Instruction
            dup_top()
            extend_output(insts)
            foreach(items)
            get_instructions() List[Instruction]
            graph_output_vars()
            load_attr(name)
            load_deref(varname)
            load_function_name(fn_name, push_null, num_on_stack)
            load_graph_output(index)
            load_import_from(module_name, object_name) None
            load_method(name)
            make_call_generated_code(fn_name: str) None
            make_function_with_closure(fn_name: str, code: types.CodeType, push_null: bool, num_on_stack)
            pop_null()
            pop_top()
            restore_stack(stack_values)
            rot_n(n)
            setup_globally_cached(name, value)
            store(varname)
            store_attr(name)
          }
          class PyExprCSEPass {
            ALLOWED_NODE_TYPES : tuple
            USE_THRESHOLD : int
            count(exprs: List[str]) None
            replace(expr: str) Tuple[List[str], str]
          }
          class PyTorchLegacyPickler {
            persistent_id(obj)
          }
          class PyTorchPickler {
            persistent_id(obj)
          }
          class PyTreeSpec {
            namespace : Literal['torch']
            none_is_leaf : Literal[True]
            num_children : int
            num_leaves : int
            num_nodes : int
            type
            child(index: int) PyTreeSpec
            children() list[PyTreeSpec]
            entries() list[Any]
            entry(index: int) Any
            flatten_up_to(tree: PyTree) list[PyTree]
            is_leaf() bool
            unflatten(leaves: Iterable[Any]) PyTree
          }
          class PythonCode {
            globals : Dict[str, Any]
            src : str
          }
          class PythonDispatcher {
            alias_keys : list
            name : str
            namespace : str
            ref
            runtime_keys : list
            supported_keys : list
            dispatchTable()
            keys()
            rawDispatchTable()
            rawRegistrations()
            register(dispatchKeys)
            registrations()
          }
          class PythonFunctionalizeAPI {
            mode
            pre_dispatch : bool
            commit_update(tensor) None
            functionalize(inner_f: Callable) Callable
            mark_mutation_hidden_from_autograd(tensor) None
            redispatch_to_next() ContextManager
            replace(input_tensor, output_tensor) None
            sync(tensor) None
            unwrap_tensors(args: Union[torch.Tensor, Tuple[torch.Tensor, ...], List[torch.Tensor]]) Any
            wrap_tensors(args: Tuple[Any]) Tuple[Any]
          }
          class PythonKeyTracer {
            enable_thunkify : bool
            graph
            root
            script_object_tracker : MutableMapping[_AnyScriptObjectType, Proxy]
            symnode_tracker
            sympy_expr_tracker : Dict[sympy.Symbol, object]
            tensor_attrs : dict
            tensor_tracker : MutableMapping[Tensor, _ProxyTensor]
            torch_fn_counts : Dict[OpOverload, int]
            torch_fn_metadata : NoneType
            call_module(m: Module, forward: Callable[..., Any], args: Tuple[Any, ...], kwargs: Dict[str, Any]) Any
            create_arg(a: object) fx.node.Node
            getattr(attr: str, attr_val: object, parameter_proxy_cache: Dict[str, Proxy]) object
            unwrap_proxy(e: Tensor) Union[Proxy, Tensor]
          }
          class PythonMod {
            is_integer : bool
            nargs : Tuple[int, ...]
            precedence : int
            eval(p: sympy.Expr, q: sympy.Expr) Optional[sympy.Expr]
          }
          class PythonModuleVariable {
            is_torch
            value : module
            as_python_constant()
            call_hasattr(tx: 'InstructionTranslator', name)
            python_type()
            var_getattr(tx: 'InstructionTranslator', name)
          }
          class PythonPrinter {
          }
          class PythonPrinter {
            doprint(expr)
          }
          class PythonRefInfo {
            torch_opinfo
            torch_opinfo_name
            torch_opinfo_variant_name : str
            validate_view_consistency : bool
          }
          class PythonReferenceAnalysis {
            bitwise_and(a, b)
            bitwise_or(a, b)
            ceil(x)
            ceil_to_int(x, dtype)
            constant(c, dtype)
            exp(x)
            floor(x)
            floor_to_int(x, dtype)
            floordiv(a, b)
            log(x)
            log2(x)
            maximum(a, b)
            minimum(a, b)
            mod(x, y)
            not_(a)
            pow(a, b)
            pow_by_natural(a, b)
            round_decimal(a, b)
            round_to_int(a, dtype)
            sqrt(x)
            sym_sum(args)
            to_dtype(x, dtype)
            truediv(a, b)
            truncdiv(a, b)
          }
          class PythonSysModulesVariable {
            call_contains(tx: 'InstructionTranslator', key: VariableTracker)
            call_get(tx: 'InstructionTranslator', key: VariableTracker, default: Optional[VariableTracker])
            call_getitem(tx: 'InstructionTranslator', key: VariableTracker)
            call_method(tx: 'InstructionTranslator', name, args: List[VariableTracker], kwargs: Dict[str, VariableTracker])
            python_type()
            reconstruct(codegen)
          }
          class PythonWrapperCodegen {
            add_import_once
            additional_files : list
            allocated
            allocated_workspaces : Dict[str, Any]
            already_codegened_subgraphs
            codegened_graph_stack : list
            comment : str
            computed_sizes : OrderedSet[sympy.Symbol]
            computed_sizes_stack : list
            debug_printer
            declare : str
            declare_maybe_reference : str
            ending : str
            freed
            header
            imports
            kernel_autotune_calls
            kernel_autotune_defs
            kernel_autotune_names
            kernel_numel_expr : OrderedSet[Tuple[str, GraphLowering]]
            last_seen_device_guard_index : Optional[int]
            launcher_fn_name : NoneType, str
            lines : List[Union[MemoryPlanningLine, LineContext]]
            move_begin : str
            move_end : str
            multi_kernel_state
            none_str : str
            prefix
            reuses : Dict[BufferName, BufferName]
            src_to_kernel : Dict[str, str]
            subgraph_definitions
            suffix
            supports_intermediate_hooks : bool
            unbacked_symbol_decls
            user_defined_kernel_cache : Dict[Tuple[Any, ...], Tuple[str, Any]]
            wrapper_call
            write_get_raw_stream
            add_benchmark_harness(output)
            add_meta_once(meta: TritonMetaParams) str
            benchmark_compiled_module(output)
            can_prove_buffer_has_static_shape(buffer)
            can_reuse(input_buffer, output_buffer)
            codegen_alloc_from_pool(name, offset, dtype, shape, stride) str
            codegen_allocation(buffer: ir.Buffer)
            codegen_conditional(conditional)
            codegen_cpp_sizevar(x: Expr) str
            codegen_deferred_allocation(name, layout)
            codegen_device_copy(src, dst, non_blocking: bool)
            codegen_device_guard_enter(device_idx: int) None
            codegen_device_guard_exit() None
            codegen_dynamic_scalar(node)
            codegen_exact_buffer_reuse(old_name: str, new_name: str, del_line: str)
            codegen_free(buffer)
            codegen_inplace_reuse(input_buffer: ir.Buffer, output_buffer: ir.Buffer)
            codegen_input_nan_asserts() None
            codegen_input_size_and_nan_asserts() None
            codegen_input_size_asserts() None
            codegen_input_symbol_assignment(name: str, value: ir.TensorBox, bound_vars: OrderedSet[sympy.Symbol])
            codegen_inputs()
            codegen_invoke_subgraph(invoke_subgraph)
            codegen_multi_output(name, value)
            codegen_python_shape_tuple(shape: Sequence[Expr]) str
            codegen_python_sizevar(x: Expr) str
            codegen_reinterpret_view(data, size, stride, offset, writeline: Callable[..., None], dtype) str
            codegen_shape_tuple(shape: Sequence[Expr]) str
            codegen_sizevar(x: Expr) str
            codegen_subgraph(subgraph, outer_inputs, outer_outputs)
            codegen_subgraph_by_inlining(subgraph, outer_inputs, outer_outputs)
            codegen_subgraph_call(subgraph, outer_inputs, outer_outputs)
            codegen_subgraph_prefix(subgraph, outer_inputs, outer_outputs)
            codegen_tuple_access(basename: str, name: str, index: str) str
            codegen_unbacked_symbol_decl(symbol)
            codegen_while_loop(while_loop)
            create(is_subgraph: bool, subgraph_name: str, parent_wrapper: PythonWrapperCodegen)
            define_kernel(kernel_name: str, kernel_body: str, metadata: Optional[str], gpu)
            define_subgraph_launcher_fn(fn_code: str)
            define_user_defined_triton_kernel(kernel, configs, kwargs, restore_value_args, reset_to_zero_args)
            did_reuse(buffer, reused_buffer)
            ensure_size_computed(sym: sympy.Symbol)
            enter_context(ctx)
            finalize_prefix()*
            generate(is_inference)
            generate_and_run_autotune_block()
            generate_before_suffix(result: IndentedBuffer) None
            generate_default_grid(kernel_name: str, grid_args: List[Any], gpu: bool, grid_callable: Optional[Callable[..., Any]])
            generate_end(result: IndentedBuffer) None
            generate_end_graph()
            generate_example_arg_value(arg, arg_type, raw_arg, index)
            generate_extern_kernel_alloc(extern_kernel, args)
            generate_extern_kernel_out(kernel: str, out: str, out_view: Optional[str], args: List[str])
            generate_fallback_kernel(fallback_kernel, args)
            generate_fallback_kernel_with_runtime_lookup(buf_name: str, python_kernel_name: str, cpp_kernel_name: str, codegen_args: List[str], op_overload: Optional[torch._ops.OpOverload], raw_args, outputs)
            generate_index_put_fallback(kernel, x, indices, values, accumulate)
            generate_kernel_call(kernel_name: str, call_args, grid, device_index, gpu, triton, arg_types, raw_args, grid_fn: str, triton_meta, autotune_configs, grid_extra_kwargs)
            generate_numel_expr(kernel_name: str, tree, suffix: Optional[str])
            generate_profiler_mark_wrapper_call(stack)
            generate_reset_kernel_saved_flags()
            generate_return(output_refs: List[str]) None
            generate_save_uncompiled_kernels()
            generate_scatter_fallback(output, inputs, cpp_kernel_name, python_kernel_name, src_is_tensor, reduce, kwargs)
            generate_start_graph()
            generate_tma_descriptor(desc)
            generate_user_defined_triton_kernel(kernel_name: str, raw_args: List[Any], grid: List[Any], configs, triton_meta, constexprs)
            generate_workspace_allocation(ws: WorkspaceArg)
            generate_workspace_deallocation(ws: WorkspaceArg)
            get_codegened_graph()
            get_output_refs() List[str]
            include_extra_header(header: str)*
            is_statically_known_list_of_ints(lst)
            make_allocation(name, device, dtype, shape, stride)
            make_buffer_allocation(buffer: BufferLike)
            make_buffer_free(buffer: BufferLike)
            make_buffer_reuse(old: BufferLike, new: BufferLike, delete_old: bool)
            make_free_by_names(names_to_del: List[str])
            make_tensor_alias(new_name, old_name, comment)
            make_zero_buffer(name)
            mark_output_type() None
            memory_plan()
            memory_plan_reuse()
            next_kernel_suffix() str
            pop_codegened_graph()
            pop_computed_sizes()
            prepare_triton_kernel_call(device_index, call_args)
            push_codegened_graph(graph)
            push_computed_sizes(computed_sizes)
            set_launcher_fn_name() None
            static_shape_for_buffer_or_none(buffer)
            statically_known_int_or_none(x)
            statically_known_list_of_ints_or_none(lst)
            val_to_arg_str(s, type_)
            wrap_kernel_call(name, call_args)
            write_async_compile_wait() None
            write_constant(name: str, hashed: str) None
            write_get_raw_stream(device_idx: int, graph) str
            write_get_raw_stream_header_once() None
            write_header() None
            write_kernel_autotune_defs_header() None
            write_prefix() None
            write_triton_header_once() None
            writeline(line)
            writelines(lines)
          }
          class PytreeFlatten {
            forward(x)
          }
          class PytreeThunk {
            is_really_simple : Optional[bool]
            is_simple : Optional[bool]
            spec : Optional[pytree.TreeSpec]
            set(spec: pytree.TreeSpec) None
            unflatten(x: List[Any]) Any
          }
          class QConfig {
          }
          class QConfigDynamic {
          }
          class QConfigMapping {
            global_qconfig : Optional[QConfigAny]
            module_name_object_type_order_qconfigs : OrderedDict[Tuple[str, Callable, int], QConfigAny]
            module_name_qconfigs : OrderedDict[str, QConfigAny]
            module_name_regex_qconfigs : OrderedDict[str, QConfigAny]
            object_type_qconfigs : OrderedDict[Union[Callable, str], QConfigAny]
            from_dict(qconfig_dict: Dict[str, Any]) QConfigMapping
            set_global(global_qconfig: QConfigAny) QConfigMapping
            set_module_name(module_name: str, qconfig: QConfigAny) QConfigMapping
            set_module_name_object_type_order(module_name: str, object_type: Callable, index: int, qconfig: QConfigAny) QConfigMapping
            set_module_name_regex(module_name_regex: str, qconfig: QConfigAny) QConfigMapping
            set_object_type(object_type: Union[Callable, str], qconfig: QConfigAny) QConfigMapping
            to_dict() Dict[str, Any]
          }
          class QConfigMultiMapping {
            qconfig_mappings_list : List[QConfigMapping]
            from_list_qconfig_mapping(qconfig_mapping_list: List[QConfigMapping]) QConfigMultiMapping
            set_global(global_qconfig_list: List[QConfigAny]) QConfigMultiMapping
            set_module_name(module_name: str, qconfig_list: List[QConfigAny]) QConfigMultiMapping
            set_module_name_object_type_order(module_name: str, object_type: Callable, index: int, qconfig_list: List[QConfigAny]) QConfigMultiMapping
            set_module_name_regex(module_name_regex: str, qconfig_list: List[QConfigAny]) QConfigMultiMapping
            set_object_type(object_type: Union[Callable, str], qconfig_list: List[QConfigAny]) QConfigMultiMapping
          }
          class QConvPointWiseBinaryPT2E {
            has_bias
            idx_for_inplace_sum : int
            codegen(wrapper)
            create(qx: 'TensorBox', x_scale: 'TensorBox', x_zero_point: 'TensorBox', qw: 'TensorBox', w_scale, w_zero_point, qaccum: 'TensorBox', bias: 'TensorBox', stride: List[int], padding: List[int], dilation: List[int], groups: int, output_scale: 'TensorBox', output_zero_point: 'TensorBox', output_dtype, accum_scale, accum_zero_point, binary_attr, alpha, unary_attr, unary_scalars, unary_algorithm)
            get_mutation_names()
            get_unbacked_symbol_defs() OrderedSet[sympy.Symbol]
          }
          class QConvPointWisePT2E {
            has_bias
            codegen(wrapper)
            create(qx: 'TensorBox', x_scale: 'TensorBox', x_zero_point: 'TensorBox', qw: 'TensorBox', w_scale: 'TensorBox', w_zero_point: 'TensorBox', bias: 'TensorBox', stride: List[int], padding: List[int], dilation: List[int], groups: int, output_scale: float, output_zero_point: int, output_dtype, attr, scalars, algorithm)
          }
          class QFunctional {
            activation_post_process
            scale : float
            zero_point : int
            add(x: Tensor, y: Tensor) Tensor
            add_relu(x: Tensor, y: Tensor) Tensor
            add_scalar(x: Tensor, y: float) Tensor
            cat(x: List[Tensor], dim: int) Tensor
            extra_repr()
            forward(x)
            from_float(mod, use_precomputed_fake_quant)
            matmul(x: Tensor, y: Tensor) Tensor
            mul(x: Tensor, y: Tensor) Tensor
            mul_scalar(x: Tensor, y: float) Tensor
          }
          class QInt32Storage {
            dtype()
          }
          class QInt8Storage {
            dtype()
          }
          class QLinearPointwiseBinaryPT2E {
            has_bias : bool
            idx_for_inplace_sum : int
            codegen(wrapper)
            create(qx: 'TensorBox', x_scale: 'TensorBox', x_zero_point: 'TensorBox', qw: 'TensorBox', w_scale: 'TensorBox', w_zero_point: 'TensorBox', other: 'TensorBox', bias: 'TensorBox', output_scale: float, output_zero_point: int, output_dtype, other_scale, other_zp, binary_post_op, binary_alpha, unary_post_op, unary_post_op_args, unary_post_op_algorithm)
            get_mutation_names()
          }
          class QLinearPointwisePT2E {
            has_bias : bool
            codegen(wrapper)
            create(qx: 'TensorBox', x_scale: 'TensorBox', x_zero_point: 'TensorBox', qw: 'TensorBox', w_scale: 'TensorBox', w_zero_point: 'TensorBox', bias: 'TensorBox', output_scale: float, output_zero_point: int, output_dtype, post_op_name, post_op_args, post_op_algorithm)
          }
          class QUInt2x4Storage {
            dtype()
          }
          class QUInt4x2Storage {
            dtype()
          }
          class QUInt8Storage {
            dtype()
          }
          class QualnameWrapper {
            training
          }
          class QuantStub {
            qconfig : NoneType
            forward(x)
          }
          class QuantStubModel {
            dequant
            fc
            qconfig
            quant
            forward(x)
          }
          class QuantSubModel {
            fc3
            sub1
            sub2
            forward(x)
          }
          class QuantType {
            name
          }
          class QuantWrapper {
            dequant
            module
            qconfig
            quant
            forward(X)
          }
          class QuantizationAnnotation {
            allow_implicit_sharing : bool
            input_qspec_map : Dict[Node, Optional[QuantizationSpecBase]]
            output_qspec : Optional[QuantizationSpecBase]
          }
          class QuantizationComparisonResult {
            actual
            mse_loss
            ref
            sqnr
            loss(loss_function: Callable[[torch.Tensor, torch.Tensor], torch.Tensor]) torch.Tensor
          }
          class QuantizationConfig {
            bias : Optional[QuantizationSpec]
            input_activation : Optional[QuantizationSpec]
            is_qat : bool
            output_activation : Optional[QuantizationSpec]
            weight : Optional[QuantizationSpec]
          }
          class QuantizationLiteTestCase {
          }
          class QuantizationSpec {
            ch_axis : Optional[int]
            dtype
            is_dynamic : bool
            observer_or_fake_quant_ctr : Union
            qscheme : Optional[torch.qscheme]
            quant_max : Optional[int]
            quant_min : Optional[int]
          }
          class QuantizationSpecBase {
          }
          class QuantizationTestCase {
            all_quant_types : list
            calib_data
            img_data_1d
            img_data_1d_train
            img_data_2d
            img_data_2d_train
            img_data_3d
            img_data_3d_train
            img_data_dict : dict
            static_quant_types : list
            train_data
            assert_ns_compare_dict_valid(act_compare_dict: Dict[str, Dict[str, Dict[str, Any]]]) None
            assert_types_for_matched_subgraph_pairs(matched_subgraph_pairs: Dict[str, Tuple[NSSubgraph, NSSubgraph]], expected_types: Dict[str, Tuple[Tuple[Callable, Callable], Tuple[Callable, Callable]]], gm_a: GraphModule, gm_b: GraphModule) None
            checkDynamicQuantizedLSTM(mod, reference_module_type, dtype)
            checkDynamicQuantizedLinear(mod, dtype)
            checkDynamicQuantizedLinearRelu(mod, dtype)
            checkDynamicQuantizedModule(mod, reference_module_type, dtype)
            checkEmbeddingSerialization(qemb, num_embeddings, embedding_dim, indices, offsets, set_qconfig, is_emb_bag, dtype)
            checkGraphModeFxOp(model, inputs, quant_type, expected_node, expected_node_occurrence, expected_node_list, is_reference, print_debug_info, custom_qconfig_dict, prepare_expected_node, prepare_expected_node_occurrence, prepare_expected_node_list, prepare_custom_config, backend_config)
            checkGraphModeOp(module, inputs, quantized_op, tracing, debug, check, eval_mode, dynamic, qconfig)
            checkGraphModuleNodes(graph_module, expected_node, expected_node_occurrence, expected_node_list)
            checkHasPrepModules(module)
            checkLinear(mod)
            checkNoPrepModules(module)
            checkNoQconfig(module)
            checkObservers(module, propagate_qconfig_list, prepare_custom_config_dict)
            checkQuantDequant(mod)
            checkQuantizedLinear(mod)
            checkScriptable(orig_mod, calib_data, check_save_load)
            checkWrappedQuantizedLinear(mod)
            check_eager_serialization(ref_model, loaded_model, x)
            check_weight_bias_api(ref_model, weight_keys, bias_keys)
            printGraphModule(graph_module, print_str)
            setUp()
          }
          class QuantizationTracer {
            record_stack_traces : bool
            scope
            skipped_module_classes : List[Callable]
            skipped_module_names : List[str]
            is_leaf_module(m: torch.nn.Module, module_qualified_name: str) bool
          }
          class Quantize {
            dtype
            scale
            zero_point
            extra_repr()
            forward(X)
            from_float(mod, use_precomputed_fake_quant)
          }
          class QuantizeHandler {
            is_custom_module_ : bool
            is_standalone_module_ : bool
            modules : Dict[str, torch.nn.Module]
            node_pattern : Union
            num_tensor_args : int
            root_node
            is_custom_module()
            is_general_tensor_value_op() bool
            is_standalone_module()
          }
          class QuantizedEngine {
            engine
            m
            supported_engines
          }
          class QuantizedGRU {
          }
          class QuantizedGRUCell {
          }
          class QuantizedGraphModule {
            preserved_attr_names : Set[str]
          }
          class QuantizedLSTM {
          }
          class QuantizedLSTMCell {
          }
          class QuantizedLinear {
          }
          class QuantizedLinearFP16 {
          }
          class QuantizedRNNBase {
          }
          class QuantizedRNNCell {
          }
          class QuantizedRNNCellBase {
          }
          class Quantizer {
            annotate(model: torch.fx.GraphModule)* torch.fx.GraphModule
            prepare_obs_or_fq_callback(model: torch.fx.GraphModule, edge_or_node_to_obs_or_fq: Dict[EdgeOrNode, ObserverOrFakeQuantize]) None
            transform_for_annotation(model: torch.fx.GraphModule) torch.fx.GraphModule
            validate(model: torch.fx.GraphModule)* None
          }
          class Queue {
          }
          class RAdam {
            step(closure)
          }
          class REDUCTION_OUTPUT_TYPE_KIND {
            name
          }
          class RETURN_TYPE {
            name
          }
          class RMSNorm {
            elementwise_affine : bool
            eps : Optional[float]
            normalized_shape : Tuple[int, ...]
            weight
            extra_repr() str
            forward(x: torch.Tensor) torch.Tensor
            reset_parameters() None
          }
          class RMSNormPython {
            eps : float
            weight
            forward(x)
          }
          class RMSprop {
            step(closure)
          }
          class RNN {
            nonlinearity
            forward(input: Tensor, hx: Optional[Tensor])* Tuple[Tensor, Tensor]
          }
          class RNNBase {
            batch_first : bool
            bias : bool
            bidirectional : bool
            dropout : float
            dtype
            hidden_size
            input_size
            mode
            num_layers : int
            training : bool
            version : int
            check_forward_args(input: Tensor, hidden: Tensor, batch_sizes: Optional[Tensor]) None
            check_hidden_size(hx: Tensor, expected_hidden_size: Tuple[int, int, int], msg: str) None
            check_input(input: Tensor, batch_sizes: Optional[Tensor]) None
            extra_repr()
            from_float(mod, use_precomputed_fake_quant)
            get_bias()
            get_expected_hidden_size(input: Tensor, batch_sizes: Optional[Tensor]) Tuple[int, int, int]
            get_weight()
            permute_hidden(hx: Tensor, permutation: Optional[Tensor]) Tensor
            set_weight_bias(weight_bias_dict)
          }
          class RNNBase {
            is_decomposed
          }
          class RNNBase {
            all_weights
            batch_first : bool
            bias : bool
            bidirectional : bool
            dropout : float
            hidden_size : int
            input_size : int
            mode : str
            num_layers : int
            proj_size : int
            check_forward_args(input: Tensor, hidden: Tensor, batch_sizes: Optional[Tensor])
            check_hidden_size(hx: Tensor, expected_hidden_size: Tuple[int, int, int], msg: str) None
            check_input(input: Tensor, batch_sizes: Optional[Tensor]) None
            extra_repr() str
            flatten_parameters() None
            get_expected_hidden_size(input: Tensor, batch_sizes: Optional[Tensor]) Tuple[int, int, int]
            permute_hidden(hx: Tensor, permutation: Optional[Tensor])
            reset_parameters() None
          }
          class RNNCell {
            nonlinearity : str
            forward(input: Tensor, hx: Optional[Tensor]) Tensor
            from_float(mod, use_precomputed_fake_quant)
          }
          class RNNCell {
            bias_hh
            bias_ih
            nonlinearity : str
            weight_hh
            weight_ih
            forward(input: Tensor, hx: Optional[Tensor]) Tensor
            from_float(mod, weight_qparams_dict)
          }
          class RNNCell {
            nonlinearity : str
            forward(input: Tensor, hx: Optional[Tensor]) Tensor
          }
          class RNNCellBase {
            bias : bool
            bias_hh
            bias_ih
            hidden_size
            input_size
            weight_dtype
            check_forward_hidden(input: Tensor, hx: Tensor, hidden_label: str) None
            check_forward_input(input)
            extra_repr()
            from_float(mod, use_precomputed_fake_quant)
            from_reference(ref_mod)
            get_bias()
            get_weight()
            set_weight_bias(weight_bias_dict)
          }
          class RNNCellBase {
            is_decomposed
            get_quantized_weight_hh()
            get_quantized_weight_ih()
            get_weight_hh()
            get_weight_ih()
          }
          class RNNCellBase {
            bias : bool
            bias_hh
            bias_ih
            hidden_size : int
            input_size : int
            weight_hh
            weight_ih
            extra_repr() str
            reset_parameters() None
          }
          class RNNCellDynamicModel {
            mod
            qconfig
            forward(x)
          }
          class RNNDynamicModel {
            mod
            qconfig
            forward(x)
          }
          class RNNDynamicQuantizeHandler {
          }
          class ROCmBenchmarkRequest {
            DLL : Optional[DLLWrapper]
            hash_key : str
            source_code : str
            source_file : str
            workspace : NoneType, Optional[torch.Tensor]
            workspace_size : int
            cleanup_run_fn() None
            ensure_dll_loaded()
            make_run_fn() Callable[[], None]
            precompile()
            update_workspace_size() None
          }
          class ROCmCPPScheduling {
            scheduler
            can_fuse_vertical(node1: BaseSchedulerNode, node2: BaseSchedulerNode) bool
            codegen_template(template_node: BaseSchedulerNode, epilogue_nodes: Sequence[BaseSchedulerNode], prologue_nodes: Sequence[BaseSchedulerNode])
            define_kernel(src_code: str, node_schedule) str
            group_fn(sizes)
            is_rocm_cpp_template(node: BaseSchedulerNode) bool
          }
          class ROCmCodeCache {
            cache : Dict[str, CacheEntry]
            cache_clear : staticmethod
            compile(source_code: str, dst_file_ext: str, extra_args: Optional[List[str]]) Tuple[str, str, str]
            load(source_code: str, dst_file_ext: str) Tuple[DLLWrapper, str, str]
            write(source_code: str, dst_file_ext: str) Tuple[str, str]
          }
          class ROCmKernel {
            overrides
          }
          class ROCmTemplate {
            index_counter : count
            input_nodes : List[Buffer]
            input_reorder : Optional[List[int]]
            layout
            output_node
            generate() ROCmTemplateCaller
            globals() IndentedBuffer
            header() IndentedBuffer
            render()* str
          }
          class ROCmTemplateBuffer {
            template : str
            workspace_size : int
            get_workspace_size()
          }
          class ROCmTemplateCaller {
            bmreq
            category : str
            info_kwargs : Optional[Dict[str, Union[PrimitiveInfoType, List[PrimitiveInfoType]]]]
            make_kernel_render : Callable[[ROCmTemplateBuffer, Optional[List[IRNode]]], str]
            template : str
            benchmark() float
            call_name() str
            hash_key() str
            info_dict() Dict[str, Union[PrimitiveInfoType, List[PrimitiveInfoType]]]
            output_node() TensorBox
            precompile() None
          }
          class ROCmTemplateKernel {
            kernel_name
            named_nodes : Dict[str, IRNode]
            signature : str
            arg_name(node: IRNode) Optional[str]
            call_kernel(name: str, node: 'ROCmTemplateBuffer') None
            def_kernel(inputs: List[IRNode], outputs: List[IRNode], size_args: List[str], names_str: str, input_reorder: Optional[List[int]]) str
            get_signature()
          }
          class RPCExecMode {
            name
          }
          class RReLU {
            inplace : bool
            lower : float
            upper : float
            extra_repr()
            forward(input: Tensor) Tensor
          }
          class RRef {
          }
          class RRefAPITest {
            test_local_rref_local_value()
            test_rref_is_owner()
            test_rref_list_mutate()
            test_rref_local_value()
            test_user_rrefs_confirmed()
            test_user_rrefs_confirmed_remote()
          }
          class RRefMeta {
          }
          class RRefProxy {
            rpc_api
            rpc_timeout
            rref
          }
          class RRefTypingTest {
            test_my_script_module_with_rrefs()
            test_rref_as_arg_and_return()
            test_rref_python_annotation()
          }
          class RShift {
            is_integer : bool
            eval(base, shift)
          }
          class RandomClassVariable {
            call_function(tx: 'InstructionTranslator', args, kwargs)
          }
          class RandomSampler {
            data_source : Sized
            generator : NoneType
            num_samples
            replacement : bool
          }
          class RandomSeeds {
          }
          class RandomStructured {
            PRUNING_TYPE : str
            amount
            dim : int
            apply(module, name, amount, dim)
            compute_mask(t, default_mask)
          }
          class RandomUnstructured {
            PRUNING_TYPE : str
            amount
            apply(module, name, amount)
            compute_mask(t, default_mask)
          }
          class RandomValueSource {
            random_call_index : int
            guard_source()
            name()
            reconstruct(codegen)
          }
          class RandomVariable {
            random : Random
            as_python_constant()
            call_method(tx, name, args: List[VariableTracker], kwargs: Dict[str, VariableTracker]) VariableTracker
            check_state(state)
            is_supported_random_obj(val)
            python_type()
            reconstruct(codegen)
            unwrap_state(state)
            wrap_state(state)
          }
          class RandomVariable {
          }
          class RangeConstraint {
            max_val : Annotated[Optional[int], 20]
            min_val : Annotated[Optional[int], 10]
          }
          class RangeVariable {
            apply_index(index)
            apply_slice(slice)
            as_proxy()
            as_python_constant()
            debug_repr()
            getitem_const(tx: 'InstructionTranslator', arg: VariableTracker)
            python_type()
            range_length()
            reconstruct(codegen: 'PyCodegen') None
            start()
            step()
            stop()
            unpack_var_sequence(tx)
            var_getattr(tx: 'InstructionTranslator', name)
          }
          class RdzvEvent {
            error_trace : str
            hostname : str
            local_id : Optional[int]
            master_endpoint : str
            message : str
            name : str
            node_state
            pid : int
            rank : Optional[int]
            run_id : str
            deserialize(data: Union[str, 'RdzvEvent']) 'RdzvEvent'
            serialize() str
          }
          class ReInplaceTrigger {
            name
          }
          class ReLU {
            inplace : bool
            training
            extra_repr() str
            forward(input: Tensor) Tensor
          }
          class ReLU6 {
            inplace : bool
            forward(input)
            from_float(mod, use_precomputed_fake_quant)
          }
          class ReLU6 {
            extra_repr() str
          }
          class ReadItem {
            dest_index
            dest_offsets
            lengths
            storage_index
            storage_offsets
            type
          }
          class ReadWrites {
            index_exprs : OrderedSet[IndexExprDep]
            range_vars : Optional[List[sympy.Expr]]
            reads : OrderedSet[Dep]
            var_ranges : Optional[VarRanges]
            writes : OrderedSet[Dep]
            buffer_names(ignore_integer_index)
            merge(other: 'ReadWrites')
            merge_list(read_writes: List['ReadWrites'])
            reads_and_writes()
            remove_reads(rem_reads)
            rename(renames: typing.Dict[str, str]) 'ReadWrites'
            with_read(dep: Union[Dep, OrderedSet[Dep]]) 'ReadWrites'
          }
          class ReaderInterp {
            run_node(n)
          }
          class ReadsWrites {
            reads : Set[Any]
            visited : Set[Any]
            writes : Set[Any]
          }
          class Realize {
            backward(ctx, grad_output)
            forward(ctx, x)
          }
          class ReasonFusedNodes {
            name
          }
          class RecompileError {
          }
          class RecompileLimitExceeded {
          }
          class RecordLoadStore {
          }
          class RecordOptimizationContext {
            current_node : Optional[torch.fx.Node]
            func_name : str
            opt_ctx : Optional[OptimizationContext]
            get_fx_node()
            get_opt_ctx()
          }
          class RecordingObserver {
            tensor_val : list
            calculate_qparams()
            forward(x)
            get_tensor_value()
          }
          class Rectangle {
            bottom : Optional[float]
            left : Optional[float]
            message : Optional[_message.Message]
            properties : Optional[_property_bag.PropertyBag]
            right : Optional[float]
            top : Optional[float]
          }
          class RecursiveScriptClass {
            forward_magic_method(method_name)
          }
          class RecursiveScriptModule {
            code
            code_with_constants
            graph
            inlined_graph
            original_name
            define(src)
            extra_repr()
            forward_magic_method(method_name)
            get_debug_state()
            graph_for()
            save(f)
            save_to_buffer()
          }
          class RedisRemoteCache {
          }
          class RedisRemoteCacheBackend {
          }
          class Redistribute {
            backward(ctx, grad_output: 'dtensor.DTensor')
            forward(ctx, input: 'dtensor.DTensor', device_mesh: DeviceMesh, placements: Tuple[Placement, ...], async_op: bool)
          }
          class ReduceAddCoalesced {
            backward(ctx)
            forward(ctx, destination, num_inputs)
          }
          class ReduceLROnPlateau {
            best : float
            cooldown : int
            cooldown_counter : int
            default_min_lr : NoneType, int
            eps : float
            factor : float
            in_cooldown
            last_epoch : NoneType, int
            min_lrs : list
            mode : Literal['min', 'max']
            mode_worse : float
            num_bad_epochs : int
            optimizer
            patience : int
            threshold : float
            threshold_mode : Literal['rel', 'abs']
            verbose : bool, str
            is_better(a, best)
            load_state_dict(state_dict)
            state_dict()
            step(metrics: SupportsFloat, epoch)
          }
          class ReduceScatter {
            op
            work(data)
          }
          class ReduceScatterState {
            event
            reduce_scatter_input
          }
          class Reduction {
            name
          }
          class Reduction {
            name
          }
          class Reduction {
            reduction_hint
            reduction_ranges : Sequence[_IntLike]
            reduction_type : str
            src_dtype
            constant_to_device(device: torch.device) IRNode
            create(device: torch.device, dst_dtype: torch.dtype, src_dtype: torch.dtype, inner_fn: Callable[..., Any], ranges: Sequence[Expr], reduction_ranges: Sequence[Expr], reduction_type: str, reduction_hint: ReductionHint, input_node: Optional[IRNode]) TensorBox
            create_multilayer(device: torch.device, dst_dtype: torch.dtype, src_dtype: torch.dtype, inner_fn: Callable[..., Any], ranges: Sequence[Expr], reduction_ranges: Sequence[Expr], reduction_type: str, split: _IntLike, reduction_hint: ReductionHint) TensorBox
            create_multilayer_existing_ranges(device: torch.device, dst_dtype: torch.dtype, src_dtype: torch.dtype, inner_fn: Callable[..., Any], original_ranges: Sequence[Expr], original_reduction_ranges: Sequence[Expr], new_ranges: List[Integer], new_reduction_ranges: List[Integer], reduction_type: str, reduction_hint: ReductionHint) TensorBox
            create_multilayer_helper(device: torch.device, dst_dtype: torch.dtype, src_dtype: torch.dtype, wrapper_fn: Callable[..., Any], original_ranges: Sequence[Expr], original_reduction_ranges: Sequence[Expr], new_ranges: List[Expr], new_reduction_ranges: List[Integer], reduction_type: str, split: _IntLike, reduction_hint: ReductionHint) TensorBox
            default_accumulator(reduction_type: str, dtype: torch.dtype) Union[_NumLike, Sequence[_NumLike]]
            default_value(reduction_type: str, dtype: torch.dtype) Union[_NumLike, Sequence[_NumLike]]
            get_reduction_size() Sequence[sympy.Expr]
            get_reduction_type() Optional[str]
            get_unbacked_symbol_uses() OrderedSet[Symbol]
            index_length() int
            inner_fn_args() Sequence[Sequence[Expr]]
            inner_fn_free_unbacked_symbols() OrderedSet[Symbol]
            num_splits(device: torch.device, dst_dtype: torch.dtype, src_dtype: torch.dtype, inner_fn: Callable[..., OpsValue], ranges: Sequence[_IntLike], reduction_ranges: Sequence[_IntLike], reduction_type: str, reduction_numel: Expr, input_node: Optional[IRNode]) Tuple[ReductionHint, _IntLike]
            store_reduction(output_name: Optional[str], indexer: Callable[[Sequence[Expr]], Never], vars: Sequence[Expr], reduction_vars: Sequence[Symbol]) OpsValue
          }
          class ReductionHint {
            name
          }
          class ReductionOpInfo {
            complex_to_real : bool
            generate_args_kwargs
            identity : NoneType
            nan_policy : NoneType
            promotes_int_to_int64 : bool
            result_dtype : NoneType
            supports_multiple_dims : bool
          }
          class ReductionPythonRefInfo {
            torch_opinfo
            torch_opinfo_name
            torch_opinfo_variant_name : str
            validate_view_consistency : bool
          }
          class ReductionTypePromotionRule {
            promotion_kind : REDUCTION_OUTPUT_TYPE_KIND
            preview_type_promotion(args: tuple, kwargs: dict) TypePromotionSnapshot
          }
          class ReenterWith {
            stack_index : int
            target_values : Optional[Tuple[Any, ...]]
            try_except_torch_function_mode(code_options, cleanup: List[Instruction])
            try_finally(code_options, cleanup: List[Instruction])
          }
          class ReferenceAnalysis {
            abs(x)
            add(a, b)
            and_(a, b)
            bitwise_and(a, b)
            bitwise_or(a, b)
            ceil(x)
            ceil_to_int(x, dtype)
            constant(c, dtype)
            eq(a, b)
            exp(x)
            floor(x)
            floor_to_int(x, dtype)
            floordiv(a, b)
            ge(a, b)
            gt(a, b)
            int_truediv(a, b)
            le(a, b)
            log(x)
            log2(x)
            lt(a, b)
            maximum(a, b)
            minimum(a, b)
            mod(x, y)
            mul(a, b)
            ne(a, b)
            neg(x)
            not_(a)
            or_(a, b)
            pow(a, b)
            pow_by_natural(a, b)
            reciprocal(x)
            round_decimal(a, b)
            round_to_int(a, dtype)
            sqrt(x)
            square(x)
            sub(a, b)
            sym_sum(args)
            to_dtype(x, dtype)
            truediv(a, b)
            trunc_to_int(x, dtype)
            truncdiv(a, b)*
          }
          class ReferenceQuantizedModule {
            is_decomposed : bool
            weight_axis_int : int
            weight_dtype
            weight_qscheme
            weight_quant_max : typing.Optional[int]
            weight_quant_min : typing.Optional[int]
            get_quantized_weight()
            get_weight()
          }
          class Refine {
            constraints : list
            symbol_iter : count
            traced
            convert_to_sympy_symbols(typ)
            infer_symbolic_relations(n: Node)
            refine()
            refine_node(n: Node)
            replace_dyn_with_fresh_var(typ)
            symbolic_relations()
          }
          class ReflectionPad1d {
            padding : Tuple[int, int]
          }
          class ReflectionPad2d {
            padding : Tuple[int, int, int, int]
          }
          class ReflectionPad3d {
            padding : Tuple[int, int, int, int, int, int]
          }
          class RegexPatternVariable {
          }
          class Region {
            byte_length : Optional[int]
            byte_offset : int
            char_length : Optional[int]
            char_offset : int
            end_column : Optional[int]
            end_line : Optional[int]
            message : Optional[_message.Message]
            properties : Optional[_property_bag.PropertyBag]
            snippet : Optional[_artifact_content.ArtifactContent]
            source_language : Optional[str]
            start_column : Optional[int]
            start_line : Optional[int]
          }
          class RegisterPostBackwardFunction {
            backward(ctx)
            forward(ctx, param_group: FSDPParamGroup)
          }
          class RegistrationHandle {
            destroy() None
          }
          class Registry {
            register(target: Callable, impl: Callable) None
          }
          class RegistryItem {
          }
          class ReinforcementLearningRpcTest {
            test_rl_rpc()
          }
          class ReinplaceCounters {
            add_missed_bytes(trigger: ReInplaceTrigger, bytes: int)
            add_missed_opportunities(trigger: ReInplaceTrigger, count: int)
            clear()
            get_total_missed()
            get_total_missed_bytes()
            log()
          }
          class ReinterpretView {
            dtype
            layout
            codegen_reference(writer: Optional[IndentedBuffer]) str
            freeze_layout()*
            get_device() Optional[torch.device]
            get_layout() Layout
            get_name()
            get_origin_node() Optional[torch.fx.Node]
            get_size() Sequence[Expr]
            get_stride()
            get_unbacked_symbol_uses() OrderedSet[sympy.Symbol]
            make_indexer() Callable[[Sequence[Expr]], Expr]
            make_loader() Callable[[Sequence[Expr]], OpsValue]
            num_reads() int
          }
          class RelaxedBernoulli {
            arg_constraints : dict
            has_rsample : bool
            logits
            probs
            support
            temperature
            expand(batch_shape, _instance)
          }
          class RelaxedBooleanPair {
          }
          class RelaxedNumberPair {
            atol
            rtol
          }
          class RelaxedOneHotCategorical {
            arg_constraints : dict
            has_rsample : bool
            logits
            probs
            support
            temperature
            expand(batch_shape, _instance)
          }
          class RelaxedUnspecConstraint {
            render(source: Source) str
          }
          class ReluCompileError {
          }
          class RemoteAOTAutogradCache {
          }
          class RemoteAutotuneCache {
          }
          class RemoteBundledAutotuneCache {
          }
          class RemoteCache {
            backend
            backend_override_cls : Optional[Callable[[], RemoteCacheBackend[Any]]]
            serde : RemoteCacheSerde[_T, _U]
            get(key: str) Optional[_T]
            put(key: str, value: _T) None
          }
          class RemoteCacheBackend {
            get(key: str) Optional[_T]
            put(key: str, data: _T) None
          }
          class RemoteCacheJsonSerde {
            decode(data: bytes) JsonDataTy
            encode(data: JsonDataTy) bytes
          }
          class RemoteCachePassthroughSerde {
            decode(data: _T) _T
            encode(data: _T) _T
          }
          class RemoteCacheSerde {
            decode(data: _U)* _T
            encode(data: _T)* _U
          }
          class RemoteDynamoPGOCache {
          }
          class RemoteEM {
            em
            forward(input: torch.Tensor)
          }
          class RemoteFxGraphCache {
          }
          class RemoteModule {
            generated_methods
            is_scriptable : bool
            module_rref : rpc.RRef[nn.Module]
          }
          class RemoteModuleTest {
            test_bad_module()
            test_forward_async()
            test_forward_async_script()
            test_forward_sync()
            test_forward_sync_script()
            test_forward_with_kwargs()
            test_get_module_rref()
            test_remote_module_py_pickle_not_supported()
            test_remote_module_py_pickle_not_supported_script()
            test_remote_parameters()
            test_send_remote_module_with_a_new_attribute_not_pickled_over_the_wire()
            test_train_eval()
            test_unsupported_methods()
          }
          class RemoteMyModuleInterface {
            forward(tensor: Tensor, number: int, word: str)* Tuple[str, int, Tensor]
            forward_async(tensor: Tensor, number: int, word: str)* Future[Tuple[str, int, Tensor]]
          }
          class RemoteNet {
            fc
            relu
            forward(input: torch.Tensor)
          }
          class RemovableHandle {
            extra_dict_ref : Tuple, tuple
            hooks_dict_ref
            id : int
            next_id : int
            remove() None
          }
          class RemovableHandleClass {
          }
          class RemovableHandleVariable {
            REMOVED : int
            idx : NoneType, int
            mutation_type : NoneType
            call_method(tx: 'InstructionTranslator', method_name, args, kwargs)
            python_type()
            reconstruct(codegen)
          }
          class RemoveInputMutation {
          }
          class RemoveNonTensorInputStep {
            apply(model_args: Sequence[Any], model_kwargs: Mapping[str, Any], model: torch.nn.Module | Callable | torch_export.ExportedProgram | None) tuple[Sequence[Any], Mapping[str, Any]]
          }
          class RemoveNoneInputStep {
            apply(model_args: Sequence[Any], model_kwargs: Mapping[str, Any], model: torch.nn.Module | Callable | torch_export.ExportedProgram | None) tuple[Sequence[Any], Mapping[str, Any]]
          }
          class RendezvousBackend {
            name
            get_state()* Optional[Tuple[bytes, Token]]
            set_state(state: bytes, token: Optional[Token])* Optional[Tuple[bytes, Token, bool]]
          }
          class RendezvousClosedError {
          }
          class RendezvousConnectionError {
          }
          class RendezvousError {
          }
          class RendezvousGracefulExitError {
          }
          class RendezvousHandler {
            use_agent_store
            get_backend()* str
            get_run_id()* str
            is_closed()* bool
            next_rendezvous()* RendezvousInfo
            num_nodes_waiting()* int
            set_closed()*
            shutdown()* bool
          }
          class RendezvousHandlerRegistry {
            create_handler(params: RendezvousParameters) RendezvousHandler
            register(backend: str, creator: RendezvousHandlerCreator) None
          }
          class RendezvousInfo {
            bootstrap_store_info
            rank
            store
            world_size
          }
          class RendezvousParameters {
            backend : str
            config : dict
            endpoint : str
            local_addr : Optional[str]
            max_nodes : int
            min_nodes : int
            run_id : str
            get(key: str, default: Any) Any
            get_as_bool(key: str, default: Optional[bool]) Optional[bool]
            get_as_int(key: str, default: Optional[int]) Optional[int]
          }
          class RendezvousSettings {
            keep_alive_interval : timedelta
            keep_alive_max_attempt : int
            max_nodes : int
            min_nodes : int
            run_id : str
            timeout
          }
          class RendezvousStateError {
          }
          class RendezvousStoreInfo {
            MASTER_ADDR_KEY : ClassVar[str]
            MASTER_PORT_KEY : ClassVar[str]
            master_addr : str
            master_port : int
            build(rank: int, store: Store, local_addr: Optional[str], server_port: Optional[int]) 'RendezvousStoreInfo'
          }
          class RendezvousTimeout {
            close
            heartbeat
            join
            last_call
          }
          class RendezvousTimeoutError {
          }
          class Repeat {
            input_dim
            times : int
            inputs() Iterable[DimSpec]
            new(dim: DimSpec, times: int) DimSpec
          }
          class RepeatIteratorVariable {
            item
            next_variable(tx)
            reconstruct(codegen)
          }
          class RepeatedExpr {
            fns
            inner_pattern
            op
            pattern_eq(other: Any) bool
          }
          class ReplaceFn {
          }
          class ReplaceGetAttrWithPlaceholder {
            replaced_attrs
          }
          class ReplaceViewOpsWithViewCopyOpsPass {
            call_operator(op, args, kwargs, meta)
          }
          class ReplacedPatterns {
            anchor
            nodes_map : Dict[Node, Node]
            replacements : List[Node]
          }
          class Replacement {
            deleted_region
            inserted_content : Optional[_artifact_content.ArtifactContent]
            properties : Optional[_property_bag.PropertyBag]
          }
          class ReplacementPatternEntry {
            normalize_args : Callable[..., List[Any]]
            apply(match: Match, graph: torch.fx.Graph, node: torch.fx.Node) None
            replace_with_graph(match: Match, graph: torch.fx.Graph, replacement_graph: Union[torch.fx.Graph, torch.fx.GraphModule], args: Sequence[torch.fx.Node]) None
          }
          class Replacer {
            preface : List[str]
            visit(node: ast.AST) Any
          }
          class Replacer {
            call_method : NoneType
            call_module : NoneType
            get_attr : NoneType
            run_node(node: torch.fx.Node) Any
          }
          class Replicate {
          }
          class ReplicationPad1d {
            padding : Tuple[int, int]
          }
          class ReplicationPad2d {
            padding : Tuple[int, int, int, int]
          }
          class ReplicationPad3d {
            padding : Tuple[int, int, int, int, int, int]
          }
          class ReportingConfiguration {
            enabled : bool
            level : Literal['none', 'note', 'warning', 'error']
            parameters : Optional[_property_bag.PropertyBag]
            properties : Optional[_property_bag.PropertyBag]
            rank : float
          }
          class ReportingDescriptor {
            default_configuration : Optional[_reporting_configuration.ReportingConfiguration]
            deprecated_guids : Optional[List[str]]
            deprecated_ids : Optional[List[str]]
            deprecated_names : Optional[List[str]]
            full_description : Optional[_multiformat_message_string.MultiformatMessageString]
            guid : Optional[str]
            help : Optional[_multiformat_message_string.MultiformatMessageString]
            help_uri : Optional[str]
            id : str
            message_strings : Optional[Any]
            name : Optional[str]
            properties : Optional[_property_bag.PropertyBag]
            relationships : Optional[List[_reporting_descriptor_relationship.ReportingDescriptorRelationship]]
            short_description : Optional[_multiformat_message_string.MultiformatMessageString]
          }
          class ReportingDescriptorReference {
            guid : Optional[str]
            id : Optional[str]
            index : int
            properties : Optional[_property_bag.PropertyBag]
            tool_component : Optional[_tool_component_reference.ToolComponentReference]
          }
          class ReportingDescriptorRelationship {
            description : Optional[_message.Message]
            kinds : List[str]
            properties : Optional[_property_bag.PropertyBag]
            target
          }
          class ReproState {
            graph
            inps : List[torch.Tensor]
          }
          class RequestQueue {
            get(size: int, timeout: float)* List[TimerRequest]
            size()* int
          }
          class ResNetBase {
            avgpool
            bn1
            conv1
            downsample
            fc
            myop
            relu1
            relu2
            forward(x)
            fuse_model()
          }
          class ReservedWorkflowIdUserError {
          }
          class ResetRequired {
          }
          class ReshapeTransform {
            bijective : bool
            in_shape
            out_shape
            codomain()
            domain()
            forward_shape(shape)
            inverse_shape(shape)
            log_abs_det_jacobian(x, y)
            with_cache(cache_size)
          }
          class Resize {
            backward(ctx, grad_output)
            forward(ctx, tensor, sizes)
          }
          class ResizeStorageBytes {
            cpp_kernel_name : str
            name
            python_kernel_name : str
          }
          class ResolvedExportOptions {
            decomposition_table : dict[torch._ops.OpOverload, Callable]
            diagnostic_context
            diagnostic_options
            dynamic_shapes : bool
            fake_context
            fx_tracer
            onnx_registry
            onnxfunction_dispatcher
          }
          class RestartAnalysis {
            restart_reason : str
          }
          class RestoreParameterAndBufferNames {
            original_nn_module
          }
          class RestrictedListSubclassVariable {
            user_cls
            user_cls_source
            value
            as_proxy()
            as_python_constant()*
            call_function(tx: 'InstructionTranslator', args: 'List[VariableTracker]', kwargs: 'Dict[str, VariableTracker]') 'VariableTracker'
            call_method(tx, name, args: List['VariableTracker'], kwargs: Dict[str, 'VariableTracker']) 'VariableTracker'
            debug_repr()
            is_matching_cls(user_cls: type)
            is_python_constant()
            modified(items)
            python_type()
            reconstruct(codegen: 'PyCodegen') None
          }
          class Result {
            analysis_target : Optional[_artifact_location.ArtifactLocation]
            attachments : Optional[List[_attachment.Attachment]]
            baseline_state : Optional[Literal['new', 'unchanged', 'updated', 'absent']]
            code_flows : Optional[List[_code_flow.CodeFlow]]
            correlation_guid : Optional[str]
            fingerprints : Optional[Any]
            fixes : Optional[List[_fix.Fix]]
            graph_traversals : Optional[List[_graph_traversal.GraphTraversal]]
            graphs : Optional[List[_graph.Graph]]
            guid : Optional[str]
            hosted_viewer_uri : Optional[str]
            kind : Literal['notApplicable', 'pass', 'fail', 'review', 'open', 'informational']
            level : Literal['none', 'note', 'warning', 'error']
            locations : Optional[List[_location.Location]]
            message
            occurrence_count : Optional[int]
            partial_fingerprints : Optional[Any]
            properties : Optional[_property_bag.PropertyBag]
            provenance : Optional[_result_provenance.ResultProvenance]
            rank : float
            related_locations : Optional[List[_location.Location]]
            rule : Optional[_reporting_descriptor_reference.ReportingDescriptorReference]
            rule_id : Optional[str]
            rule_index : int
            stacks : Optional[List[_stack.Stack]]
            suppressions : Optional[List[_suppression.Suppression]]
            taxa : Optional[List[_reporting_descriptor_reference.ReportingDescriptorReference]]
            web_request : Optional[_web_request.WebRequest]
            web_response : Optional[_web_response.WebResponse]
            work_item_uris : Optional[List[str]]
          }
          class Result {
            exception : Exception | None
            exported_program : torch.export.ExportedProgram | None
            strategy : str
            success
          }
          class Result {
            constants : Dict[str, Union[torch.Tensor, FakeScriptObject, torch.ScriptObject]]
            example_inputs : Optional[Tuple[Tuple[torch.Tensor, ...], Dict[str, Any]]]
            graph_module
            module_call_graph : List[ep.ModuleCallEntry]
            names_to_symbols : Dict[str, sympy.Symbol]
            signature
            state_dict : Dict[str, Union[torch.Tensor, torch.nn.Parameter]]
          }
          class ResultProvenance {
            conversion_sources : Optional[List[_physical_location.PhysicalLocation]]
            first_detection_run_guid : Optional[str]
            first_detection_time_utc : Optional[str]
            invocation_index : int
            last_detection_run_guid : Optional[str]
            last_detection_time_utc : Optional[str]
            properties : Optional[_property_bag.PropertyBag]
          }
          class ResumeFunctionMetadata {
            block_target_offset_remap : Optional[Dict[int, int]]
            code
            instructions : List[Instruction]
            prefix_block_target_offset_remap : List[int]
          }
          class ReturnValueHandler {
            index : List[List[int]]
            total_count
            duplicate_eager_tensors(eager_tensor_list)
          }
          class ReturnValueOp {
          }
          class ReuseInputObserver {
            calculate_qparams()
            forward(x)
          }
          class ReuseLine {
            delete_old : bool
            node : Union
            reused_as : Union
            codegen(code: IndentedBuffer) None
            plan(state: MemoryPlanningState) MemoryPlanningLine
          }
          class RewritingTracer {
            trace(root: Union[torch.nn.Module, Callable], concrete_args: Optional[Dict[str, Any]]) Graph
          }
          class RewrittenModule {
          }
          class RootDim {
            derived : List[str]
            max : Union[int, None]
            min : int
          }
          class RoundDecimal {
            is_real : bool
            eval(number, ndigits)
          }
          class RoundRobinDispatch {
            codegen_pid_range(kernel: 'ComboKernel', num: int, code: IndentedBuffer) None
            grid(sub_kernel_numels: List[List[int]], x_blocks_list: List[Union[str, int]], dynamic_shape: bool) Tuple[Any, ...]
          }
          class RoundToInt {
            is_integer : bool
            eval(number)
          }
          class RoutedDecoderIterDataPipe {
            datapipe : Iterable[Tuple[str, BufferedIOBase]]
            decoder
            add_handler() None
          }
          class RowwiseParallel {
            desired_input_layouts : Tuple[Placement, ...], tuple
            input_layouts : tuple
            output_layouts : tuple
            use_local_output : bool
          }
          class RpcAgentTestFixture {
            file_init_method
            init_method
            rpc_backend
            rpc_backend_options
            world_size
            get_shutdown_error_regex()*
            get_timeout_error_regex()*
            setup_fault_injection(faulty_messages, messages_to_delay)*
          }
          class RpcTest {
            timed_out_rpc_event : NoneType
            check_profiling_info(self_worker_name, dst_worker_name, func, rpc_event, rpc_exec_mode)
            return_callee_id()
            run_profiling_workload(dst)
            test_add()
            test_add_done_callback()
            test_add_with_id()
            test_all_gather()
            test_all_gather_timeout()
            test_async_add()
            test_async_class_method()
            test_async_class_method_remote()
            test_async_class_rref_proxy()
            test_async_class_rref_proxy_async()
            test_async_class_rref_proxy_remote()
            test_async_function_chained()
            test_async_function_chained_remote()
            test_async_function_multi_chained()
            test_async_function_multi_chained_async()
            test_async_function_multi_chained_remote()
            test_async_function_multi_fanout()
            test_async_function_multi_fanout_async()
            test_async_function_multi_fanout_remote()
            test_async_function_nested()
            test_async_function_nested_remote()
            test_async_function_raise()
            test_async_function_raise_async()
            test_async_function_raise_remote()
            test_async_function_simple()
            test_async_function_with_future_ctor()
            test_async_function_with_future_ctor_remote()
            test_async_function_wrong_return_type()
            test_async_function_wrong_return_type_async()
            test_async_function_wrong_return_type_remote()
            test_async_record_function_cbs_jit_call()
            test_async_record_function_double_end_callbacks()
            test_async_record_function_legacy()
            test_async_static_method()
            test_async_static_method_remote()
            test_build_rpc_profiling_key()
            test_builtin_remote_ret()
            test_builtin_remote_self()
            test_call_method_on_rref()
            test_callback_chain()
            test_callback_in_rpc()
            test_callback_multi()
            test_callback_none()
            test_callback_simple()
            test_callback_with_error()
            test_callback_with_ret()
            test_callback_wrong_arg_num()
            test_callback_wrong_arg_type()
            test_cannot_infer_backend_from_options()
            test_custom_exception_throw_during_reconstruction()
            test_deadlock()
            test_debug_info()
            test_default_timeout_used()
            test_disable_gil_profiling()
            test_dist_init_decorator()
            test_duplicate_name()
            test_duplicate_name_2()
            test_expected_src()
            test_function_not_on_callee()
            test_future_done()
            test_future_done_exception()
            test_future_in_rpc()
            test_future_nested_callback()
            test_future_wait_twice()
            test_get_worker_infos()
            test_graceful_shutdown_with_uneven_workload()
            test_handle_send_exceptions()
            test_ignore_rref_leak()
            test_init_pg_then_rpc()
            test_init_rpc_then_pg()
            test_init_rpc_twice()
            test_int_callee()
            test_invalid_names()
            test_local_rref_no_fork()
            test_local_shutdown()
            test_local_shutdown_with_rpc()
            test_local_value_not_on_owner()
            test_mark_future_twice()
            test_multi_builtin_remote_ret()
            test_multi_layer_nested_async_rpc()
            test_multi_py_udf_remote()
            test_multi_rpc()
            test_my_parameter_server()
            test_nested_remote()
            test_nested_rpc()
            test_nested_rref()
            test_nested_rref_stress()
            test_non_cont_tensors()
            test_non_garbage_collected_user_rref_due_to_local_circular_dependency()
            test_nonzero()
            test_owner_equality()
            test_owner_rref_backward()
            test_pass_local_rrefs()
            test_pg_init_no_rpc_init()
            test_pickle_future()
            test_profiler_export_trace()
            test_profiler_remote_events_profiled()
            test_profiler_remote_events_profiled_single_threaded()
            test_profiler_rpc_key_names()
            test_profiler_rpc_memory()
            test_profiler_rpc_record_shapes()
            test_profiler_with_async_rpc_builtin()
            test_profiler_with_async_rpc_builtin_single_threaded()
            test_profiler_with_async_rpc_udf()
            test_profiler_with_async_rpc_udf_single_threaded()
            test_profiler_with_autograd_context()
            test_profiler_with_autograd_context_single_threaded()
            test_profiler_with_remote_builtin()
            test_profiler_with_remote_builtin_single_threaded()
            test_profiler_with_remote_udf()
            test_profiler_with_remote_udf_single_threaded()
            test_profiler_with_script_async_rpc()
            test_profiler_with_script_async_rpc_single_threaded()
            test_profiler_with_script_remote_rpc()
            test_profiler_with_script_remote_rpc_single_threaded()
            test_profiler_with_script_sync_rpc()
            test_profiler_with_script_sync_rpc_single_threaded()
            test_profiler_with_sync_rpc_builtin()
            test_profiler_with_sync_rpc_builtin_single_threaded()
            test_profiler_with_sync_rpc_udf()
            test_profiler_with_sync_rpc_udf_single_threaded()
            test_py_built_in()
            test_py_class_constructor()
            test_py_class_instance_method()
            test_py_class_method()
            test_py_class_static_method()
            test_py_function_exception()
            test_py_multi_async_call()
            test_py_nested_pickle()
            test_py_no_return_result()
            test_py_raise_in_user_func()
            test_py_raise_in_user_func_escaped_str()
            test_py_rpc_rref_args()
            test_py_rref_args()
            test_py_rref_args_user_share()
            test_py_tensors()
            test_py_tensors_in_container()
            test_py_tensors_multi_async_call()
            test_py_udf_remote()
            test_py_user_defined()
            test_register_rpc_backend_and_set_and_start_rpc_backend(mock_rpc_agent, mock_dist_autograd_init)
            test_reinit()
            test_remote_same_worker()
            test_remote_throw()
            test_remote_with_exception()
            test_return_future()
            test_return_future_async()
            test_return_future_remote()
            test_return_local_rrefs()
            test_rpc_barrier_all()
            test_rpc_barrier_multithreaded()
            test_rpc_barrier_partial_subset()
            test_rpc_barrier_subset()
            test_rpc_profiling_async_function()
            test_rpc_profiling_async_function_single_threaded()
            test_rpc_profiling_remote_record_function()
            test_rpc_return_rref()
            test_rpc_timeouts()
            test_rref_context_debug_info()
            test_rref_forward_chain()
            test_rref_get_future()
            test_rref_leak()
            test_rref_proxy_class()
            test_rref_proxy_class_self()
            test_rref_proxy_non_exist()
            test_rref_proxy_reuse()
            test_rref_proxy_tensor()
            test_rref_proxy_tensor_self()
            test_rref_py_pickle_not_supported()
            test_rref_str()
            test_rref_timeout()
            test_rref_type_blocking()
            test_rref_type_non_blocking()
            test_rref_type_owner_blocking()
            test_rref_type_owner_non_blocking()
            test_rref_type_slow_init()
            test_rref_type_with_error_blocking()
            test_rref_type_with_error_non_blocking()
            test_scalar_add()
            test_self_add()
            test_self_py_udf_remote()
            test_self_remote_rref_as_remote_arg()
            test_self_remote_rref_as_rpc_arg()
            test_self_remote_rref_as_self_remote_arg()
            test_self_remote_rref_as_self_rpc_arg()
            test_send_to_rank()
            test_server_process_global_profiler()
            test_set_and_get_default_rpc_timeout()
            test_shutdown_errors()
            test_shutdown_followed_by_rpc()
            test_stress_heavy_rpc()
            test_stress_heavy_rpc_torchscript()
            test_stress_light_rpc()
            test_use_rpc_pickler()
            test_use_rref_after_shutdown()
            test_user_rref_backward()
            test_user_rrefs_confirmed()
            test_user_rrefs_confirmed_remote()
            test_wait_all()
            test_wait_all_exit_early_builtin()
            test_wait_all_exit_early_python()
            test_wait_all_exit_early_script_function()
            test_wait_all_multiple_call()
            test_wait_all_raise_in_body()
            test_wait_all_raise_in_user_func()
            test_wait_all_timeout()
            test_wait_all_with_exception()
            test_wait_all_with_partial_exception()
            test_wait_all_workers_dense()
            test_wait_all_workers_timeout()
            test_wait_all_workers_twice_dense()
            test_worker_id()
            test_worker_info_pickle()
            test_world_size_one()
            test_wrong_types()
            timed_out_rpc()
            validate_profiling_workload(dst, prof)
          }
          class RpcTestCommon {
          }
          class Rprop {
            step(closure)
          }
          class Rule {
            full_description : str | None
            full_description_markdown : str | None
            help_uri : str | None
            id : str
            message_default_template : str
            name : str
            short_description : str | None
            format(level: Level) tuple[Rule, Level, str]
            format_message() str
            from_sarif()
            sarif() sarif.ReportingDescriptor
          }
          class RuleCollection {
            custom_collection_from_list(new_collection_class_name: str, rules: Sequence[Rule]) RuleCollection
          }
          class Run {
            addresses : Optional[List[_address.Address]]
            artifacts : Optional[List[_artifact.Artifact]]
            automation_details : Optional[_run_automation_details.RunAutomationDetails]
            baseline_guid : Optional[str]
            column_kind : Optional[Literal['utf16CodeUnits', 'unicodeCodePoints']]
            conversion : Optional[_conversion.Conversion]
            default_encoding : Optional[str]
            default_source_language : Optional[str]
            external_property_file_references : Optional[_external_property_file_references.ExternalPropertyFileReferences]
            graphs : Optional[List[_graph.Graph]]
            invocations : Optional[List[_invocation.Invocation]]
            language : str
            logical_locations : Optional[List[_logical_location.LogicalLocation]]
            newline_sequences : List[str]
            original_uri_base_ids : Optional[Any]
            policies : Optional[List[_tool_component.ToolComponent]]
            properties : Optional[_property_bag.PropertyBag]
            redaction_tokens : Optional[List[str]]
            results : Optional[List[_result.Result]]
            run_aggregates : Optional[List[_run_automation_details.RunAutomationDetails]]
            special_locations : Optional[_special_locations.SpecialLocations]
            taxonomies : Optional[List[_tool_component.ToolComponent]]
            thread_flow_locations : Optional[List[_thread_flow_location.ThreadFlowLocation]]
            tool
            translations : Optional[List[_tool_component.ToolComponent]]
            version_control_provenance : Optional[List[_version_control_details.VersionControlDetails]]
            web_requests : Optional[List[_web_request.WebRequest]]
            web_responses : Optional[List[_web_response.WebResponse]]
          }
          class RunAndSaveRngState {
          }
          class RunAutomationDetails {
            correlation_guid : Optional[str]
            description : Optional[_message.Message]
            guid : Optional[str]
            id : Optional[str]
            properties : Optional[_property_bag.PropertyBag]
          }
          class RunConstGraph {
          }
          class RunOnlyContext {
          }
          class RunProcsResult {
            failures : Dict[int, ProcessFailure]
            return_values : Dict[int, Any]
            stderrs : Dict[int, str]
            stdouts : Dict[int, str]
            is_failed() bool
          }
          class RunResult {
            failures : Dict[int, ProcessFailure]
            return_values : Dict[int, Any]
            state
            is_failed() bool
          }
          class RunWithRNGStateHigherOrderVariable {
            call_function(tx: 'InstructionTranslator', args: 'List[VariableTracker]', kwargs: 'Dict[str, VariableTracker]') 'VariableTracker'
          }
          class RunWithRngState {
          }
          class Runnable {
            score : tuple
            snode
          }
          class RuntimeAssert {
            expr : str
            msg : str
            stack
          }
          class RuntimeErrorWithDiagnostic {
            diagnostic
          }
          class RuntimeEstimator {
            fake_mode
            mod_bw_post_order : List[str]
            mod_bw_pre_order : List[str]
            mod_fw_post_order : List[str]
            mod_fw_pre_order : List[str]
            mod_runtimes : Dict[str, Dict[str, float]], defaultdict
            total_runtime : float
            display_modulewise_stats(depth: int) None
          }
          class RuntimeSchemaInfo {
            needs_pytree : bool
            static_argnum : int
            static_kwargkey : Optional[List[str]]
          }
          class RuntimeWrapper {
            disable_amp : bool
            indices_of_inps_to_detach : List[int]
            trace_joint : bool
            post_compile(compiled_fn, aot_config: AOTConfig)
          }
          class SACDecision {
            name
          }
          class SACEstimator {
            sac_mod_greedy_order_meta : Dict[str, SACGreedyOrderMeta]
            sac_mod_stats : Dict[str, SACStats]
            sac_mod_tradeoff_stats : Dict[str, SACTradeOffStats]
            display_modulewise_sac_stats(depth: int, print_tabular: bool) None
            display_sac_stats(sac_stats: SACStats, print_tabular: bool) None
            display_sac_tradeoff_stats(greedy_order_meta: SACGreedyOrderMeta, sac_stats: SACStats, print_tabular: bool) None
            pwlf_sac_tradeoff_curve(n_segments: int, save_tradeoff_graphs: bool) None
          }
          class SACGreedyOrderMeta {
            inplace_op_groups : Dict[int, Set[int]]
            msps_meta : List[MSPS]
            random_ops_group : Dict[int, Set[int]]
            recomputed_ops : Set[int]
            stored_ops : Set[int]
          }
          class SACStats {
            force_store_random : bool
            func_names : List[str]
            inplace_ops : List[Tuple[int, int]]
            memory : List[int]
            rand_ops : List[int]
            runtimes : List[float]
            saved_autograd_ops : List[int]
            view_like_ops : List[int]
          }
          class SACTradeOffStats {
            fit_breaks : List[float]
            intercepts : List[float]
            n_segments : int
            sac_memory : int
            sac_runtime : float
            slopes : List[float]
            tradeoff_curve : OrderedDict[float, float]
          }
          class SDPAKernelVariable {
            prev_backends : list
            create(tx: 'InstructionTranslator', backends)
            enter(tx)
            exit(tx: 'InstructionTranslator')
            fn_name()
            module_name()
          }
          class SDPAParamsVariable {
            param_vars
            proxy
            as_proxy()
            create(tx: 'InstructionTranslator', value, source)
            is_sdpa_params(value)
            reconstruct(codegen)
            var_getattr(tx: 'InstructionTranslator', name: str) VariableTracker
          }
          class SELU {
            inplace : bool
            extra_repr() str
            forward(input: Tensor) Tensor
          }
          class SGD {
            step(closure)
          }
          class SHARDING_PRIORITIES {
            name
          }
          class SIMDKernel {
            allow_block_ptr : bool
            body
            code_hash : Optional[str]
            cooperative_reduction : Optional[bool]
            features
            index_dtype
            indexing_code
            inside_reduction : bool
            iter_vars_count : count
            kernel_name : str
            kexpr : Callable[[sympy.Expr], str]
            mutations
            no_x_dim : bool
            num_reduction_dims
            numels
            persistent_reduction : Optional[bool]
            range_tree_nodes : Dict[sympy.Symbol, IterationRangesEntry]
            range_trees : List[IterationRangesRoot]
            sexpr
            simplify_indexing
            active_range_trees(reorder)
            call_kernel(name: str, node: Optional[IRNode])* None
            codegen_body()*
            codegen_indexing(expr: sympy.Expr)
            codegen_iteration_ranges_entry(entry: IterationRangesEntry)*
            codegen_kernel()*
            codegen_nan_check()* None
            combine_contiguous_dims(index: sympy.Expr, tree: IterationRangesRoot)
            combine_modular_indexing_pairs(index)
            construct_range_trees(pid_cache, inside_reduction, is_reduction, numels, no_x_dim)
            dense_size_list() List[str]
            dense_size_str()
            disable_reduction()
            dtype_to_str(dtype: torch.dtype)* str
            estimate_kernel_num_bytes()
            finalize_indexing(indices: Sequence[sympy.Expr])*
            get_strides_of_load(index: sympy.Expr)
            index_to_str(index: sympy.Expr) str
            indexing_size_str(i)
            initialize_range_tree(pid_cache)
            is_broadcasted(index: sympy.Expr)
            is_compatible(groups: Iterable[sympy.Expr], lengths: Sequence[Sequence[sympy.Expr]])
            is_indirect_indexing(index: sympy.Expr)
            map_kernel_groups_to_node_sizes(groups: Sequence[sympy.Expr], lengths: Sequence[Sequence[sympy.Expr]], set_ranges) List[List[sympy.Expr]]
            mask_loads(mask, value)
            prepare_indexing(index: sympy.Expr)
            set_ranges()
            should_use_cooperative_reduction() bool
            should_use_persistent_reduction() bool
            split_and_set_ranges(lengths: Sequence[Sequence[sympy.Expr]])
            store_reduction(name: str, index: sympy.Expr, value: CSEVariable)
            triton_tensor_ndim()
            var_ranges()
            want_no_x_dim()
            warn_mix_layout(kernel_name)
            welford_reduce_fallback(dtype, value)
          }
          class SIMDKernelFeatures {
            node_schedule : List[NodeScheduleEntry]
            numel : Expr
            reduction_numel : Expr
            buf_accesses() Dict[str, List[Dep]]
            contains_op(op_name: str) bool
            get_mutations() OrderedSet[str]
            get_reduction_hint() ReductionHint
            has_non_contiguous_pw_in_reduction_kernel() bool
            is_reduction() bool
            op_counts() collections.Counter[str]
            reduction_hint(node: Any) ReductionHint
            reduction_nodes() List[SchedulerNode]
            scheduler_nodes() Iterable[SchedulerNode]
            select_index_dtype() torch.dtype
          }
          class SIMDScheduling {
            can_fuse_horizontal
            can_fuse_vertical
            kernel_type
            scheduler
            can_fuse(node1, node2)
            can_use_32bit_indexing(numel: sympy.Expr, buffers: Iterable[Union[ir.Buffer, ir.TensorBox]]) bool
            candidate_tilings(node)
            codegen_combo_kernel(combo_kernel_node)
            codegen_comment(node_schedule)*
            codegen_node(node: Union[scheduler.FusedSchedulerNode, scheduler.SchedulerNode])
            codegen_node_schedule(kernel_features: SIMDKernelFeatures)
            codegen_node_schedule_with_kernel(node_schedule, kernel)
            codegen_sync()
            codegen_template(template_node, epilogue_nodes, prologue_nodes) Optional[str]
            create_kernel_choices(kernel_features: SIMDKernelFeatures, kernel_args, kernel_kwargs) List[SIMDKernel]
            create_tiling(pw_tiling: Sequence[sympy.Expr], reduction_tiling: Sequence[sympy.Expr]) Dict[str, sympy.Expr]
            define_kernel(src_code, node_schedule, kernel)*
            flush()*
            generate_combo_kernel_code(subkernel_nodes: List[BaseSchedulerNode], custom_part_algorithm: bool, enable_autotune: bool, mixed_sizes: bool, only_gen_src_code: bool) List[Tuple[str, Any, Any]]
            generate_kernel_code_from_nodes(nodes, benchmark_kernel)
            generate_node_schedule(nodes, numel, rnumel)
            group_fn(sizes)
            ready_to_flush() bool
            select_tiling(node_schedule, numel, reduction_numel) Dict[str, sympy.Expr]
          }
          class SLoc {
            framework_loc : Optional[Union[traceback.FrameSummary, str]]
            maybe_user_loc : Optional[str]
          }
          class SWALR {
            anneal_epochs : int
            anneal_func
            get_lr()
          }
          class SaliencyPruner {
            update_mask(module, tensor_name)
          }
          class SampleInput {
            args : NoneType, tuple
            broadcasts_input : NoneType, bool
            input
            kwargs : NoneType, dict
            name : NoneType, str
            output_process_fn_grad : NoneType
            noncontiguous()
            numpy()
            summary()
            transform(f)
            with_metadata()
          }
          class SampleRule {
            name : str
            op_match_fn : Optional[Callable[[str, OpInfo], bool]]
            sample_match_fn : Optional[Callable[[torch.device, SampleInput], bool]]
            get_context(test_case)*
            type()* str
          }
          class Sampler {
          }
          class SamplerIterDataPipe {
            datapipe
            sampler
            sampler_args : NoneType, tuple
            sampler_kwargs : NoneType, dict
          }
          class SarifLog {
            inline_external_properties : Optional[List[_external_properties.ExternalProperties]]
            properties : Optional[_property_bag.PropertyBag]
            runs : List[_run.Run]
            schema_uri : Optional[str]
            version : Literal['2.1.0']
          }
          class SaveForwardInputsModel {
            c1
            c2
            forward_inputs : Dict[nn.Module, torch.Tensor]
            forward(x: torch.Tensor) torch.Tensor
          }
          class SaveForwardInputsModule {
            cast_forward_inputs : bool
            forward_inputs : Dict[nn.Module, torch.Tensor]
            l
            forward(x: torch.Tensor) torch.Tensor
          }
          class SavePlan {
            items : List[WriteItem]
            planner_data : Optional[Any]
            storage_data : Optional[Any]
          }
          class SavePlanner {
            create_global_plan(all_plans: List[SavePlan])* Tuple[List[SavePlan], Metadata]
            create_local_plan()* SavePlan
            finish_plan(new_plan: SavePlan)* SavePlan
            resolve_data(write_item: WriteItem)* Union[torch.Tensor, io.BytesIO]
            set_up_planner(state_dict: STATE_DICT_TYPE, storage_meta: Optional[StorageMeta], is_coordinator: bool)* None
          }
          class SavedTensorBox {
            tensors : List[VariableTracker]
          }
          class ScalarOutput {
            forward(x)
          }
          class ScalarType {
            Double : int
            Float : int
            Int : int
            Long : int
            Undefined : int
          }
          class ScalarType {
            name
          }
          class ScaleGradGenVmap {
            generate_vmap_rule : bool
            scale : float
            backward(ctx, grad_output)
            forward(x)
            jvp(ctx, x_tangent)
            setup_context(ctx, inputs, outputs)*
          }
          class Scan {
            combine_fn : Callable[[Tuple[Any, ...], Tuple[Any, ...]], Tuple[Any, ...]]
            dtypes : Tuple[torch.dtype, ...]
            inner_fns : Tuple[Callable[..., Any], ...]
            output_index : int
            reduction_hint
            reindex : Callable[[Sequence[_IntLike], Sequence[_IntLike]], Sequence[_IntLike]]
            scan_ranges : List[Integer]
            size : List[Integer]
            create(device: torch.device, dtypes: Tuple[torch.dtype, ...], inner_fns: Tuple[Callable[[Sequence[Expr]], Any], ...], size: List[Integer], axis: int, combine_fn: Callable[[Tuple[Any, ...], Tuple[Any, ...]], Tuple[Any, ...]], reduction_hint: ReductionHint) Sequence[Optional[TensorBox]]
            get_pointwise_size() Sequence[Expr]
            get_reduction_size() Sequence[sympy.Expr]
            get_reduction_type() Optional[str]
            get_size() Sequence[Expr]
            get_unbacked_symbol_uses() OrderedSet[Symbol]
            index_length() int
            inner_fn_args() Sequence[Sequence[_IntLike]]
            inner_fn_free_unbacked_symbols() OrderedSet[Symbol]
            num_splits(device: torch.device, dtype: torch.dtype, inner_fn: Callable[[Sequence[Expr]], OpsValue], axis: int, pointwise_ranges: List[Integer], scan_ranges: List[Integer], combine_fn: Callable[[Tuple[Any, ...], Tuple[Any, ...]], Tuple[Any, ...]], scan_numel: Expr) Tuple[ReductionHint, _IntLike]
            store_reduction(output_name: Optional[str], indexer: Callable[[Sequence[_IntLike]], Never], vars: Sequence[Expr], scan_vars: Sequence[Symbol]) OpsValue
          }
          class ScanHigherOrderVariable {
            call_function(tx: 'InstructionTranslator', args: List[VariableTracker], kwargs: Dict[str, VariableTracker]) VariableTracker
          }
          class ScanOp {
          }
          class ScanState {
            name
          }
          class Scatter {
            output_indexer : Callable[[Sequence[Expr]], Expr]
            scatter_mode : Optional[str]
            constant_to_device(device: torch.device) IRNode
            store_output(output_name: Optional[str], indexer: Callable[[Sequence[Expr]], Never], vars: Sequence[Expr]) OpsValue
          }
          class Scatter {
            src
            work(data)
          }
          class Scatter {
            backward(ctx)
            forward(ctx, target_gpus, chunk_sizes, dim, input)
          }
          class ScatterFallback {
            name
            src_is_tensor
            codegen(wrapper) None
            get_mutation_names()
            get_unbacked_symbol_defs() OrderedSet[sympy.Symbol]
            should_allocate() bool
          }
          class Schedule1F1B {
          }
          class ScheduleGPipe {
          }
          class ScheduleInterleaved1F1B {
            microbatches_per_round
            n_local_stages
            number_of_rounds
            pipeline_order : Dict[int, List[Optional[_Action]]]
            pp_group_size
            rank
          }
          class ScheduleInterleavedZeroBubble {
            microbatches_per_round
            n_local_stages
            number_of_rounds
            pipeline_order : Dict[int, List[Optional[_Action]]], dict
            pp_group_size
            rank
          }
          class ScheduleLoopedBFS {
            pipeline_order : Dict[int, List[Optional[_Action]]]
          }
          class ScheduleZBVZeroBubble {
            n_local_stages
            num_stages
            pipeline_order : Dict[int, List[Optional[_Action]]]
            pp_group_size
            rank
          }
          class Scheduler {
            available_buffer_names
            backends : Dict[torch.device, BaseScheduling]
            buffer_names_to_free
            completed_operations
            current_device
            logged_slow_fusion
            mutation_real_name : Dict[str, str]
            mutation_renames : Dict[str, str]
            name_to_buf : Dict[str, SchedulerBuffer]
            name_to_donated_buffer : Dict[str, SchedulerDonatedBuffer]
            name_to_fused_node : Dict[str, BaseSchedulerNode]
            name_to_node : Dict[str, BaseSchedulerNode]
            nodes : list
            num_orig_nodes
            origin_to_index : Dict[torch.fx.Node, int]
            post_grad_graph_id
            are_long_distant_nodes(node1: BaseSchedulerNode, node2: BaseSchedulerNode) bool
            benchmark_combo_kernel(node_list: Sequence[BaseSchedulerNode]) Tuple[float, float, str]
            benchmark_fused_nodes(nodes: Sequence[BaseSchedulerNode]) Tuple[float, str]
            can_buffer_be_removed_through_fusion(name: str, fused_node_names: OrderedSet[str]) bool
            can_fuse(node1: BaseSchedulerNode, node2: BaseSchedulerNode) bool
            can_fuse_vertical(node1: BaseSchedulerNode, node2: BaseSchedulerNode) bool
            can_fusion_increase_peak_memory(node1: BaseSchedulerNode, node2: BaseSchedulerNode) bool
            codegen() None
            codegen_extern_call(scheduler_node: ExternKernelSchedulerNode) None
            compute_ancestors() None
            compute_dependencies() None
            compute_last_usage() None
            create_backend(device: torch.device) BaseScheduling
            create_combo_kernel_nodes(num_ck_nodes: Optional[int]) None
            create_foreach_nodes() None
            create_scheduler_node(node: ir.Operation) BaseSchedulerNode
            dead_node_elimination() None
            debug_draw_graph() None
            debug_print_nodes(label: str) None
            decide_fusion_fail_reason(node1: BaseSchedulerNode, node2: BaseSchedulerNode, common_buf_names: Tuple[str, ...]) str
            dep_size_hint(dep: Dep) int
            enter_context(node: BaseSchedulerNode) None
            finalize_multi_template_buffers() None
            flush() None
            free_buffers() None
            fusable_read_and_write(read: Dep, write: MemoryDep) bool
            fusable_weak_dep(weak_dep: WeakDep, node1: BaseSchedulerNode, node2: BaseSchedulerNode) bool
            fuse_nodes(nodes: List[BaseSchedulerNode]) List[BaseSchedulerNode]
            fuse_nodes_once(nodes: List[BaseSchedulerNode]) List[BaseSchedulerNode]
            get_backend(device: Optional[torch.device]) BaseScheduling
            get_buffer_layout(buf_name: str) ir.Layout
            get_donated_buffers() Dict[str, SchedulerDonatedBuffer]
            get_possible_fusions(nodes: List[BaseSchedulerNode]) List[Tuple[BaseSchedulerNode, BaseSchedulerNode]]
            get_possible_fusions_with_highest_priority(possible_fusions: List[Tuple[BaseSchedulerNode, BaseSchedulerNode]]) List[Tuple[BaseSchedulerNode, BaseSchedulerNode]]
            merge_loops() None
            process_grouped_nodes() None
            prune_redundant_deps(nodes: List[BaseSchedulerNode]) None
            score_fusion_key(nodes: Tuple[BaseSchedulerNode, BaseSchedulerNode]) Any
            score_fusion_memory(node1: BaseSchedulerNode, node2: BaseSchedulerNode) int
            shared_data_after_reordering_loop(node1: BaseSchedulerNode, node2: BaseSchedulerNode) int
            speedup_by_combo_kernel(nodes: List[BaseSchedulerNode]) bool
            speedup_by_fusion(node1: BaseSchedulerNode, node2: BaseSchedulerNode) bool
            topological_sort_schedule(nodes: List[BaseSchedulerNode]) List[BaseSchedulerNode]
            unfusable_node(node: BaseSchedulerNode) bool
            update_zero_dim_cpu_tensor() None
            will_fusion_create_cycle(node1: BaseSchedulerNode, node2: BaseSchedulerNode) bool
          }
          class SchedulerBuffer {
            defining_op
            mpi_buffer
            node
            scheduler
            users : List[NodeUser]
            allocate() None
            can_free() bool
            debug_str() str
            get_aliases() Sequence[str]
            get_mutations() Sequence[str]
            get_name() str
            set_users(users: List[NodeUser]) None
          }
          class SchedulerDonatedBuffer {
            defining_op : Optional[BaseSchedulerNode]
          }
          class SchedulerNode {
            group : tuple
            last_usage
            max_order
            min_order
            apply_new_loop_order(new_order: Sequence[int]) None
            can_inplace(read_dep: dependencies.Dep) bool
            codegen(index_vars: Sequence[Sequence[sympy.Expr]]) None
            debug_str_extra() str
            get_ranges() Sequence[Sequence[sympy.Expr]]
            get_template_node() Optional[ir.TemplateBuffer]
            is_reduction() bool
            is_split_scan() bool
            is_template() bool
            pointwise_read_writes() dependencies.ReadWrites
            ranges_from_index_vars(index_vars: Sequence[Sequence[sympy.Expr]]) Dict[sympy.Expr, sympy.Expr]
            recompute_size_and_body(extra_indexing_constraints: Optional[Tuple[Dict[Any, Any], List[Any]]], recompute_sizes_body_func: Optional[Callable[..., Any]]) None
            refresh_dependencies(normalize: bool) None
            reorder_loops_by_dep_pair(self_dep: MemoryDep, other_dep: MemoryDep) None
            run() None
          }
          class SchemaCheckMode {
            aliasing : list
            mutated : list
            ops : list
            display_ops()
            reset_cache()
          }
          class SchemaInfo {
            args : List[AliasInfo]
            outs : List[AliasInfo]
          }
          class SchemaMatcher {
            inputs_are_mutable(t: _ExtraFields_TorchOp) Tuple[Optional[bool], ...]
            lookup_schemas(name: str) Optional[Tuple[FunctionSchema, ...]]
            match_schemas(t: _ExtraFields_TorchOp) Tuple[FunctionSchema, ...]
          }
          class SchemaUpdateError {
          }
          class SchemaVersion {
            major : Annotated[int, 10]
            minor : Annotated[int, 20]
          }
          class Scope {
            module_path : str
            module_type : Any
          }
          class ScopeContextManager {
          }
          class ScopeContextManager {
          }
          class ScopedDict {
            new_items : dict
            original_dict
            get(key, default)
          }
          class ScriptMeta {
          }
          class ScriptModule {
            forward : Callable[..., Any]
            define(src)
          }
          class ScriptObjectMeta {
            class_fqn : str
            constant_name : str
          }
          class ScriptObjectQualifiedNameSource {
            guard_source()
            name()
            reconstruct(codegen)
          }
          class ScriptWarning {
          }
          class SearchFn {
          }
          class Select {
            backward(ctx, grad_output)
            forward(x, idx)
            jvp(ctx, x_tangent, _)
            setup_context(ctx, inputs, output)
            vmap(info, in_dims, x, idx)
          }
          class SelectGenVmap {
            generate_vmap_rule : bool
            backward(ctx, grad_output)
            forward(x, idx)
            jvp(ctx, x_tangent, _)
            setup_context(ctx, inputs, outputs)
          }
          class SelectiveCheckpointContext {
            is_recompute
          }
          class SequenceKey {
            idx : int
            get(sequence: Sequence[T]) T
          }
          class SequenceParallel {
            sequence_sharding : tuple
            use_local_output : bool
          }
          class SequenceWrapperMapDataPipe {
            sequence
          }
          class Sequential {
            append(module: Module) 'Sequential'
            extend(sequential) 'Sequential'
            forward(input)
            insert(index: int, module: Module) 'Sequential'
            pop(key: Union[int, slice]) Module
          }
          class SequentialDispatch {
            codegen_pid_range(kernel: 'ComboKernel', num: int, code: IndentedBuffer) None
            grid(sub_kernel_numels: List[List[int]], x_blocks_list: List[Union[str, int]], dynamic_shape: bool) Tuple[Any, ...]
          }
          class SequentialLR {
            last_epoch : int
            optimizer
            load_state_dict(state_dict)
            recursive_undo(sched)
            state_dict()
            step()
          }
          class SequentialSampler {
            data_source : Sized
          }
          class SerializableAOTDispatchCompiler {
            compiler_fn : Callable[[torch.fx.GraphModule, Sequence[InputType]], TOutputCode]
            output_code_ty : Type[TOutputCode]
          }
          class Serialization {
            name
          }
          class SerializeError {
          }
          class SerializedArtifact {
            constants : bytes
            example_inputs : bytes
            exported_program : bytes
            state_dict : bytes
          }
          class SetFwdGradEnabledContextManager {
            prev_state
            create(tx: 'InstructionTranslator', target_values)
            enter(tx)
            exit(tx: 'InstructionTranslator')
          }
          class SetPair {
            CLS : set
          }
          class SetSourceTensorKernel {
            mutation_outputs : list
            get_inputs_that_alias_output() Sequence[str]
          }
          class SetVariable {
            set_items
            as_proxy()
            as_python_constant()
            call_method(tx, name, args: List[VariableTracker], kwargs: Dict[str, VariableTracker]) 'VariableTracker'
            debug_repr()
            getitem_const(tx: 'InstructionTranslator', arg: VariableTracker)
            python_type()
            reconstruct(codegen)
          }
          class Shadow {
            dequant : DeQuantize
            logger
            orig_module
            shadow_module
            add(x: torch.Tensor, y: torch.Tensor) torch.Tensor
            add_relu(x: torch.Tensor, y: torch.Tensor) torch.Tensor
            add_scalar(x: torch.Tensor, y: float) torch.Tensor
            cat(x: List[torch.Tensor], dim: int) torch.Tensor
            forward() torch.Tensor
            mul(x: torch.Tensor, y: torch.Tensor) torch.Tensor
            mul_scalar(x: torch.Tensor, y: float) torch.Tensor
          }
          class ShadowLogger {
            forward(x, y)
          }
          class ShapeAsConstantBuffer {
            expr : Expr
            codegen_reference(writer: Optional[IndentedBuffer]) str
            get_unbacked_symbol_uses() OrderedSet[sympy.Symbol]
            has_tensor_output() bool
          }
          class ShapeComputeModule {
          }
          class ShapeEnv {
            allow_complex_guards_as_runtime_asserts
            allow_dynamic_output_shape_ops
            allow_scalar_outputs
            assume_static_by_default
            axioms : Dict[sympy.Expr, sympy.Expr]
            check_recorded_events
            co_fields : dict
            counter : Counter[str]
            deferred_runtime_asserts : Dict[Optional[sympy.Symbol], List[RuntimeAssert]]
            dim_constraints : Optional[DimConstraints]
            divisible : Set[sympy.Expr], set
            duck_shape
            events : List[ShapeEnvEvent]
            fake_tensor_cache : Dict[torch._subclasses.fake_tensor._DispatchCacheKey, torch._subclasses.fake_tensor._DispatchCacheEntry]
            frozen : bool
            fx_node_cache : Dict[Tuple[Callable, Tuple[Any, ...]], torch.fx.Node]
            graph
            guards : List[ShapeGuard]
            is_recording : bool
            log : NoneType, RootLogger
            name_to_node : Dict[str, torch.fx.Node]
            num_deferred_runtime_asserts : int
            oblivious_var_to_val : Dict[sympy.Symbol, sympy.Integer]
            pending_fresh_unbacked_symbols : List[sympy.Symbol]
            prefer_deferred_runtime_asserts_over_guards
            replacements : Dict[sympy.Symbol, sympy.Expr]
            replacements_slocs : Dict[sympy.Symbol, SLoc]
            runtime_asserts_frozen : bool
            settings
            should_record_events : NoneType
            size_like : Set[sympy.Symbol]
            source_name_to_debug_name : Dict[str, str]
            source_to_symbol : Dict[str, sympy.Symbol]
            source_to_var : Dict[str, sympy.Symbol]
            specialize_zero_one
            symbol_guard_counter : Counter[sympy.Symbol]
            tracked_fakes : NoneType
            unbacked_alloc_order : Dict[sympy.Symbol, int]
            unbacked_renamings : Dict[sympy.Symbol, sympy.Symbol]
            unbacked_symfloat_counter : count
            unbacked_symint_counter : count
            unbacked_var_to_val : Dict[sympy.Symbol, sympy.Integer]
            val_to_var : Dict[int, sympy.Symbol], dict
            validator
            var_to_range : Dict[sympy.Symbol, ValueRanges]
            var_to_range_sloc : Dict[sympy.Symbol, ValueRangesSLoc]
            var_to_sources : Dict[sympy.Symbol, List[Source]]
            var_to_stack : Dict[sympy.Symbol, CapturedTraceback]
            var_to_val : Dict[sympy.Symbol, sympy.Integer]
            add_var_to_val(expr: sympy.Symbol, val: int) None
            bind_symbols(placeholders: Sequence[FakeTensor], args: Sequence[Tensor]) Dict[sympy.Symbol, int]
            bound_sympy(expr: sympy.Expr, size_oblivious: bool) ValueRanges
            check_equal(other: ShapeEnv) None
            cleanup() None
            constrain_symbol_range(s: sympy.Symbol, compiler_min: int, compiler_max: int) None
            create_symbol(val: int, source: Source, dynamic_dim: DimDynamic, constraint_dim: DimConstraint, positive: Optional[bool], do_not_specialize_zero_one: bool, symbolic_context: Optional[StatelessSymbolicContext]) sympy.Expr
            create_symbolic_sizes_strides_storage_offset(ex: torch.Tensor, source: Source) Tuple[Tuple[Union[int, SymInt], ...], Tuple[Union[int, SymInt], ...], Union[int, SymInt]]
            create_symboolnode(sym: sympy.Expr) SymBool
            create_symfloatnode(sym: sympy.Expr) Union[float, SymFloat]
            create_symintnode(sym: sympy.Expr) Union[int, SymInt]
            create_unbacked_symbool() SymBool
            create_unbacked_symfloat() SymFloat
            create_unbacked_symint(source: Optional[Source]) SymInt
            create_unspecified_symbol(val: Union[int, SymInt, float, SymFloat], source: Source, dynamic_dim: DimDynamic, constraint_dim: DimConstraint, symbolic_context: Optional[StatelessSymbolicContext]) sympy.Expr
            create_unspecified_symint_and_symbol(value: int, source: Source, dynamic_dim: DimDynamic) Union[int, SymInt]
            defer_runtime_assert(orig_expr: SympyBoolean, msg: str, fx_node: Optional[torch.fx.Node]) bool
            deserialize_symexpr(code: str) Union[SymInt, SymFloat, SymBool]
            evaluate_expr(orig_expr: sympy.Basic, hint: Optional[Union[int, bool, float]], fx_node: Optional[torch.fx.Node], size_oblivious: bool) sympy.Basic
            evaluate_guards_expression(code: str, args: Sequence[object]) bool
            evaluate_guards_for_args(placeholders: Sequence[FakeTensor], args: Sequence[Tensor]) bool
            evaluate_symexpr(code: str) Union[int, float, bool]
            format_guards(verbose: bool) str
            freeze() None
            freeze_runtime_asserts() None
            get_axioms(symbols: Optional[Tuple[sympy.Symbol]], compute_hint: bool) Tuple[SympyBoolean, ...]
            get_implications(e: SympyBoolean) Tuple[Tuple[SympyBoolean, sympy.logic.boolalg.BooleanAtom], ...]
            get_nontrivial_guards() List[SympyBoolean]
            get_pruned_guards(symints: Sequence[torch.SymInt]) List[ShapeGuard]
            has_hint(expr: sympy.Expr) bool
            ignore_fresh_unbacked_symbols() Iterator[None]
            is_unbacked_symint(symbol: sympy.Symbol) bool
            produce_guards() List[str]
            produce_guards_expression(placeholders: Sequence[Union[SymInt, FakeTensor]]) Optional[str]
            produce_guards_verbose(placeholders: Sequence[FakeTensor], sources: Sequence[Source], source_ref: Callable[[Source], str]) Tuple[List[str], List[str]]
            replace(expr: _SympyT) _SympyT
            set_unbacked_var_to_val(k: sympy.Symbol, v: int) None
            simplify(expr: _SympyT) _SympyT
            size_hint(expr: sympy.Basic) Optional[sympy.Basic]
            suppress_guards() _GeneratorContextManager[None]
          }
          class ShapeEnvEvent {
            args : Optional[List[Any]]
            f : Callable
            kwargs : Optional[Dict[str, Any]]
            name : Optional[str]
            tracked_fakes : Optional[List[Any]]
            is_create_fx_call_function() bool
            is_defer_runtime_assert() bool
            is_evaluate_expr() bool
            run(shape_env) Any
          }
          class ShapeEnvSettings {
            allow_complex_guards_as_runtime_asserts : bool
            allow_dynamic_output_shape_ops : bool
            allow_scalar_outputs : bool
            assume_static_by_default : bool
            duck_shape : bool
            prefer_deferred_runtime_asserts_over_guards : bool
            specialize_zero_one : bool
          }
          class ShapeEnvSource {
            guard_source()
            name()
          }
          class ShapeFuncInfo {
            ref
          }
          class ShapeGuard {
            expr : Boolean
            sloc
          }
          class ShapeGuardPrinter {
          }
          class ShapeGuardPythonPrinter {
            print_source(source: Source) str
          }
          class ShapeProp {
            fake_mode : NoneType
            fake_module : NoneType
            module
            real_module
            propagate()
            run_node(n: Node) Any
          }
          class ShapesCollection {
            dynamic_shapes(m, args, kwargs)
          }
          class Shard {
            dim : int
          }
          class Shard {
            metadata
            tensor
            from_tensor_and_offsets(tensor: torch.Tensor, shard_offsets: List[int], rank: int)
          }
          class ShardMetadata {
            placement : Optional[_remote_device]
            shard_offsets : List[int]
            shard_sizes : List[int]
          }
          class ShardedGradScaler {
            process_group
            scale(outputs: torch.Tensor) torch.Tensor
            unscale_(optimizer: torch.optim.Optimizer) None
            update(new_scale: Optional[Union[float, torch.Tensor]]) None
          }
          class ShardedOptimStateDictConfig {
          }
          class ShardedOptimizer {
            named_params : Mapping[str, Union[Tensor, ShardedTensor]]
            param_groups
            state
            add_param_group(param_group: Any)*
            load_state_dict(state_dict: Mapping[str, Any])*
            state_dict()* Dict[str, Any]
            step(closure)
            zero_grad(set_to_none: bool)
          }
          class ShardedState {
            name
          }
          class ShardedStateDictConfig {
          }
          class ShardedTensor {
            cpu(memory_format, process_group) ShardedTensor
            cuda(device, non_blocking, memory_format, process_group) ShardedTensor
            gather(dst: int, out: Optional[torch.Tensor], enforce_dtype: bool, dtype: Optional[torch.dtype]) None
            is_pinned() bool
            local_tensor() torch.Tensor
            remote_shards() Dict[int, List[rpc.RRef[Shard]]]
            reshard(resharding_spec: shard_spec.ShardingSpec) ShardedTensor
            sharding_spec() shard_spec.ShardingSpec
            to() ShardedTensor
          }
          class ShardedTensorBase {
            local_shards() List[Shard]
            metadata() ShardedTensorMetadata
          }
          class ShardedTensorMetadata {
            shards_metadata : List[ShardMetadata]
            size
            tensor_properties
          }
          class ShardedTensorTestBase {
            world_size
            assert_sharded_tensor_equal(st1, st2)
            destroy_comms(destroy_rpc)
            init_comms(init_rpc, backend)
            init_pg(backend)
            init_rpc()
            setUp() None
          }
          class Sharder {
            shard(module: nn.Module)* nn.Module
          }
          class ShardingFilterIterDataPipe {
            groups : Dict[int, Tuple[int, int]]
            instance_id : int
            num_of_instances : int
            sharding_group_filter : NoneType
            source_datapipe
            apply_sharding(num_of_instances, instance_id, sharding_group)
          }
          class ShardingPlan {
            output_plan : Optional[Dict[str, ShardingSpec]]
            plan : Dict[str, Union[ShardingSpec, Sharder]]
            return_local_tensor : Optional[List[str]]
          }
          class ShardingPlanner {
            build_plan(module: nn.Module)* ShardingPlan
          }
          class ShardingPropagator {
            op_strategy_funcs : Dict[OpOverload, Callable[[DeviceMesh, OpSchema], StrategyType]]
            op_to_rules : Dict[OpOverload, Callable[[OpSchema], OutputSharding]]
            op_to_schema_info : Dict[OpOverload, RuntimeSchemaInfo]
            op_to_shape_and_stride_idx : Dict[OpOverload, Union[int, Tuple[int, int]]]
            propagate_op_sharding
            propagate(op_info: OpInfo) None
            propagate_op_sharding_non_cached(op_schema: OpSchema) OutputSharding
            register_op_strategy(op_overload: OpOverload, strategy_func: Callable[[DeviceMesh, OpSchema], StrategyType], schema_info: Optional[RuntimeSchemaInfo])
            register_sharding_prop_rule(op_overload: OpOverload, rule_func: Callable[[OpSchema], OutputSharding], schema_info: Optional[RuntimeSchemaInfo])
          }
          class ShardingSpec {
            build_metadata(tensor_sizes: torch.Size, tensor_properties: sharded_tensor_meta.TensorProperties)* sharded_tensor_meta.ShardedTensorMetadata
            shard(tensor: torch.Tensor, src_rank: int, process_group)* 'ShardedTensor'
          }
          class ShardingStrategy {
            name
          }
          class SharedCache {
            limit : int
            lock : lock
            free_dead_references()
            get(key)
          }
          class SharedParamInfo {
            module
            module_name : str
            param_name : str
            prim_module
            prim_module_name : str
            prim_param_name : str
          }
          class SharedQuantizationSpec {
            edge_or_node : Union
          }
          class Shim {
            ref : Any
          }
          class Shim {
            ref : Any
          }
          class ShortStorage {
            dtype()
          }
          class ShortStorage {
            dtype()
          }
          class ShuffleDataFramesPipe {
            source_datapipe
          }
          class ShufflerIterDataPipe {
            datapipe : MapDataPipe[_T_co]
            indices : NoneType, list
            reset() None
            set_seed(seed: int)
            set_shuffle(shuffle)
          }
          class ShufflerIterDataPipe {
            buffer_size : int
            datapipe : IterDataPipe[_T_co]
            reset() None
            set_seed(seed: int)
            set_shuffle(shuffle)
          }
          class SiLU {
            inplace : bool
            extra_repr() str
            forward(input: Tensor) Tensor
          }
          class SideEffects {
            ca_final_callbacks_var : NoneType
            id_to_variable : Dict[int, VariableTracker]
            keepalive : List[Any]
            output_graph_weakref
            save_for_backward : list
            store_attr_mutations : Dict[VariableTracker, Dict[str, VariableTracker]]
            tensor_hooks : dict
            track_mutable
            check_allowed_side_effect(item)
            clear()
            clone()
            cls_supports_mutation_side_effects(cls)
            codegen_hooks(cg)
            codegen_save_tempvars(cg: PyCodegen)
            codegen_update_mutated(cg: PyCodegen)
            diff(other: 'SideEffects') Optional[str]
            get_ca_final_callbacks_var()
            has_pending_mutation(item)
            has_pending_mutation_of_attr(item, name)
            is_attribute_mutation(item)
            is_empty()
            is_modified(item)
            load_attr(item, name, deleted_ok, check)
            load_cell(cellvar)
            load_global(gvar: VariableTracker, name: str)
            mutation(var)
            prune_dead_object_new(tx)
            register_hook(tensor, hook, handle, name)
            remove_hook(idx)
            should_allow_side_effects_under_checkpoint()
            store_attr(item: VariableTracker, name: str, value: VariableTracker)
            store_cell(cellvar, value)
            store_global(gvar: VariableTracker, name: str, value: VariableTracker)
            track_cell_existing(source: Optional[Source], cell: CellType, contents: VariableTracker)
            track_cell_new()
            track_global_existing(source: Source, item: Any)
            track_object_existing(item: Any, variable: VariableTracker)
            track_object_new(cls_source: Source, user_cls: Any, variable_cls: Any, options)
            track_object_new_from_user_defined_class(cls_variable: 'variables.UserDefinedClassVariable')
            track_save_for_backward(ctx, args)
            track_tensor_variables_from_runahead_side_effects(other)
          }
          class Sigmoid {
            output_scale : float
            output_zero_point : int
            forward(input)
            from_float(mod, use_precomputed_fake_quant)
          }
          class Sigmoid {
            forward(input: Tensor) Tensor
          }
          class SigmoidTransform {
            bijective : bool
            codomain
            domain
            sign : int
            log_abs_det_jacobian(x, y)
          }
          class SignalException {
            sigval : Signals
          }
          class SimpleCSEHandler {
            cse_cache : Dict[str, Union[T, Tuple[T, ...]]]
            mock
            indirect_indexing() sympy.Expr
            store()* T
            store_reduction()* T
          }
          class SimpleConditionalModel {
            nn1
            nn2
            nn3
            nn4
            rank
            state : int
            forward(input)
          }
          class SimpleConv2d {
            conv2d1
            conv2d2
            seq
            forward(x: torch.Tensor) torch.Tensor
          }
          class SimpleElasticAgent {
            get_event_failed() Event
            get_event_succeeded() Event
            get_worker_group(role: str) WorkerGroup
            record_duration(state: str)
            run(role: str) RunResult
          }
          class SimpleLibraryRegistry {
            find(qualname: str) 'SimpleOperatorEntry'
          }
          class SimpleLinear {
            linear1
            linear2
            seq
            forward(x: torch.Tensor) torch.Tensor
          }
          class SimpleMegatronLM {
            fc1
            fc2
            gelu
            forward(inp)
            get_bias_grads()
            get_biases()
            get_weight_grads()
            get_weights()
          }
          class SimpleOperatorEntry {
            abstract_impl
            fake_impl
            qualname : str
            torch_dispatch_rules
          }
          class SimpleProfiler {
            profiling : Set[str]
            results : Dict[str, float]
            dump_and_reset(msg: str) None
            profile(profile_type: str) Iterator[None]
            reset() None
          }
          class SimpleQueue {
          }
          class SimplifyIndexing {
            name : str
            check_bounds(index, size, lower, upper)
            index_expr(index, dtype)
            load(name: str, index: sympy.Expr)
            store(name, index, value, mode)
            store_reduction(name, index, value)
          }
          class SimulateBackwardError {
            backward(ctx, input)
            forward(ctx, input)
          }
          class SimulateError {
            backward(ctx, grad_output)
            forward(ctx, input)
          }
          class SingleLayerFunctionalConvModel {
            conv1
            forward(x)
            get_example_inputs() Tuple[Any, ...]
          }
          class SingleLayerFunctionalLinearModel {
            linear1
            forward(x)
            get_example_inputs() Tuple[Any, ...]
          }
          class SingleLayerLinearDynamicModel {
            fc1
            qconfig
            forward(x)
            get_example_inputs() Tuple[Any, ...]
          }
          class SingleLayerLinearModel {
            fc1
            forward(x)
            get_example_inputs() Tuple[Any, ...]
          }
          class Singleton {
          }
          class SingletonInt {
            free_symbols
          }
          class SizeArg {
            alias_of
            expr : Expr
            name : str
          }
          class SizeMap {
          }
          class SizeVarAllocator {
            inv_precomputed_replacements : Dict[sympy.Symbol, Expr]
            precomputed_replacements : Dict[Expr, sympy.Symbol]
            replacements : Dict[sympy.Symbol, Expr]
            shape_env : NoneType
            simplify_with_ranges
            stride_vars
            var_to_val : dict
            atomically_apply_size_hint(expr: Union[Expr, int]) Union[Expr, int]
            combine_modular_indexing_pairs(index: sympy.Expr) sympy.Expr
            evaluate_expr(left: Union[Expr, sympy.logic.boolalg.Boolean]) bool
            evaluate_max(left: Expr, right: Expr) Expr
            evaluate_min(left: Expr, right: Expr) Expr
            evaluate_static_shape(left: Union[Expr, int]) int
            evaluate_static_shapes(left: Sequence[Union[Expr, int]]) List[int]
            expand_floor_div(index: sympy.Expr) Union[bool, Tuple[sympy.Expr, sympy.Expr]]
            free_symbols() OrderedSet[sympy.Symbol]
            guard_equals(left: Expr, right: Expr) Expr
            guard_leq(left: Expr, right: Expr) None
            guard_lt(left: Expr, right: Expr) None
            guarded_order(seq)
            is_expr_static_and_true(expr: Union[sympy.Basic, bool]) bool
            lookup_precomputed_size(expr: Expr) Expr
            make_simplify_loops_cache()
            make_simplify_with_ranges_cache() Callable[[Expr, VarRanges], Expr]
            make_stride_vars_cache()
            offset_var(index: Expr, vars: List[sympy.Symbol]) Expr
            remove_precomputed_replacements(expr: Expr) Expr
            simplify(expr: Expr)
            size_hint(expr: Union[Expr, int]) int
            size_hints(exprs: Iterable[Expr]) Tuple[int, ...]
            statically_known_equals(left: Union[Expr, int], right: Union[Expr, int]) bool
            statically_known_geq(left: Expr, right: Union[Expr, int]) bool
            statically_known_gt(left: Expr, right: Union[Expr, int]) bool
            statically_known_leq(left: Expr, right: Union[Expr, int]) bool
            statically_known_list_equals(left: List[Expr], right: List[Expr]) bool
            statically_known_lt(left: Expr, right: Union[Expr, int]) bool
            statically_known_multiple_of(numerator: Expr, denominator: Union[Expr, int]) bool
            statically_known_power_of_2(expr: Expr) bool
            stride_hints(index: Expr, vars: Sequence[sympy.Symbol], support_vars: Optional[Sequence[sympy.Symbol]]) List[int]
            stride_order(index: Expr, vars: List[sympy.Symbol]) List[int]
            symbolic_hint(expr: Union[Expr, int]) Union[Expr, int]
          }
          class SizeVariable {
            class_type
            has_grad_fn : bool
            proxy : Optional[torch.fx.Proxy]
            as_proxy()
            call_hasattr(tx: 'InstructionTranslator', name: str) 'VariableTracker'
            call_method(tx, name, args: List['VariableTracker'], kwargs: Dict[str, 'VariableTracker']) 'VariableTracker'
            debug_repr()
            get_item_dyn(tx: 'InstructionTranslator', arg: VariableTracker)
            numel(tx)
            python_type()
            reconstruct(codegen: 'PyCodegen') None
            unpack_var_sequence(tx)
          }
          class SkipCodeRecursiveException {
          }
          class SkipFrame {
          }
          class SkipFunctionVariable {
            reason : NoneType
            value
            as_python_constant()
            call_function(tx: 'InstructionTranslator', args: 'List[VariableTracker]', kwargs: 'Dict[str, VariableTracker]') 'VariableTracker'
            create_with_source(value, source)
            fold_through_function_to_wrapper()
          }
          class SkipModel {
            linear
            linear_skip
            nested_linear
            forward(x)
          }
          class SkipModule {
            lin
            forward(x)
          }
          class SkipQuantModel {
            fc
            sub
            forward(x)
            fuse_modules()
          }
          class SkipResult {
            reason : Optional[str]
            skipped : bool
          }
          class SkipRule {
            type
            get_context(test_case)
          }
          class SliceVariable {
            as_proxy()
            as_python_constant()
            debug_repr()
            python_type()
            reconstruct(codegen: 'PyCodegen') None
            var_getattr(tx: 'InstructionTranslator', name)
          }
          class SliceView {
            create(x, dim, start, end, step, clamp)
            normalize_start_end(x, dim, start, end)
          }
          class SliceViewInfo {
            dim : Union[int, torch.SymInt]
            end : Union[int, torch.SymInt]
            start : Union[int, torch.SymInt]
            regenerate_view(bases_list: List[Tensor])
          }
          class SlowPickleClass {
            t
          }
          class SmoothL1Loss {
            beta : float
            forward(input: Tensor, target: Tensor) Tensor
          }
          class SobolEngine {
            MAXBIT : int
            MAXDIM : int
            dimension
            num_generated : int
            quasi
            scramble : bool
            seed : NoneType
            shift
            sobolstate
            draw(n: int, out: Optional[torch.Tensor], dtype: Optional[torch.dtype]) torch.Tensor
            draw_base2(m: int, out: Optional[torch.Tensor], dtype: Optional[torch.dtype]) torch.Tensor
            fast_forward(n)
            reset()
          }
          class SoftMarginLoss {
            forward(input: Tensor, target: Tensor) Tensor
          }
          class Softmax {
            dim : NoneType
            scale : float
            zero_point : int
            forward(input)
            from_float(mod, use_precomputed_fake_quant)
            from_reference(mod, scale, zero_point)
          }
          class Softmax {
            dim : Optional[int]
            extra_repr() str
            forward(input: Tensor) Tensor
          }
          class Softmax2d {
            forward(input: Tensor) Tensor
          }
          class SoftmaxTransform {
            codomain
            domain
            forward_shape(shape)
            inverse_shape(shape)
          }
          class Softmin {
            dim : Optional[int]
            extra_repr()
            forward(input: Tensor) Tensor
          }
          class Softplus {
            beta : float
            threshold : float
            extra_repr() str
            forward(input: Tensor) Tensor
          }
          class SoftplusTransform {
            bijective : bool
            codomain
            domain
            sign : int
            log_abs_det_jacobian(x, y)
          }
          class Softshrink {
            lambd : float
            extra_repr() str
            forward(input: Tensor) Tensor
          }
          class Softsign {
            forward(input: Tensor) Tensor
          }
          class Sort {
            descending : bool
            dtypes : Tuple[torch.dtype, ...]
            inner_fns : Tuple[Callable[..., Any], ...]
            output_index : int
            reduction_hint
            reindex : Callable[[Sequence[Expr], Sequence[Expr]], Sequence[Expr]]
            size : List[Integer]
            sort_ranges : List[Integer]
            stable : bool
            create(device: torch.device, dtypes: Tuple[torch.dtype, ...], inner_fns: Tuple[Callable[[List[Expr]], Any], ...], size: List[Integer], axis: int, stable: bool, descending: bool, reduction_hint: ReductionHint) Sequence[Optional[TensorBox]]
            get_pointwise_size() Sequence[Expr]
            get_reduction_size() Sequence[Expr]
            get_reduction_type() Optional[str]
            get_size() Sequence[Expr]
            get_unbacked_symbol_uses() OrderedSet[Symbol]
            index_length() int
            inner_fn_args() Sequence[Sequence[Expr]]
            inner_fn_free_unbacked_symbols() OrderedSet[Symbol]
            store_reduction(output_name: Optional[str], indexer: Callable[[Sequence[Expr]], Expr], vars: Sequence[Expr], reduction_vars: Sequence[Expr]) OpsValue
          }
          class SortGenVmap {
            generate_vmap_rule : bool
            backward(ctx, grad_output, _0, _1)
            forward(x, dim)
            jvp(ctx, x_tangent, _)
            setup_context(ctx, inputs, outputs)
          }
          class Sortable {
          }
          class Source {
            guard_source()* GuardSource
            is_dict_key()
            is_ephemeral()
            is_specialized_nn_module() bool
            make_guard(fn) Guard
            name()* str
            reconstruct(codegen)*
            subguards_allowed()
          }
          class SourceChangeWarning {
          }
          class SourceContext {
            filename
            funcname : NoneType
            uses_true_division : bool
          }
          class SourceContext {
          }
          class SourceLoader {
            content : dict
            cache(fn, source)
            get_source(fn)
          }
          class SourcePartition {
            input_nodes : List[Node]
            nodes : List[Node]
            output_nodes : List[Node]
            params : List[Node]
            source : Any
          }
          class SourceType {
            name
          }
          class SourcelessBuilder {
            create(tx: 'InstructionTranslator', value) VariableTracker
            make_type_handlers()
            wrap_constant_literal(value)
          }
          class SourcelessGraphModuleVariable {
            call_method(tx, name, args: 'List[VariableTracker]', kwargs: 'Dict[str, VariableTracker]') 'VariableTracker'
          }
          class SourcelessUserDefinedObjectBuilder {
            create(tx: 'InstructionTranslator', value) VariableTracker
          }
          class SparseAdam {
            step(closure)
          }
          class SparseNNModel {
            dense_top
            model_sparse
            forward(sparse_indices: torch.Tensor, sparse_offsets: torch.Tensor, dense: torch.Tensor) torch.Tensor
          }
          class SparseSemiStructuredTensor {
            BACKEND : str
            SPARSE_DISPATCH : Dict[Callable, Callable]
            alg_id_cusparselt : int
            compressed_swizzled_bitmask : Optional[torch.Tensor]
            fuse_transpose_cusparselt : bool
            meta : Optional[torch.Tensor]
            meta_t : Optional[torch.Tensor]
            packed : Optional[torch.Tensor]
            packed_t : Optional[torch.Tensor]
            from_dense(original_tensor: torch.Tensor)* 'SparseSemiStructuredTensor'
            to_dense()
          }
          class SparseSemiStructuredTensorCUSPARSELT {
            BACKEND : str
            from_dense(original_tensor: torch.Tensor) 'SparseSemiStructuredTensorCUSPARSELT'
            prune_dense_static_sort(original_tensor: torch.Tensor, algorithm) 'SparseSemiStructuredTensor'
          }
          class SparseSemiStructuredTensorCUTLASS {
            BACKEND : str
            from_dense(original_tensor: torch.Tensor) 'SparseSemiStructuredTensorCUTLASS'
            prune_dense_static_sort(original_tensor: torch.Tensor, algorithm) 'SparseSemiStructuredTensor'
            to_dense()
          }
          class SparseTensor {
            indices
            values
            from_dense(t)
            get_wrapper_properties(size, values, indices, requires_grad)
            sparse_to_dense()
          }
          class SpatialSplit {
            left
            right
            create(left, extra_space)
            finalize(pool, offset)
            get_live_ranges()
            get_size_hint() int
            get_symbolic_size() sympy.Expr
          }
          class SpawnContext {
          }
          class SpawnHelper {
            setUp()
            tearDown()
          }
          class SpecViolationError {
          }
          class SpecialLocations {
            display_base : Optional[_artifact_location.ArtifactLocation]
            properties : Optional[_property_bag.PropertyBag]
          }
          class SpecializedAttribute {
            a : str
            b : int
            forward(x)
          }
          class SpectralFuncInfo {
            ndimensional
            ref : NoneType
          }
          class SpectralFuncPythonRefInfo {
            torch_opinfo
            torch_opinfo_name
          }
          class SpectralNorm {
            dim : int
            eps : float
            n_power_iterations : int
            name : str
            apply(module: Module, name: str, n_power_iterations: int, dim: int, eps: float) 'SpectralNorm'
            compute_weight(module: Module, do_power_iteration: bool) torch.Tensor
            remove(module: Module) None
            reshape_weight_to_matrix(weight: torch.Tensor) torch.Tensor
          }
          class SpectralNormLoadStateDictPreHook {
            fn
          }
          class SpectralNormStateDictHook {
            fn
          }
          class SpectralOpFuzzer {
          }
          class SpeculationEntry {
            failed : bool
            filename : str
            inst
            instruction_pointer : int
            lineno : int
            reason : Optional[GraphCompileReason]
            fail_and_restart_analysis()
          }
          class SpeculationLog {
            entries : List[SpeculationEntry]
            index : int
            clear()
            next(filename: str, lineno: int, instruction_pointer, inst) SpeculationEntry
            restart()
          }
          class SpeculationRestartAnalysis {
          }
          class Split {
            group_shape : Tuple
            input_dim
            split_id : int
            inputs() Iterable[DimSpec]
            new(dim: DimSpec, group_shape: Tuple[int, ...], idx: int) DimSpec
          }
          class SplitCatSimplifier {
            erase_old_nodes(graph: torch.fx.Graph, split_node: torch.fx.Node, next_users: List[torch.fx.Node])
            fill_gaps(ranges: List[_Range], min_: int, max_: int) List[_Range]
            get_merged_user_inputs(split_node: torch.fx.Node, cat_node: torch.fx.Node) List[Union[torch.fx.Node, _Range]]
            get_non_cat_node_input(split_node: torch.fx.Node, node: torch.fx.Node) List[_Range]
            get_simplified_split_ranges(split_sections, next_users, user_inputs_list: List[List[Union[torch.fx.Node, _Range]]]) Optional[List[_Range]]
            get_transform_params(split_node: torch.fx.Node, next_users: List[torch.fx.Node], user_inputs_list: List[List[Union[torch.fx.Node, _Range]]]) Optional[List[List[_TransformParam]]]
            get_user_input_list(split_node: torch.fx.Node, next_users: List[torch.fx.Node]) List[List[Union[torch.fx.Node, _Range]]]
            has_non_overlapping_ranges(ranges: List[_Range]) bool
            merge_consecutive_inputs(inputs: List[Union[torch.fx.Node, int]]) List[Union[torch.fx.Node, _Range]]
            replace_cat(graph: torch.fx.Graph, split_node: torch.fx.Node, next_users: List[torch.fx.Node], user_inputs_list_new, transform_params_list: List[List[_TransformParam]])
            replace_split(graph: torch.fx.Graph, split_node: torch.fx.Node, split_sections: List[int], user_inputs_list: List[List[Union[torch.fx.Node, _Range]]], split_ranges: List[_Range]) List[List[torch.fx.Node]]
            simplify(graph: torch.fx.Graph, split_node: torch.fx.Node, split_sections: List[int])
          }
          class SplitInputs {
            all_tensors : List[Any]
            arg_types : List[str]
            kwarg_order : List[str]
            kwarg_types : Dict[str, Any]
            nontensor_args : List[Any]
            nontensor_kwargs : Dict[str, Any]
            tensor_args : List[Any]
            tensor_kwargs : Dict[str, Any]
            nontensors_match(other: 'SplitInputs')
          }
          class SplitPoint {
            name
          }
          class SplitResult {
            non_acc_submodule_prefix : str
            split_module
            submodule_inputs : Dict[str, Any]
          }
          class SplitScan {
          }
          class SqueezeView {
            create(x)
            squeezer(size: Sequence[sympy.Expr])
          }
          class Stack {
            frames : list[StackFrame]
            message : str | None
            sarif() sarif.Stack
          }
          class Stack {
            frames : List[_stack_frame.StackFrame]
            message : Optional[_message.Message]
            properties : Optional[_property_bag.PropertyBag]
          }
          class StackDataset {
            datasets : Union[tuple, dict]
          }
          class StackFrame {
            location
            sarif() sarif.StackFrame
          }
          class StackFrame {
            location : Optional[_location.Location]
            module : Optional[str]
            parameters : Optional[List[str]]
            properties : Optional[_property_bag.PropertyBag]
            thread_id : Optional[int]
          }
          class StackSize {
            fixed_point
            high : Union[int, float]
            low : Union[int, float]
            exn_tab_jump(depth)
            offset_of(other, n)
            zero()
          }
          class StackTransform {
            bijective
            dim : int
            transforms : List[Transform]
            codomain()
            domain()
            log_abs_det_jacobian(x, y)
            with_cache(cache_size)
          }
          class StandaloneModuleConfigEntry {
            backend_config : Optional[BackendConfig]
            example_inputs : Tuple[Any, ...]
            prepare_custom_config : Optional[PrepareCustomConfig]
            qconfig_mapping : Optional[QConfigMapping]
          }
          class StandaloneModuleQuantizeHandler {
          }
          class StarDep {
            index
            mode : Optional[str]
            name : str
            get_numel() sympy.Expr
            has_unbacked_symbols()
            is_contiguous() bool
            is_indirect() bool
            is_scalar() bool
            numbytes_hint()
            rename(renames: Dict[str, str]) 'StarDep'
          }
          class StateDictConfig {
            offload_to_cpu : bool
          }
          class StateDictOptions {
            broadcast_from_rank0 : bool
            cpu_offload : bool
            flatten_optimizer_state_dict : bool
            full_state_dict : bool
            ignore_frozen_params : bool
            keep_submodule_prefixes : bool
            strict : bool
          }
          class StateDictSettings {
            optim_state_dict_config
            state_dict_config
            state_dict_type
          }
          class StateDictType {
            name
          }
          class StateInfo {
            non_tensors : Dict[str, Any]
            scalar_tensors : Dict[str, torch.Tensor]
            tensors : Dict[str, _PosDimTensorInfo]
          }
          class Stateful {
            load_state_dict(state_dict: Dict[str, Any]) None
            state_dict() Dict[str, Any]
          }
          class StatefulSymbolicContext {
            shape_env_to_source_to_symbol_cache : Optional[Dict[int, Dict[str, sympy.Expr]]]
            tensor_source : Optional[Source]
          }
          class StatelessSymbolicContext {
            constraint_sizes : Optional[DimList[DimConstraint]]
            constraint_strides : Optional[DimList[DimConstraint]]
            dynamic_sizes : DimList[DimDynamic]
            dynamic_strides : Optional[DimList[DimDynamic]]
            view_base_context : Optional[SymbolicContext]
          }
          class StaticForLoop {
            forward(x)
          }
          class StaticIf {
            forward(x)
          }
          class StaticModule {
            static_module
            benchmark(args, kwargs, warmup_runs, main_runs)
            benchmark_individual_ops(args, kwargs, warmup_runs, main_runs)
            runAsync(args, kwargs)
          }
          class StaticTCPRendezvous {
            master_addr : str
            master_port : int
            rank : int
            run_id : str
            timeout : timedelta
            use_agent_store
            world_size : int
            get_backend() str
            get_run_id() str
            is_closed()
            next_rendezvous() RendezvousInfo
            num_nodes_waiting()
            set_closed()*
            shutdown() bool
          }
          class Stats {
            num_get_hit : int
            num_get_miss : int
            num_put : int
            reset() None
          }
          class Std {
            name
            from_str(vm: str) Union['Std', Dict[int, 'Std']]
          }
          class StepLR {
            gamma : float
            step_size : int
            get_lr()
          }
          class StickBreakingTransform {
            bijective : bool
            codomain
            domain
            forward_shape(shape)
            inverse_shape(shape)
            log_abs_det_jacobian(x, y)
          }
          class StmtBuilder {
            augassign_map : dict
            build_AnnAssign(ctx, stmt)
            build_Assert(ctx, stmt)
            build_Assign(ctx, stmt)
            build_AugAssign(ctx, stmt)
            build_Break(ctx, stmt)
            build_Continue(ctx, stmt)
            build_Delete(ctx, stmt)
            build_Expr(ctx, stmt)
            build_For(ctx, stmt)
            build_If(ctx, stmt)
            build_Pass(ctx, stmt)
            build_Print(ctx, stmt)
            build_Raise(ctx, stmt)
            build_Return(ctx, stmt)
            build_While(ctx, stmt)
            build_With(ctx, stmt)
          }
          class Storage {
            device
            dtype
            cpu()* 'Storage'
            data_ptr()* int
            element_size()* int
            from_file(filename: str, shared: bool, nbytes: int)* 'Storage'
            is_shared()* bool
            nbytes()* int
            share_memory_()* 'Storage'
          }
          class StorageBox {
            data
            has_exceeded_max_reads() bool
            is_input_buffer()
            is_module_buffer()
            mark_reuse(users: int) None
            num_reads()
            realize() Optional[str]
            realize_hint() None
            should_realize_on_reuse(users)
          }
          class StorageMeta {
            checkpoint_id : Optional[Union[str, os.PathLike, None]]
            load_id : Optional[str]
            save_id : Optional[str]
          }
          class StorageOverlap {
            non_overlapping_sources : List[Source]
            overlapping_sources : List[Source]
          }
          class StorageReader {
            prepare_global_plan(plans: List[LoadPlan])* List[LoadPlan]
            prepare_local_plan(plan: LoadPlan)* LoadPlan
            read_data(plan: LoadPlan, planner: LoadPlanner)* Future[None]
            read_metadata()* Metadata
            reset(checkpoint_id: Union[str, os.PathLike, None])* None
            set_up_storage_reader(metadata: Metadata, is_coordinator: bool)* None
            validate_checkpoint_id(checkpoint_id: Union[str, os.PathLike])* bool
          }
          class StorageType {
            dtype
          }
          class StorageWeakRef {
            cdata : Any
            expired()
            from_weakref(cdata)
          }
          class StorageWeakRefWrapper {
            extra_ref_check : NoneType, Optional[Callable[[], bool]]
            ref
            storage_ref : Optional[StorageWeakRef]
            data_ptr() int
            expired() bool
            from_weakref_and_data_ptr(cdata: Any, data_ptr: int, extra_ref_check: Optional[Callable[[], bool]]) StorageWeakRefWrapper
            remove_extra_reference() None
            swap_weakref(cdata: Any) None
          }
          class StorageWriter {
            finish(metadata: Metadata, results: List[List[WriteResult]])* None
            prepare_global_plan(plans: List[SavePlan])* List[SavePlan]
            prepare_local_plan(plan: SavePlan)* SavePlan
            reset(checkpoint_id: Union[str, os.PathLike, None])* None
            set_up_storage_writer(is_coordinator: bool)* None
            storage_meta() Optional[StorageMeta]
            validate_checkpoint_id(checkpoint_id: Union[str, os.PathLike])* bool
            write_data(plan: SavePlan, planner: SavePlanner)* Future[List[WriteResult]]
          }
          class StoreOutputSubstitution {
            store(name: str, index: sympy.Expr, value: 'CSEVariable', mode: 'StoreMode')
          }
          class StrategyType {
          }
          class Stream {
            query() bool
            record_event(event)
            synchronize() None
            wait_event(event) None
            wait_stream(stream) None
          }
          class Stream {
          }
          class Stream {
            wait_stream(stream)* None
          }
          class Stream {
            query() bool
            record_event(event)
            synchronize() None
            wait_event(event) None
            wait_stream(stream) None
          }
          class StreamContext {
            cur_stream : Optional['torch.xpu.Stream']
            dst_prev_stream
            idx : NoneType, int
            src_prev_stream
            stream : Optional['torch.xpu.Stream']
          }
          class StreamContext {
            cur_stream : Optional[Stream]
            prev_stream
            stream
          }
          class StreamContext {
            cur_stream : Optional['torch.cuda.Stream']
            dst_prev_stream : NoneType
            idx : int
            src_prev_stream : NoneType
            stream : Optional['torch.cuda.Stream']
          }
          class StreamContext {
            cur_stream : Optional['torch.mtia.Stream']
            dst_prev_stream : NoneType
            idx : NoneType, int
            src_prev_stream : NoneType
            stream : Optional['torch.mtia.Stream']
          }
          class StreamContextVariable {
            device
            set_stream
            set_stream_id
            create(tx: 'InstructionTranslator', target_value)
            enter(tx)
            exit(tx: 'InstructionTranslator')
          }
          class StreamReaderIterDataPipe {
            chunk : NoneType
            datapipe
          }
          class StreamSynchronizations {
            current_sync_states : Dict[StreamId, Dict[StreamId, SeqNum]]
            host_sync_state : Dict[StreamId, SeqNum]
            recorded_sync_states : Dict[EventId, Dict[StreamId, SeqNum]]
            all_streams_wait_for_event(event: EventId) None
            all_streams_wait_for_stream(stream: StreamId) None
            create_event(event: EventId) None
            create_stream(stream: StreamId) None
            delete_event(event: EventId) None
            is_ordered_after(current_stream: StreamId, seq_num: SeqNum, other_stream: StreamId) bool
            record_state(event: EventId, stream: StreamId) None
            stream_wait_for_event(stream: StreamId, event: EventId) None
            sync_all_streams() None
            update_seq_num(stream: StreamId, seq_num: SeqNum) None
          }
          class StreamVariable {
            device
            proxy
            value
            as_proxy()
            call_method(tx, name, args: 'List[VariableTracker]', kwargs: 'Dict[str, VariableTracker]') 'VariableTracker'
            reconstruct(codegen)
          }
          class StreamWrapper {
            child_counter : int
            close_on_last_child : bool
            closed : bool
            debug_unclosed_streams : bool
            file_obj
            name : NoneType
            parent_stream : NoneType
            session_streams : Dict[Any, int]
            autoclose()
            close()
            close_streams(v, depth)
          }
          class StrictMinMaxConstraint {
            vr
            render(source: Source) str
          }
          class StrictMode {
          }
          class StrictModeHigherOrderVariable {
            call_function(tx: 'InstructionTranslator', args: 'List[VariableTracker]', kwargs: 'Dict[str, VariableTracker]') 'VariableTracker'
          }
          class StringFormatVariable {
            format_string
            sym_args
            sym_kwargs
            create(format_string, sym_args, sym_kwargs)
            reconstruct(codegen)
          }
          class StringPair {
            CLS : tuple
            TYPE_NAME : str
          }
          class StringTable {
          }
          class StrobelightCLIFunctionProfiler {
            current_run_id : NoneType, Optional[int]
            max_profile_duration_sec : int
            run_user_name : str
            sample_each : float
            sample_tags : NoneType
            stop_at_error : bool
            timeout_wait_for_finished_sec : int
            timeout_wait_for_running_sec : int
            profile(work_function: Any) Any
          }
          class StrobelightCLIFunctionProfiler {
            current_run_id : NoneType, Optional[int]
            max_profile_duration_sec : int
            profile_result : NoneType, Optional[List[str]], list
            run_user_name : str
            sample_each : float
            sample_tags : NoneType
            stop_at_error : bool
            timeout_wait_for_finished_sec : int
            timeout_wait_for_running_sec : int
            profile(work_function: Any) Any
          }
          class StrobelightCLIProfilerError {
          }
          class StrobelightCLIProfilerError {
          }
          class StrobelightCompileTimeProfiler {
            current_phase : Optional[str]
            enabled : bool
            failed_profile_count : int
            identifier : Optional[str]
            ignored_profile_runs : int
            inside_profile_compile_time : bool
            max_profile_time : int
            max_stack_length : int
            profiler : Optional[Any]
            sample_each : int
            success_profile_count : int
            enable(profiler_class: Any) None
            profile_compile_time(func: Any, phase_name: str) Any
          }
          class StubRpcAgent {
            world_size
            get_worker_infos()
          }
          class StudentT {
            arg_constraints : dict
            df
            has_rsample : bool
            loc
            mean
            mode
            scale
            support
            variance
            entropy()
            expand(batch_shape, _instance)
            log_prob(value)
            rsample(sample_shape: _size) torch.Tensor
          }
          class SubConfigProxy {
          }
          class SubModelForFusion {
            bn
            conv
            forward(x)
          }
          class SubModelWithoutFusion {
            conv
            qconfig : NoneType
            relu
            forward(x)
          }
          class SubModule {
            bn
            embedding_net
            lin
            lin_layer
            forward(x)
          }
          class SubclassAttrListSource {
            guard_source()
            name() str
          }
          class SubclassCreationMeta {
            arg_count : int
            attrs : Dict[str, Union['SubclassCreationMeta', PlainTensorMeta]]
            flat_tensor_start_idx : int
            included_subclass_symints : bool
            memory_format : Optional[torch.memory_format]
            meta : Any
            original_subclass : Optional[torch.Tensor]
            original_subclass_type : Optional[type]
            outer_size : Iterable[Union[None, int, torch.SymInt]]
            outer_stride : Iterable[Union[None, int, torch.SymInt]]
            subclass_type : list
            compute_outer_size_and_stride(all_args)
            creation_fn(all_args)
            make_runtime_safe()
          }
          class SubclassInfo {
            closed_under_ops : bool
            create_fn
            name
          }
          class SubclassMeta {
            fw_metadata
            grad_input_metas : Optional[List[Union[PlainTensorMeta, SubclassCreationMeta]]]
          }
          class SubclassSymbolicContext {
            inner_contexts : Optional[Dict[str, SymbolicContext]]
          }
          class SubclassWithTensorFactory {
            src
          }
          class Subgraph {
            device_ordinal : Optional[int]
            is_acc : bool
            nodes : List
          }
          class Subgraph {
            graph : Optional[GraphLowering]
            graph_module
            name : str
          }
          class SubgraphInfo {
            body
            compute
            indexing_code
            loads
            numels : NoneType
            only_copy_if_non_none_fields : tuple
            ops_handler : Optional[V.WrapperHandler]
            range_trees : Optional[List['IterationRangesRoot']]
            stores
            template_mask : Optional[str]
            template_out : Optional[str]
            to_dict()
          }
          class SubgraphLowering {
            parent
            init_wrapper_code(is_subgraph: bool, subgraph_name: Optional[str], parent_wrapper_code: Optional[PythonWrapperCodegen]) None
          }
          class SubgraphLoweringException {
          }
          class SubgraphMatcher {
            ignore_literals : bool
            match_output : bool
            match_placeholder : bool
            pattern
            pattern_anchors : List[Node], list
            pattern_placeholder_nodes
            pattern_returning_nodes : List[Node]
            remove_overlapping_matches : bool
            match(graph: Graph) List[InternalMatch]
          }
          class SubgraphMatcherWithNameNodeMap {
            name_node_map : dict
            match(graph: Graph) List[InternalMatch]
          }
          class SubgraphPythonWrapperCodegen {
            launcher_fn_name
            parent_wrapper
            subgraph_name
            add_benchmark_harness(output)*
            benchmark_compiled_module(output)*
            next_kernel_suffix() str
            set_launcher_fn_name() None
            write_async_compile_wait()*
            write_get_raw_stream_header_once() None
            write_header()* None
            write_triton_header_once() None
          }
          class SubgraphTracer {
            allow_side_effects_under_checkpoint : bool
            bound_symbols : Dict[sympy.Symbol, Union[torch.fx.Proxy, LazyProxy]]
            debug_level : int
            graph
            input_name_to_proxy : Dict[str, fx.Proxy]
            is_export : bool
            lifted_freevars : dict
            output_graph
            parent : NoneType
            prev_inst : NoneType
            real_value_cache : Dict[fx.Node, torch.Tensor]
            source_fn_stack : list
            under_activation_checkpoint : bool
            create_graph_input(name, type_expr, example_value, before, source)
            create_node(op, target, args, kwargs, name, type_expr)
            create_proxy(kind, target, args, kwargs, name, type_expr, proxy_factory_fn)
            lift_tracked_freevar_to_input(proxy)
            lookup_unbound_symbols(s: torch.SymInt) List[sympy.Symbol]
            maybe_lift_tracked_freevar_to_input(arg)
            remove_node(node)
            track_unbacked_symbols(example_value, e_proxy: Union[LazyProxy, torch.fx.Proxy])
          }
          class SubgraphTypeRelationship {
            name
          }
          class SubmodCompiler {
            compiler
            fake_mode
            compile_submod(input_mod, args, kwargs)
            run_node(n: Node) Any
          }
          class SubprocException {
          }
          class SubprocMain {
            nprocs : int
            pool : ProcessPoolExecutor
            read_pipe : BinaryIO
            running : bool
            write_lock : lock
            write_pipe : BinaryIO
            do_job(data: bytes) bytes
            main() None
            submit(job_id: int, data: bytes) None
          }
          class SubprocPool {
            futures_lock : lock
            job_id_count : count
            pending_futures : Dict[int, Future[Any]]
            process : Popen
            read_pipe
            read_thread : Thread
            ready_future : Future
            running : bool
            write_lock : lock
            write_pipe
            shutdown() None
            submit(job_fn: Callable[_P, _T]) Future[_T]
          }
          class SubprocessContext {
            subprocess_handlers : Dict[int, SubprocessHandler]
            pids() Dict[int, int]
          }
          class SubprocessHandler {
            local_rank_id : int
            proc : Popen
            close(death_sig: Optional[signal.Signals]) None
          }
          class Subset {
            dataset : Dataset[_T_co]
            indices : Sequence[int]
          }
          class SubsetRandomSampler {
            generator : NoneType
            indices : Sequence[int]
          }
          class Subsystem {
            name : str
          }
          class SumLikeReductionTypePromotionRule {
            preview_type_promotion(args: tuple, kwargs: dict) TypePromotionSnapshot
          }
          class SummaryWriter {
            all_writers : NoneType, dict
            default_bins : list
            file_writer : NoneType
            filename_suffix : str
            flush_secs : int
            log_dir : NoneType
            max_queue : int
            purge_step : NoneType
            add_audio(tag, snd_tensor, global_step, sample_rate, walltime)
            add_custom_scalars(layout)
            add_custom_scalars_marginchart(tags, category, title)
            add_custom_scalars_multilinechart(tags, category, title)
            add_embedding(mat, metadata, label_img, global_step, tag, metadata_header)
            add_figure(tag: str, figure: Union['Figure', List['Figure']], global_step: Optional[int], close: bool, walltime: Optional[float]) None
            add_graph(model, input_to_model, verbose, use_strict_trace)
            add_histogram(tag, values, global_step, bins, walltime, max_bins)
            add_histogram_raw(tag, min, max, num, sum, sum_squares, bucket_limits, bucket_counts, global_step, walltime)
            add_hparams(hparam_dict, metric_dict, hparam_domain_discrete, run_name, global_step)
            add_image(tag, img_tensor, global_step, walltime, dataformats)
            add_image_with_boxes(tag, img_tensor, box_tensor, global_step, walltime, rescale, dataformats, labels)
            add_images(tag, img_tensor, global_step, walltime, dataformats)
            add_mesh(tag, vertices, colors, faces, config_dict, global_step, walltime)
            add_onnx_graph(prototxt)
            add_pr_curve(tag, labels, predictions, global_step, num_thresholds, weights, walltime)
            add_pr_curve_raw(tag, true_positive_counts, false_positive_counts, true_negative_counts, false_negative_counts, precision, recall, global_step, num_thresholds, weights, walltime)
            add_scalar(tag, scalar_value, global_step, walltime, new_style, double_precision)
            add_scalars(main_tag, tag_scalar_dict, global_step, walltime)
            add_tensor(tag, tensor, global_step, walltime)
            add_text(tag, text_string, global_step, walltime)
            add_video(tag, vid_tensor, global_step, fps, walltime)
            close()
            flush()
            get_logdir()
          }
          class SuperVariable {
            objvar : NoneType
            typevar
            call_method(tx, name, args: 'List[VariableTracker]', kwargs: 'Dict[str, VariableTracker]') 'VariableTracker'
            reconstruct(codegen)
            var_getattr(tx: 'InstructionTranslator', name: str) 'VariableTracker'
          }
          class SupportLevel {
            name
          }
          class Suppression {
            guid : Optional[str]
            justification : Optional[str]
            kind : Literal['inSource', 'external']
            location : Optional[_location.Location]
            properties : Optional[_property_bag.PropertyBag]
            state : Optional[Literal['accepted', 'underReview', 'rejected']]
          }
          class SwapTensorsGuard {
            swap_tensors_restore : bool
            use_swap_tensors
          }
          class SymBool {
            node
          }
          class SymBool {
            as_bool : Annotated[bool, 20]
            as_expr : Annotated[SymExpr, 10]
          }
          class SymBoolArgument {
            as_bool : Annotated[bool, 20]
            as_name : Annotated[str, 10]
          }
          class SymBoolArgument {
            name : str
          }
          class SymExpr {
            expr_str : Annotated[str, 10]
            hint : Optional[Annotated[Optional[SymExprHint], 20]]
          }
          class SymExprHint {
            as_bool : Annotated[bool, 20]
            as_float : Annotated[float, 30]
            as_int : Annotated[int, 10]
          }
          class SymExprPrinter {
          }
          class SymFloat {
            node
            as_integer_ratio() _Tuple[builtins.int, builtins.int]
            conjugate() 'SymFloat'
            hex() str
            is_integer()
          }
          class SymFloat {
            as_expr : Annotated[SymExpr, 10]
            as_float : Annotated[float, 20]
          }
          class SymFloatArgument {
            as_float : Annotated[float, 20]
            as_name : Annotated[str, 10]
          }
          class SymFloatArgument {
            name : str
          }
          class SymInt {
            node
            as_integer_ratio() _Tuple['SymInt', builtins.int]
            bit_length() builtins.int
            conjugate() 'SymInt'
          }
          class SymInt {
            as_expr : Annotated[SymExpr, 10]
            as_int : Annotated[int, 20]
          }
          class SymIntArgument {
            as_int : Annotated[int, 20]
            as_name : Annotated[str, 10]
          }
          class SymIntArgument {
            name : str
          }
          class SymIntEqByExpr {
            val : Union[torch.SymInt, int]
          }
          class SymNode {
            constant : Optional[Union[int, float, bool]]
            expr
            fx_node
            hint
            pytype
            shape_env
            abs() SymNode
            add(other) SymNode
            and_(other) SymNode
            bitwise_and(other)
            bitwise_or(other)
            bool_()
            ceil() SymNode
            clone()
            eq(other) SymNode
            expect_size(file, line)
            expect_true(file, line)
            float_pow(other) SymNode
            float_truediv(other) SymNode
            floor() SymNode
            floordiv(other) SymNode
            ge(other) SymNode
            gt(other) SymNode
            guard_bool(file, line)
            guard_float(file, line)
            guard_int(file, line)
            guard_size_oblivious(file, line)
            has_hint()
            int_()
            int_floordiv(other) SymNode
            int_truediv(other) SymNode
            is_bool()
            is_channels_last_contiguous_2d(sizes, strides) SymNode
            is_channels_last_contiguous_3d(sizes, strides) SymNode
            is_channels_last_strides_2d(sizes, strides) SymNode
            is_channels_last_strides_3d(sizes, strides) SymNode
            is_constant()
            is_contiguous(sizes, strides) SymNode
            is_float()
            is_int()
            is_integer() SymNode
            is_nested_int()
            is_non_overlapping_and_dense(sizes, strides)
            is_non_overlapping_and_dense_indicator(sizes, strides) SymNode
            is_symbolic()
            le(other) SymNode
            lshift(other) SymNode
            lt(other) SymNode
            maybe_as_bool()
            maybe_as_float()
            maybe_as_int()
            mod(other) SymNode
            mul(other) SymNode
            ne(other) SymNode
            neg() SymNode
            nested_int()
            or_(other) SymNode
            pos() SymNode
            pow(other)
            pow_by_natural(other) SymNode
            require_hint(fallback)
            round(ndigits) SymNode
            rshift(other) SymNode
            str()
            sub(other) SymNode
            sym_and(other)
            sym_float() SymNode
            sym_int() SymNode
            sym_ite(then_val, else_val) SymNode
            sym_max(other) SymNode
            sym_min(other) SymNode
            sym_not() SymNode
            sym_or(other)
            sym_sum(args) SymNode
            truediv(other)
            trunc() SymNode
            with_shape_env(shape_env: ShapeEnv) SymNode
            wrap_bool(num)
            wrap_float(num)
            wrap_int(num)
          }
          class SymNodeVariable {
            proxy
            sym_num
            as_proxy()
            as_tensor(tx, dtype)
            call_method(tx, name, args: 'List[VariableTracker]', kwargs: 'Dict[str, VariableTracker]') 'VariableTracker'
            create(tx, proxy, sym_num)
            debug_repr()
            evaluate_expr(output_graph)
            python_type()
          }
          class SymNumberMemoDescriptor {
          }
          class SymPyOps {
            abs(x: TypedExpr) TypedExpr
            add(x: TypedExpr, y: TypedExpr) TypedExpr
            constant(value: Union[int, float, bool], dtype: torch.dtype) TypedExpr
            floordiv(x: TypedExpr, y: TypedExpr) TypedExpr
            identity(value: Any) Any
            index_expr(value: Union[sympy.Expr, int], dtype: torch.dtype) TypedExpr
            maximum(x: TypedExpr, y: TypedExpr) TypedExpr
            minimum(x: TypedExpr, y: TypedExpr) TypedExpr
            mod(x: TypedExpr, y: TypedExpr) Optional[TypedExpr]
            mul(x: TypedExpr, y: TypedExpr) TypedExpr
            neg(x: TypedExpr) TypedExpr
            remainder(x: TypedExpr, y: TypedExpr) Optional[TypedExpr]
            square(x: TypedExpr) TypedExpr
            sub(x: TypedExpr, y: TypedExpr) TypedExpr
            to_dtype(value: TypedExpr, dtype: torch.dtype, src_dtype: Optional[torch.dtype], use_compute_types: bool) TypedExpr
          }
          class SymPyValueRangeAnalysis {
            abs(x)
            acos(x)
            add(a, b)
            and_(a, b)
            asin(x)
            atan(x)
            bitwise_and(a, b)
            bitwise_or(a, b)
            ceil(x)
            ceil_to_int(x, dtype)
            constant(value, dtype)
            cos(x)
            cosh(x)
            eq(a, b)
            exp(x)
            expr_cond_pair(a, b)
            floor(x)
            floor_to_int(x, dtype)
            floordiv(a, b)
            ge(a, b)
            gt(a, b)
            identity(a)
            int_truediv(a, b)
            is_non_overlapping_and_dense_indicator()
            le(a, b)
            log(x)
            log2(x)
            lt(a, b)
            maximum(a, b)
            min_or_max(a, b, fn)
            minimum(a, b)
            mod(x, y)
            modular_indexing(a, b, c)
            mul(a, b)
            ne(a, b)
            not_(a)
            or_(a, b)
            piecewise()
            pow(a, b)
            pow_by_natural(a, b)
            reciprocal(x)
            round_decimal(number, ndigits)
            round_to_int(number, dtype)
            sin(x)
            sinh(x)
            sqrt(x)
            tan(x)
            tanh(x)
            to_dtype(a, dtype, src_dtype)
            truediv(a, b)
            trunc(x)
            trunc_to_int(a, dtype)
            where(a, b, c)
          }
          class SymT {
            name
          }
          class SymbolicCallArg {
            inner : str
            inner_expr : Expr
          }
          class SymbolicContext {
          }
          class SymbolicRegistry {
            all_functions() Set[str]
            get_function_group(name: str) Optional[_SymbolicFunctionGroup]
            is_registered_op(name: str, version: int) bool
            register(name: str, opset: OpsetVersion, func: Callable, custom: bool) None
            unregister(name: str, opset: OpsetVersion) None
          }
          class SymbolicTensor {
            rank
          }
          class SymbolicTorchFunctionState {
            cur_mode : NoneType
            mode_stack : Deque[TorchFunctionModeVariable]
            torch_function_mode_enabled
            torch_function_subclass_enabled
            call_torch_function_mode(tx, fn, types, args, kwargs)
            in_torch_function_mode()
            pop_torch_function_mode()
            push_torch_function_mode(mode_var)
          }
          class SymbolicValueError {
          }
          class SympyToZ3 {
            OPERATOR_HANDLES : set
            ceil_to_int(x: z3.ArithRef, dtype: torch.dtype) z3.ArithRef
            constant(value: Any, dtype: torch.dtype) z3.ExprRef
            div(numerator: z3.ArithRef, denominator: z3.ArithRef) z3.ArithRef
            floor_to_int(x: z3.ArithRef, dtype: torch.dtype) z3.ArithRef
            floordiv(numerator: z3.ArithRef, denominator: z3.ArithRef) z3.ArithRef
            int_truediv(numerator: z3.ArithRef, denominator: z3.ArithRef) z3.ArithRef
            mod(p: z3.ArithRef, q: z3.ArithRef) z3.ArithRef
            pow(base: z3.ArithRef, exp: z3.ArithRef) z3.ArithRef
            pow_by_natural(base: z3.ArithRef, exp: z3.ArithRef) z3.ArithRef
            round_to_int(x: z3.ArithRef, dtype: torch.dtype) z3.ArithRef
            run(expr: sympy.Basic) z3.ExprRef
            to_dtype(x: z3.ArithRef, dtype: torch.dtype) z3.ArithRef
            truediv(numerator: z3.ArithRef, denominator: z3.ArithRef) z3.ArithRef
            trunc_to_int(x: z3.ArithRef, dtype: torch.dtype) z3.ArithRef
          }
          class SyncBatchNorm {
            bias
            num_batches_tracked
            process_group : Optional[Any]
            qconfig
            running_mean
            running_var
            training
            weight
            convert_sync_batchnorm(module, process_group)
            forward(input: Tensor) Tensor
          }
          class SyncBatchNorm {
            backward(self, grad_output)
            forward(self, input, weight, bias, running_mean, running_var, eps, momentum, process_group, world_size)
          }
          class SyncPayload {
            exception : Optional[Exception]
            payload : T
            stage_name : Optional[str]
            success : bool
          }
          class SynchronizationError {
          }
          class SynchronizedDataLoaderPattern {
            description : str
            name : str
            url : str
            match(event: _ProfilerEvent)
          }
          class SyntheticLocalSource {
            local_name : str
            guard_source()
            name()
            reconstruct(codegen)
          }
          class T {
          }
          class TGreatestUpperBound {
            res
            rhs1
            rhs2
          }
          class TMADescriptor {
            block_dims : List[Union[int, torch.SymInt]]
            dims : List[Union[int, torch.SymInt]]
            element_size : Optional[int]
            name
            rank
            tensor
            codegen(wrapper) None
            create(tensor: IRNode, dims: List[Union[int, torch.SymInt]], block_dims: List[Union[int, torch.SymInt]], element_size: Optional[int])
          }
          class TMADescriptorArg {
            name : str
          }
          class TMADescriptorVariable {
            block_dims : str
            data_ptr : str
            dims : str
            element_size : str
            reconstruct(codegen)
            to_metadata()
          }
          class TODO_UNKNOWN {
          }
          class TS2EPConverter {
            name_to_buffer : Dict[str, torch.Tensor]
            name_to_constant : Dict[str, Any]
            name_to_non_tensor_attributes : Dict[str, Any]
            name_to_param : Dict[str, torch.Tensor]
            params : list
            sample_args : Tuple[Any, ...]
            sample_kwargs : Optional[Dict[str, Any]]
            ts_graph
            ts_model : Union[torch.jit.ScriptModule, torch.jit.ScriptFunction]
            convert() ExportedProgram
            explain(print_output)
            lift_get_attr()
            retrace_as_exported_program(gm: torch.fx.GraphModule, name_to_constant: Dict[str, Any])
          }
          class TS2FXGraphConverter {
            blocks_to_lifted_attrs : Dict[torch._C.Block, Set[str]]
            fx_graph
            input_specs : List[InputSpec]
            name_to_attribute_fqn : Dict[str, str]
            name_to_buffer : Dict[str, torch.Tensor]
            name_to_constant : Dict[str, Any]
            name_to_node : Dict[str, Union[torch.fx.Node, List[torch.fx.Node], Dict[Any, torch.fx.Node]]]
            name_to_non_tensor_attribute : Dict[str, Any]
            name_to_non_tensor_attribute_node : Dict[str, Any]
            name_to_param : Dict[str, torch.Tensor]
            name_update_from_subblock_to_parent : Set[str]
            output_specs : List[OutputSpec]
            subgraphs : Dict[str, torch.fx.GraphModule]
            ts_graph : Union[torch._C.Graph, torch._C.Block]
            add_subgraph(subgraph) str
            convert() torch.fx.GraphModule
            convert_aten_Bool(node: torch._C.Node)
            convert_aten_Float(node: torch._C.Node)
            convert_aten_Int(node: torch._C.Node)
            convert_aten___getitem__(node: torch._C.Node)
            convert_aten__convolution(node: torch._C.Node)
            convert_aten_add(node: torch._C.Node)
            convert_aten_append(node: torch._C.Node)
            convert_aten_div(node: torch._C.Node)
            convert_aten_tensor(node: torch._C.Node)
            convert_aten_to(node: torch._C.Node)
            convert_call_function_op(node: torch._C.Node)
            convert_graph_inputs()
            convert_graph_outputs()
            convert_node(node: torch._C.Node)
            convert_prim_CallMethod(node: torch._C.Node)
            convert_prim_Constant(node: torch._C.Node)
            convert_prim_CreateObject(node: torch._C.Node)
            convert_prim_DictConstruct(node: torch._C.Node)
            convert_prim_Enter(node: torch._C.Node)
            convert_prim_Exit(node: torch._C.Node)
            convert_prim_GetAttr(node: torch._C.Node)
            convert_prim_If(node: torch._C.Node)
            convert_prim_ListConstruct(node: torch._C.Node)
            convert_prim_ListUnpack(node: torch._C.Node)
            convert_prim_Loop(node: torch._C.Node)
            convert_prim_NumToTensor(node: torch._C.Node)
            convert_prim_SetAttr(node: torch._C.Node)
            convert_prim_TupleConstruct(node: torch._C.Node)
            convert_prim_TupleUnpack(node: torch._C.Node)
            convert_prim_Uninitialized(node: torch._C.Node)
            convert_prim_device(node: torch._C.Node)
            convert_prim_tolist(node: torch._C.Node)
            convert_profiler__record_function_exit(node: torch._C.Node)
            get_args_kwargs(node: torch._C.Node, schema)
            get_fx_value_by_fqn(name)
            get_fx_value_by_ir_value(value: torch._C.Value)
            is_top_level_graph()
          }
          class TVar {
            tvar
          }
          class Table {
            column_keys : list
            columns : tuple
            label
            results : List[common.Measurement]
            row_keys : list
            rows : tuple
            time_scale
            time_unit
            col_fn(m: common.Measurement) Optional[str]
            populate_rows_and_columns() Tuple[Tuple[_Row, ...], Tuple[_Column, ...]]
            render() str
            row_fn(m: common.Measurement) Tuple[int, Optional[str], str]
          }
          class Tag {
            name
          }
          class TagActivationCheckpoint {
            divide_kwargs(kwargs)
            tag_nodes(gmod, is_sac)
          }
          class TailLog {
            start() 'TailLog'
            stop() None
            stopped() bool
          }
          class TakeGenVmap {
            generate_vmap_rule : bool
            backward(ctx, grad_output)
            forward(x, ind, ind_inv, dim)
            jvp(ctx, x_tangent, ind_tangent, ind_inv_tangent, _)
            setup_context(ctx, inputs, outputs)
          }
          class Tanh {
            forward(input: Tensor) Tensor
          }
          class TanhTransform {
            bijective : bool
            codomain
            domain
            sign : int
            log_abs_det_jacobian(x, y)
          }
          class Tanhshrink {
            forward(input: Tensor) Tensor
          }
          class Task {
            p
            forward(x)
          }
          class TaskSpec {
            description : Optional[str]
            env : Optional[str]
            global_setup : str
            label : Optional[str]
            num_threads : int
            setup : str
            stmt : str
            sub_label : Optional[str]
            title
            setup_str() str
            summarize() str
          }
          class TemplateBuffer {
            inputs : list
            make_kernel_render : Callable[..., Any]
            name
            extract_read_writes(normalize)
            get_read_writes() dependencies.ReadWrites
            get_reduction_size() Sequence[sympy.Expr]
            get_reduction_type() Optional[str]
            should_allocate() bool
            simplify_and_reorder(extra_indexing_constraints: Optional[Tuple[Dict[Any, Any], List[Any]]], recompute_sizes_body_func: Optional[Callable[..., Any]])
          }
          class TemporalSplit {
            allocations : List[AllocationTreeNode]
            finalize(pool, offset)
            get_live_ranges() LiveRanges
            get_size_hint() int
            get_symbolic_size() sympy.Expr
            is_empty()
          }
          class Tensor {
            data
            detach
            detach_
            imag
            real
            requires_grad
            align_to()
            backward(gradient, retain_graph, create_graph, inputs)
            dim_order()
            eig(eigenvectors)
            is_shared()
            istft(n_fft: int, hop_length: Optional[int], win_length: Optional[int], window: 'Optional[Tensor]', center: bool, normalized: bool, onesided: Optional[bool], length: Optional[int], return_complex: bool)
            lstsq(other)
            lu(pivot, get_infos)
            module_load(other, assign)
            norm(p: Optional[Union[float, str]], dim, keepdim, dtype)
            refine_names()
            register_hook(hook)
            register_post_accumulate_grad_hook(hook)
            reinforce(reward)
            rename()
            rename_()
            resize()
            resize_as(tensor)
            share_memory_()
            solve(other)
            split(split_size, dim)
            stft(n_fft: int, hop_length: Optional[int], win_length: Optional[int], window: 'Optional[Tensor]', center: bool, pad_mode: str, normalized: bool, onesided: Optional[bool], return_complex: Optional[bool])
            storage()
            storage_type()
            symeig(eigenvectors)
            to_sparse_coo()
            unflatten(dim, sizes)
            unique(sorted, return_inverse, return_counts, dim)
            unique_consecutive(return_inverse, return_counts, dim)
          }
          class TensorAlias {
            alias
          }
          class TensorArg {
            alias_of : Optional[str]
            buffer : str
            dtype
            name : str
            offset : Expr
          }
          class TensorArgument {
            name : Annotated[str, 10]
          }
          class TensorArgument {
            name : str
          }
          class TensorAsKey {
            key : tuple
            obj
          }
          class TensorBox {
            create(data)
          }
          class TensorChunkSpec {
            split_dim : int
            from_dict(chunk_dims: Dict[str, int])
            from_tuple(chunk_dims: Tuple[int, ...])
          }
          class TensorContainer {
          }
          class TensorDataset {
            tensors : Tuple[Tensor, ...]
          }
          class TensorExprTestOptions {
            old_cpu_fuser_state
            old_fusion_inlining
            old_gpu_fuser_state
            old_profiling_executor
            old_profiling_mode
            old_te_must_use_llvm_cpu
            texpr_fuser_state
            restore()
          }
          class TensorInfo {
            allocation_stack_trace : Optional[traceback.StackSummary]
            reads : List[Access]
            write : Optional[Access]
          }
          class TensorKey {
            id : int
            storage
            from_allocation(alloc: _ExtraFields_Allocation) Optional['TensorKey']
            from_tensor(t: Optional[_TensorMetadata]) Optional['TensorKey']
          }
          class TensorLike {
            dtype
          }
          class TensorLikePair {
            atol
            check_device : bool
            check_dtype : bool
            check_layout : bool
            check_stride : bool
            equal_nan : bool
            rtol
            compare() None
            extra_repr() Sequence[str]
          }
          class TensorMeta {
            dtype
            shape
            stride : Tuple[int, ...]
          }
          class TensorMeta {
            device
            dtype
            name : Optional[str]
            offset : int
            sizes : Union
            strides : Union
            from_irnodes(irnodes: Union[LayoutOrBuffer, Sequence[LayoutOrBuffer]]) Union[TensorMeta, List[TensorMeta]]
            to_tensor() torch.Tensor
          }
          class TensorMeta {
            device : Annotated[Device, 40]
            dtype : Annotated[ScalarType, 10]
            layout : Annotated[Layout, 70]
            requires_grad : Annotated[bool, 30]
            sizes : Annotated[List[SymInt], 20]
            storage_offset : Annotated[SymInt, 60]
            strides : Annotated[List[SymInt], 50]
          }
          class TensorMetadata {
            dtype
            is_quantized : bool
            memory_format : Optional[torch.memory_format]
            qparams : Dict[str, Any]
            requires_grad : bool
            shape
            stride : Tuple[int, ...]
          }
          class TensorMetadata {
            dense_dim : Optional[int]
            device
            dtype
            is_coalesced : Optional[bool]
            is_conj : bool
            is_inference : bool
            is_neg : bool
            is_quantized : bool
            is_sparse : bool
            layout
            memory_format : Optional[torch.memory_format]
            requires_grad : bool
            shape : Tuple[_MetadataIntLike, ...]
            sparse_dim : Optional[int]
            storage_bytes : Optional[_MetadataIntLike]
            storage_offset : Union
            stride : Tuple[_MetadataIntLike, ...]
          }
          class TensorMetadataAndValues {
            tensor_metadata
            values : List[Any]
          }
          class TensorMetadataHolder {
            device
            tensor_metadata : TensorMetadata
          }
          class TensorOrArrayPair {
            atol
            rtol
          }
          class TensorPipeAgentCudaRpcTest {
            test_async_execution_nested_with_cuda_future()
            test_async_execution_with_cuda_future()
            test_cuda_future_callback_changes_devices()
            test_cuda_future_can_extract_cuda_sparse_tensor()
            test_cuda_future_can_extract_cuda_tensor()
            test_cuda_future_can_extract_custom_class_with_cuda_sparse_tensor()
            test_cuda_future_can_extract_custom_class_with_cuda_tensor()
            test_cuda_future_can_extract_list_with_cuda_sparse_tensor()
            test_cuda_future_can_extract_list_with_cuda_tensor()
            test_cuda_future_device_as_device()
            test_cuda_future_device_as_int()
            test_cuda_future_device_as_str()
            test_cuda_future_device_not_cuda()
            test_cuda_future_modify_tensor_inplace()
            test_cuda_future_replace_tensor()
            test_cuda_future_value_on_bad_device()
            test_custom_stream()
            test_custom_stream_multi()
            test_custom_stream_nested()
            test_custom_stream_nested_multi()
            test_device_map_cpu()
            test_device_map_cpu_to_gpu_default()
            test_device_map_cpu_to_gpu_non_default()
            test_device_map_gpu_default()
            test_device_map_gpu_default_to_non_default()
            test_device_map_gpu_mixed_1()
            test_device_map_gpu_mixed_2()
            test_device_map_gpu_mixed_3()
            test_device_map_gpu_mixed_4()
            test_device_map_gpu_mixed_5()
            test_device_map_gpu_mixed_6()
            test_device_map_gpu_mixed_7()
            test_device_map_gpu_mixed_8()
            test_device_map_gpu_mixed_self_1()
            test_device_map_gpu_mixed_self_2()
            test_device_map_gpu_mixed_self_3()
            test_device_map_gpu_mixed_self_4()
            test_device_map_gpu_mixed_self_5()
            test_device_map_gpu_mixed_self_6()
            test_device_map_gpu_mixed_self_7()
            test_device_map_gpu_mixed_self_8()
            test_device_map_gpu_non_default()
            test_device_map_gpu_non_default_to_default()
            test_device_map_gpu_to_cpu_default()
            test_device_map_gpu_to_cpu_non_default()
            test_device_maps_gpu()
            test_device_maps_in_options()
            test_device_maps_invalid_max_local_device()
            test_device_maps_invalid_max_remote_device()
            test_device_maps_invalid_min_device()
            test_device_maps_many_to_one()
            test_device_maps_missing_config()
            test_device_maps_missing_config_loop()
            test_device_maps_missing_config_not_timeout()
            test_device_maps_missing_config_remote()
            test_device_maps_missing_config_remote_response()
            test_device_maps_missing_config_response()
            test_device_maps_missing_config_response_loop()
            test_device_maps_multi_gpu()
            test_device_maps_multi_gpu_self()
            test_device_maps_one_to_many()
            test_device_maps_remote()
            test_device_maps_return_to_gpu()
            test_device_maps_return_to_gpu_self()
            test_device_maps_wrong_worker_name()
            test_device_mismatch()
            test_devices_option_mismatch()
            test_devices_option_mismatch_reverse()
            test_owner_rref_forward_synchronization1()
            test_owner_rref_forward_synchronization2()
            test_owner_rref_forward_synchronization3()
            test_owner_rref_forward_synchronization4()
            test_rref_as_arg_synchronization1()
            test_rref_as_arg_synchronization2()
            test_rref_as_arg_synchronization3()
            test_rref_as_arg_synchronization4()
            test_rref_as_arg_synchronization5()
            test_rref_forward_synchronization1()
            test_rref_forward_synchronization2()
            test_rref_forward_synchronization3()
            test_rref_forward_synchronization4()
            test_rref_to_here_synchronization1()
            test_rref_to_here_synchronization2()
            test_rref_to_here_synchronization3()
            test_rref_to_here_synchronization4()
            test_rref_with_unpickleable_attributes()
            test_tensor_view_as_return_value()
          }
          class TensorPipeAgentDistAutogradTest {
            test_backward_different_dtypes_sparse()
            test_backward_multiple_round_trips_sparse()
            test_backward_no_grad_on_tensor_sparse()
            test_backward_rref_multi_sparse()
            test_backward_rref_nested_sparse()
            test_backward_rref_sparse()
            test_backward_simple_python_udf_sparse()
            test_backward_simple_script_call_sparse()
            test_backward_simple_self_sparse()
            test_backward_simple_sparse()
            test_backwards_nested_python_udf_sparse()
            test_context_cleanup_nested_rpc_sparse()
            test_context_cleanup_tensor_no_grad_sparse()
            test_context_cleanup_tensor_with_grad_sparse()
            test_embedding_bag_with_no_grad_tensors()
            test_graph_for_builtin_call_sparse()
            test_graph_for_builtin_remote_call_sparse()
            test_graph_for_py_nested_call_itself_sparse()
            test_graph_for_py_nested_call_sparse()
            test_graph_for_py_nested_remote_call_itself_sparse()
            test_graph_for_py_nested_remote_call_sparse()
            test_graph_for_python_call_sparse()
            test_graph_for_python_remote_call_sparse()
            test_mixed_requires_grad_sparse()
            test_multiple_backward_sparse()
            test_nested_backward_accumulate_grads_sparse()
            test_no_graph_with_tensors_not_require_grad_remote_sparse()
            test_no_graph_with_tensors_not_require_grad_sparse()
            test_remote_complex_args_sparse()
            test_rpc_complex_args_sparse()
            test_trainer_ps_sparse()
          }
          class TensorPipeAgentRpcTest {
            test_builtin_remote_ret_sparse()
            test_builtin_remote_self_sparse()
            test_dynamic_and_static_init_rpc_together()
            test_dynamic_rpc_existing_rank_can_communicate_with_new_rank()
            test_dynamic_rpc_existing_rank_can_communicate_with_new_rank_cuda()
            test_dynamic_rpc_init_rpc()
            test_dynamic_rpc_init_rpc_without_rank()
            test_dynamic_rpc_new_rank_can_communicated_with_existing_rank()
            test_infer_backend_from_options()
            test_mismatched_type_for_options()
            test_multi_builtin_remote_ret_sparse()
            test_multi_py_udf_remote_sparse()
            test_multi_rpc_sparse()
            test_my_parameter_server_sparse()
            test_nested_remote_sparse()
            test_nested_rpc_sparse()
            test_nested_rref_sparse()
            test_nested_rref_stress_sparse()
            test_op_with_invalid_args()
            test_py_rpc_rref_args_sparse()
            test_py_rref_args_sparse()
            test_py_rref_args_user_share_sparse()
            test_py_sparse_tensors_in_container()
            test_rref_get_type_timeout_blocking()
            test_rref_get_type_timeout_non_blocking()
            test_rref_proxy_timeout()
            test_self_py_udf_remote_sparse()
            test_self_remote_rref_as_remote_arg_sparse()
            test_self_remote_rref_as_rpc_arg_sparse()
            test_self_remote_rref_as_self_remote_arg_sparse()
            test_self_remote_rref_as_self_rpc_arg_sparse()
            test_send_to_rank_sparse()
            test_set_and_get_num_worker_threads()
            test_stress_heavy_rpc_sparse()
            test_tensorpipe_options_throw_on_timedelta_timeout()
            test_tensorpipe_set_default_timeout()
            test_wait_all_workers_sparse()
            test_wait_all_workers_twice_sparse()
            test_world_size_one_sparse()
          }
          class TensorPipeCudaDistAutogradTest {
            test_device_maps_backward_pass()
            test_dist_autograd_sync_streams()
            test_gradients_synchronizations()
          }
          class TensorPipeRpcAgentTestFixture {
            rpc_backend
            rpc_backend_options
            get_shutdown_error_regex()
            get_timeout_error_regex()
          }
          class TensorPipeRpcBackendOptions {
            devices : list
            init_method : str
            set_device_map(to: str, device_map: Dict[DeviceType, DeviceType])
            set_devices(devices: List[DeviceType])
          }
          class TensorProperties {
            dtype
            layout
            memory_format
            pin_memory : bool
            requires_grad : bool
            create_from_tensor(tensor: torch.Tensor) 'TensorProperties'
          }
          class TensorProperties {
            dtype
            layout
            memory_format
            pin_memory : bool
            requires_grad : bool
            create_from_tensor(tensor: torch.Tensor) 'TensorProperties'
          }
          class TensorProperty {
            name
            method_name()
          }
          class TensorPropertySource {
            idx : Optional[int]
            prop
            guard_source()
            name()
            reconstruct(codegen)
          }
          class TensorReferenceAnalysis {
            abs(x)
            acos(x)
            add(a, b)
            and_(a, b)
            asin(x)
            atan(x)
            bitwise_and(a, b)
            bitwise_or(a, b)
            ceil(x)
            ceil_to_int(x, dtype)
            constant(c, dtype)
            cos(x)
            cosh(x)
            eq(a, b)
            exp(x)
            floor(x)
            floor_to_int(x, dtype)
            floordiv(a, b)
            ge(a, b)
            gt(a, b)
            int_truediv(a, b)*
            le(a, b)
            log(x)
            log2(x)
            lt(a, b)
            maximum(a, b)
            minimum(a, b)
            mod(x, y)*
            mul(a, b)
            ne(a, b)
            neg(x)
            not_(a)
            or_(a, b)
            pow(a, b)
            pow_by_natural(a, b)
            reciprocal(x)
            round_decimal(a, b)*
            round_to_int(a, dtype)
            sin(x)
            sinh(x)
            sqrt(x)
            square(x)
            sub(a, b)
            tan(x)
            tanh(x)
            to_dtype(x, dtype)
            truediv(a, b)
            trunc_to_int(x, dtype)
            truncdiv(a, b)*
          }
          class TensorSetattr {
            forward(x, attr)
          }
          class TensorStaticReason {
            name
          }
          class TensorStorageMetadata {
            chunks : List[ChunkStorageMetadata]
            properties
            size
          }
          class TensorSubclassVariable {
            value
            as_python_constant()
            call_function(tx: 'InstructionTranslator', args: List[VariableTracker], kwargs: Dict[str, VariableTracker]) VariableTracker
          }
          class TensorTracker {
            assert_eq_kwargs : NoneType, dict
            tensors : list
            add(tensor)
            all_popped()
            pop_check_set(tensor_to_set, testcase)
          }
          class TensorType {
          }
          class TensorVariable {
            class_type
            device
            dtype
            has_grad_fn
            is_contiguous : NoneType
            is_nested
            is_quantized
            is_sparse
            layout
            method_ndimension
            method_nelement
            ndim
            proxy
            requires_grad
            size
            stride : NoneType
            as_proxy()
            call_hasattr(tx: 'InstructionTranslator', name)
            call_id(tx)
            call_method(tx, name, args: 'List[VariableTracker]', kwargs: 'Dict[str, VariableTracker]') 'VariableTracker'
            debug_repr()
            dynamic_getattr(tx: 'InstructionTranslator', name)
            get_real_value()
            has_unpack_var_sequence(tx)
            method___contains__(arg)
            method___getitem__()
            method___len__()
            method___setitem__(key, value)
            method_add_(other)
            method_addcdiv_(tensor1, tensor2)
            method_addcmul_(tensor1, tensor2)
            method_as_subclass(cls)
            method_attr__version(tx)
            method_attr_data(tx)
            method_attr_device(tx)
            method_attr_dtype(tx)
            method_attr_grad_fn(tx)
            method_attr_is_cuda(tx)
            method_attr_is_nested(tx)
            method_attr_is_quantized(tx)
            method_attr_is_sparse(tx)
            method_attr_layout(tx)
            method_attr_ndim(tx)
            method_attr_requires_grad(tx)
            method_attr_shape(tx)
            method_backward()
            method_data_ptr()
            method_dim()
            method_element_size()
            method_get_device()
            method_is_complex()
            method_is_contiguous(memory_format)
            method_is_floating_point()
            method_is_inference()
            method_item()
            method_new()
            method_numel()
            method_numpy()
            method_redistribute()
            method_register_hook()
            method_register_post_accumulate_grad_hook()
            method_requires_grad_(requires_grad)
            method_resize_()
            method_resize_as_()
            method_set_()
            method_size()
            method_sparse_resize_()
            method_sparse_resize_and_clear_()
            method_stride()
            method_to_local()
            method_tolist()
            method_type(dtype, non_blocking)
            method_untyped_storage()
            python_type()
            set_name_hint(name: str)
            specialize(value: torch.Tensor)
            unpack_var_sequence(tx: 'InstructionTranslator', idxes)
            valid_size()
            var_getattr(tx: 'InstructionTranslator', name)
          }
          class TensorWeakRef {
            ref : WeakRef[Tensor]
          }
          class TensorWithFlatten {
            shape
            dim() int
            size(dim: None) Tuple[int, ...]
            storage_offset() int
            stride(dim: None) Tuple[int, ...]
            to(dtype: torch.types._dtype, non_blocking: bool, copy: bool) torch.Tensor
          }
          class TensorWithTFOverrideVariable {
            torch_function_fn
            call_method(tx, name, args: 'List[VariableTracker]', kwargs: 'Dict[str, VariableTracker]') 'VariableTracker'
            call_torch_function(tx: 'InstructionTranslator', fn, types, args, kwargs)
            class_type_var(tx)
            from_tensor_var(tx, tensor_var, class_type, torch_function_fn)
            global_mangled_class_name(tx)
            install_global(tx)
            python_type()
            var_getattr(tx: 'InstructionTranslator', name)
          }
          class TensorWrapper {
            moved_to_gpu : bool
            t
          }
          class TensorWrapper {
            event
            lock : lock
            tensor
            thread : Thread
            increase(v)
            sum()
          }
          class TensorWriteData {
            chunk
            properties
            size
          }
          class TensorboardEventHandler {
          }
          class TensorifyScalarRestartAnalysis {
          }
          class TensorifyState {
            force_specializations : Set[Source]
            clear() None
            should_specialize(index: Source) bool
            specialize(index: Source) None
          }
          class TestAutocast {
            args_maybe_kwargs(op_with_args)
          }
          class TestBase {
            constructor
            constructor_args
            desc : str
            extra_args
            fullname : NoneType
            reference_fn : NoneType
            get_name()
          }
          class TestBenchmarkRequest {
            value : Optional[float]
            benchmark() float
          }
          class TestCase {
            handler : NullHandler
            setUp() None
            setUpClass() None
            tearDown() None
            tearDownClass() None
          }
          class TestCase {
            setUp()
            tearDown()
          }
          class TestCase {
            maxDiff : NoneType
            precision
            rel_tol
            assertAtenOp(onnx_model, operator, overload_name)
            assertEqual(x, y, msg: Optional[Union[str, Callable[[str], str]]])
            assertEqualBroadcasting(x, y) None
            assertEqualIgnoreType() None
            assertEqualTypeString(x, y) None
            assertExpected(s, subname)
            assertExpectedInline(actual, expect, skip, ignore_comments, ignore_empty_lines)
            assertExpectedInlineMunged(exc_type, callable, expect)
            assertExpectedRaises(exc_type, callable)
            assertExpectedStripMangled(s, subname)
            assertGreaterAlmostEqual(first, second, places, msg, delta)
            assertLeaksNoCudaTensors(name)
            assertLogs(logger, level)
            assertNoLogs(logger, level)
            assertNoUnraisable(callable)
            assertNotEqual(x, y, msg: Optional[str]) None
            assertNotWarn(callable, msg)
            assertObjectIn(obj: Any, iterable: Iterable[Any]) None
            assertRaises(expected_exception)
            assertRaisesRegex(expected_exception, expected_regex)
            assertWarnsOnceRegex(category, regex)
            check_nondeterministic_alert(fn, caller_name, should_alert)
            compare_with_numpy(torch_fn, np_fn, tensor_like, device, dtype)
            compare_with_reference(torch_fn, ref_fn, sample_input)
            enforceNonDefaultStream()
            genSparseBSCTensor(size, blocksize, nnz)
            genSparseBSRTensor(size, blocksize, nnz)
            genSparseCSCTensor(size, nnz)
            genSparseCSRTensor(size, nnz)
            genSparseCompressedTensor(size, nnz)
            genSparseTensor(size, sparse_dim, nnz, is_uncoalesced, device, dtype)
            generate_simple_inputs(layout, device, dtype, index_dtype, pin_memory, members_pin_memory, enable_batch, enable_hybrid, enable_zero_sized, enable_non_contiguous_indices, enable_non_contiguous_values, enable_batch_variable_nse, output_tensor, patterns)
            remove_comment_lines(input_string)
            remove_empty_lines(input_string)
            run(result)
            runWithPytorchAPIUsageStderr(code)
            run_process_no_exception(code, env)
            safeToDense(t)
            setUp()
            tearDown()
            wrap_method_with_policy(method, policy)
            wrap_with_cuda_memory_check(method)
            wrap_with_cuda_policy(method_name, policy)
            wrap_with_policy(method_name, policy)
          }
          class TestCaseBase {
          }
          class TestCollectorPlugin {
            tests : list
            pytest_collection_finish(session)
          }
          class TestDebugInfoFunc {
            backward(ctx, input)
            forward(ctx, input)
          }
          class TestDistBackend {
            destroy_pg_upon_exit
            file_name
            init_method
            rank
            skip_return_code_checks : list
            world_size
            setUp()
            setUpClass()
            tearDown()
          }
          class TestEnvironment {
            repro_env_vars : dict
            def_flag(name, env_var, default, include_in_repro, enabled_fn, implied_by_fn)
            def_setting(name, env_var, default, include_in_repro, parse_fn)
            repro_env_var_prefix() str
          }
          class TestException {
          }
          class TestGradients {
            exact_dtype : bool
          }
          class TestHelperModules {
          }
          class TestModel {
            fc1
            fc2
            rank
            forward(x)
          }
          class TestNamedTupleInput_1 {
            a
            b
          }
          class TestPassManager {
            test_pass_manager_builder() None
            test_these_before_those_pass_constraint() None
            test_this_before_that_pass_constraint() None
            test_two_pass_managers() None
          }
          class TestPickler {
          }
          class TestSkip {
            exit_code : int
            message : str
          }
          class TestingOnlyCompileError {
          }
          class TheModule {
            submodule
          }
          class ThreadFlow {
            id : Optional[str]
            immutable_state : Optional[Any]
            initial_state : Optional[Any]
            locations : List[_thread_flow_location.ThreadFlowLocation]
            message : Optional[_message.Message]
            properties : Optional[_property_bag.PropertyBag]
          }
          class ThreadFlowLocation {
            index : int
            location
            stack : Stack | None
            state : Mapping[str, str]
            sarif() sarif.ThreadFlowLocation
          }
          class ThreadFlowLocation {
            execution_order : int
            execution_time_utc : Optional[str]
            importance : Literal['important', 'essential', 'unimportant']
            index : int
            kinds : Optional[List[str]]
            location : Optional[_location.Location]
            module : Optional[str]
            nesting_level : Optional[int]
            properties : Optional[_property_bag.PropertyBag]
            stack : Optional[_stack.Stack]
            state : Optional[Any]
            taxa : Optional[List[_reporting_descriptor_reference.ReportingDescriptorReference]]
            web_request : Optional[_web_request.WebRequest]
            web_response : Optional[_web_response.WebResponse]
          }
          class ThreadLocalWorld {
            default_pg
            group_count
            pg_backend_config
            pg_coalesce_state
            pg_group_ranks
            pg_map
            pg_names
            pg_to_tag
            tags_to_pg
          }
          class ThreeAdd {
            forward(x1, x2, x3, x4)
          }
          class ThreeWorkersRemoteModuleTest {
            world_size
            test_create_remote_module_from_module_rref()
            test_send_remote_module_over_the_wire()
            test_send_remote_module_over_the_wire_script_not_supported()
          }
          class Threshold {
            inplace : bool
            threshold : float
            value : float
            extra_repr()
            forward(input: Tensor) Tensor
          }
          class ThroughputBenchmark {
            add_input()
            benchmark(num_calling_threads, num_warmup_iters, num_iters, profiler_output_path)
            run_once()
          }
          class Thunk {
            f : Optional[Callable[[], R]]
            r : Optional[R]
            force() R
          }
          class TileHint {
            name
          }
          class TilingSelect {
            select_tiling(fn_list, var_sizes_list) Tuple[List[int], List[int]]
          }
          class TimeitModuleType {
            timeit(number: int) float
          }
          class Timer {
            adaptive_autorange(threshold: float) common.Measurement
            autorange(callback: Optional[Callable[[int, float], NoReturn]])* None
            blocked_autorange(callback: Optional[Callable[[int, float], NoReturn]], min_run_time: float) common.Measurement
            collect_callgrind(number: int) valgrind_timer_interface.CallgrindStats
            repeat(repeat: int, number: int)* None
            timeit(number: int) common.Measurement
          }
          class TimerClass {
            timeit(number: int) float
          }
          class TimerClient {
            acquire(scope_id: str, expiration_time: float)* None
            release(scope_id: str)*
          }
          class TimerRequest {
            expiration_time : float
            scope_id : str
            worker_id : Any
          }
          class TimerServer {
            clear_timers(worker_ids: Set[Any])* None
            get_expired_timers(deadline: float)* Dict[str, List[TimerRequest]]
            register_timers(timer_requests: List[TimerRequest])* None
            start() None
            stop() None
          }
          class ToFloat {
            is_real : bool
            eval(number)
          }
          class TokenArgument {
            name : Annotated[str, 10]
          }
          class TokenArgument {
            name : str
          }
          class Tool {
            driver
            extensions : Optional[List[_tool_component.ToolComponent]]
            properties : Optional[_property_bag.PropertyBag]
          }
          class ToolComponent {
            associated_component : Optional[_tool_component_reference.ToolComponentReference]
            contents : List[Literal['localizedData', 'nonLocalizedData']]
            dotted_quad_file_version : Optional[str]
            download_uri : Optional[str]
            full_description : Optional[_multiformat_message_string.MultiformatMessageString]
            full_name : Optional[str]
            global_message_strings : Optional[Any]
            guid : Optional[str]
            information_uri : Optional[str]
            is_comprehensive : Optional[bool]
            language : str
            localized_data_semantic_version : Optional[str]
            locations : Optional[List[_artifact_location.ArtifactLocation]]
            minimum_required_localized_data_semantic_version : Optional[str]
            name : str
            notifications : Optional[List[_reporting_descriptor.ReportingDescriptor]]
            organization : Optional[str]
            product : Optional[str]
            product_suite : Optional[str]
            properties : Optional[_property_bag.PropertyBag]
            release_date_utc : Optional[str]
            rules : Optional[List[_reporting_descriptor.ReportingDescriptor]]
            semantic_version : Optional[str]
            short_description : Optional[_multiformat_message_string.MultiformatMessageString]
            supported_taxonomies : Optional[List[_tool_component_reference.ToolComponentReference]]
            taxa : Optional[List[_reporting_descriptor.ReportingDescriptor]]
            translation_metadata : Optional[_translation_metadata.TranslationMetadata]
            version : Optional[str]
          }
          class ToolComponentReference {
            guid : Optional[str]
            index : int
            name : Optional[str]
            properties : Optional[_property_bag.PropertyBag]
          }
          class TopLevelTracedModule {
            forward : Callable[..., Any]
          }
          class TorchBindObject {
            name : str
            value
            codegen_reference(writer: Optional[IndentedBuffer]) str
            get_name()
          }
          class TorchBindOpOverload {
          }
          class TorchCtxManagerClassVariable {
            call_function(tx: 'InstructionTranslator', args: 'List[VariableTracker]', kwargs: 'Dict[str, VariableTracker]') 'VariableTracker'
            is_matching_cls(value)
          }
          class TorchDispatchMode {
            old_dispatch_mode_flags : Deque[bool]
            old_non_infra_dispatch_mode_flags : Deque[bool]
            is_infra_mode()
            push()
          }
          class TorchDynamoException {
          }
          class TorchExportError {
          }
          class TorchExportNonStrictStrategy {
          }
          class TorchExportStrategy {
          }
          class TorchFunctionDisableVariable {
            create(tx: 'InstructionTranslator')
            enter(tx)
          }
          class TorchFunctionMetadataMode {
            tracer : Union
          }
          class TorchFunctionMode {
            inner : str
            push()
          }
          class TorchFunctionModeStackSource {
            ind : int
            guard_source()
            name()
            reconstruct(codegen)
          }
          class TorchFunctionModeStackStateManager {
            stack : list
            temp_restore_stack()
          }
          class TorchFunctionModeStackVariable {
            offset : int
            source
            stack_value_singleton : object
            symbolic_stack
            clear_default_device(tx: 'InstructionTranslator')
            get_mode_index(ind)
            is_device_context(var)
            register_device_context_insertion(tx: 'InstructionTranslator')
            register_mutation(tx: 'InstructionTranslator')
            reset()
          }
          class TorchFunctionModeVariable {
            cm_obj
            source : NoneType
            value
            call_torch_function(tx: 'InstructionTranslator', fn, types, args, kwargs)
            enter(tx)
            exit(tx: 'InstructionTranslator')
            exit_on_graph_break()
            fn_name()
            is_supported_torch_function_mode(ty)
            module_name()
            python_type()
            reconstruct(codegen)
            reconstruct_type(codegen)
            supports_graph_breaks()
          }
          class TorchHigherOrderOperatorVariable {
            source : Optional[Source]
            value
            call_function(tx: 'InstructionTranslator', args: List[VariableTracker], kwargs: Dict[str, VariableTracker]) VariableTracker
            make(value, source)
          }
          class TorchInGraphFunctionVariable {
            class_type
            has_grad_fn : bool
            call_function(tx: 'InstructionTranslator', args: 'List[VariableTracker]', kwargs: 'Dict[str, VariableTracker]') 'VariableTracker'
            call_nn_parameter(tx, data, requires_grad)
            call_tensor_method(tx, args, kwargs)
            get_function()
            is_tensor_method()
            torch_function_override_enabled(tx, args, kwargs)
          }
          class TorchLogsFormatter {
            format(record)
          }
          class TorchPatcher {
            patch()
            suppress_torch_distributed_warnings(fn)
          }
          class TorchRefsMode {
            prims_mode_cls : nullcontext
            should_fallback_fn
            strict : bool
          }
          class TorchRuntimeError {
          }
          class TorchScalarTypes {
            name
          }
          class TorchScriptObjectVariable {
            proxy
            source
            as_proxy()
            call_method(tx, name, args, kwargs)
            create(proxy, value)
            is_matching_cls(user_cls: type)
            var_getattr(tx, name: str) VariableTracker
          }
          class TorchScriptOnnxExportDiagnostic {
            cpp_call_stack : infra.Stack | None
            python_call_stack : infra.Stack | None
            record_cpp_call_stack(frames_to_skip: int) infra.Stack
          }
          class TorchSplit {
          }
          class TorchSymMin {
            forward(x)
          }
          class TorchTensor {
            raw
            numpy() npt.NDArray
            tobytes() bytes
          }
          class TorchVersion {
          }
          class TorchVersionVariable {
          }
          class ToyModel {
            net1
            net2
            relu
          }
          class ToyModel {
            net1
            net2
            forward(x)
          }
          class ToyModel {
            bias
            net1
            net2
            forward(x)
          }
          class ToyModel {
            lin
            forward(x, expected_type)
          }
          class ToyModel {
            lin1
            lin2
            rank
            forward(x)
          }
          class ToyModel {
            lin1
            lin2
            rank
            forward(x)
          }
          class ToyModel {
            net1
            net2
            forward(x, find_unused, dynamic)
          }
          class TracableCreateParameter {
            backward(ctx: Any) Tuple[None, torch.Tensor]
            forward(ctx: Any, tensor: Any, placeholder: Any) torch.nn.Parameter
          }
          class TraceError {
          }
          class TraceFn {
          }
          class TraceId {
            attempt : int
            compile_id : CompileId
          }
          class TraceWrapped {
          }
          class TraceWrappedHigherOrderOperatorVariable {
            call_function(tx: 'InstructionTranslator', args: 'List[VariableTracker]', kwargs: 'Dict[str, VariableTracker]') 'VariableTracker'
          }
          class TraceableTritonKernelWrapper {
            grid : Optional['TritonGridType']
            kernel : str
            kernel_idx : Optional[int]
            run() Any
            specialize_symbolic(arg: Sequence[Any]) Any
          }
          class TracedModule {
            extra_repr()
            forward()
          }
          class Tracer {
            graph
            module_stack : OrderedDict
            node_name_to_scope : Dict[str, Tuple[str, type]]
            num_calls : Dict[str, int]
            param_shapes_constant : bool
            root
            root_module_name : str
            scope
            submodule_paths : NoneType, Optional[Dict[torch.nn.Module, str]]
            tensor_attrs : Dict[Union[torch.Tensor, ScriptObject, FakeScriptObject], str]
            call_module(m: torch.nn.Module, forward: Callable[..., Any], args: Tuple[Any, ...], kwargs: Dict[str, Any]) Any
            create_arg(a: Any) 'Argument'
            create_args_for_root(root_fn, is_module, concrete_args)
            get_fresh_qualname(prefix: str) str
            getattr(attr: str, attr_val: Any, parameter_proxy_cache: Dict[str, Any])
            is_leaf_module(m: torch.nn.Module, module_qualified_name: str) bool
            path_of_module(mod: torch.nn.Module) str
            trace(root: Union[torch.nn.Module, Callable[..., Any]], concrete_args: Optional[Dict[str, Any]]) Graph
          }
          class TracerBase {
            check_mutable_operations : bool
            graph
            module_stack : OrderedDict[str, Tuple[str, Any]]
            node_name_to_scope : Dict[str, Tuple[str, type]]
            proxy_buffer_attributes : bool
            record_stack_traces : bool
            scope
            trace_asserts : bool
            traced_func_name : str
            create_arg(a: Any) Argument
            create_node(kind: str, target: Target, args: Tuple[Argument, ...], kwargs: Dict[str, Argument], name: Optional[str], type_expr: Optional[Any]) Node
            create_proxy(kind: str, target: Target, args: Tuple[Any, ...], kwargs: Dict[str, Any], name: Optional[str], type_expr: Optional[Any], proxy_factory_fn: Callable[[Node], 'Proxy'])
            iter(obj: 'Proxy') Iterator
            keys(obj: 'Proxy') Any
            proxy(node: Node) 'Proxy'
            to_bool(obj: 'Proxy') bool
          }
          class TracerWarning {
            ignore_lib_warnings()
          }
          class TracingCheckError {
            message : str
          }
          class TracingConfig {
            concrete_args : Optional[Dict[str, Any]]
            tracer
          }
          class TracingContext {
            aot_graph_name : NoneType
            fake_mode
            fakify_first_call : bool
            force_unspec_int_unbacked_size_like : bool
            frame_summary_stack : list
            fw_metadata : NoneType
            global_context
            guards_context
            hop_dispatch_set_cache
            loc_in_frame : NoneType
            module_context
            output_strides : Optional[List[Optional[Tuple[int, ...]]]]
            params_flat : NoneType
            params_flat_unwrap_subclasses : NoneType
            params_unwrapped_to_flat_index : NoneType
            tensor_to_context
            clear()
            clear_frame()
            current_frame(frame_summary)
            extract_stack()
            get() TracingContext
            patch()
            report_output_strides()
            set_current_loc(filename, lineno, frame_name)
            try_get() Optional[TracingContext]
          }
          class TracingOpsHandler {
            placeholders
            tracer
            output() torch.fx.Node
            placeholder(idx: int) torch.fx.Proxy
          }
          class TracingTritonHOPifier {
            call_HOP(variable: 'TraceableTritonKernelWrapper', grids: List['TritonGridTupleType'], combined_args: Dict[str, Any], tx: None) None
            call_grid(grid: 'TritonGridCallableType', meta: 'TritonMetaParamsType', tx: None) Tuple[Union[int, sympy.Expr, SymInt], ...]
            check_grid(grid: 'TritonGridType') Tuple[Union[int, sympy.Expr, SymInt], ...]
            get_value(val: Any) Any
            is_callable(maybe_callable: Any) bool
            raise_unsupported(msg: str) Never
          }
          class TrackedFake {
            fake : Union[FakeTensor, SymInt]
            source
            symbolic_context : Optional[SymbolicContext]
          }
          class TrackedInput {
            index : int
            type_desc : str
            val : Any
          }
          class TrackedInputIter {
            child_iter : enumerate
            input_type_desc
            item_callback : NoneType
            restrict_to_index : NoneType
            set_seed : bool
            test_fn : NoneType
            track_callback : NoneType
          }
          class Tracker {
            seen : List[ReferenceType[CodeType]]
            seen_ids : Set[int]
            add(strong_obj: CodeType) None
            clear() None
          }
          class Trainer {
            ddp_params : tuple
            hybrid_module
            non_ddp_params : tuple
            rank : int
            remote_em_rref : RRef
            remote_net_rref : RRef
            trainer_group : NoneType
            destroy_pg()
            train_batch(mini_batch: FeatureSet, trainer_has_less_inputs: bool, simulate_uneven_inputs: bool)
          }
          class Trainer {
            loss_fn
            ps_rref
            get_next_batch()
            train()
          }
          class TrainingAwareDataSparsity {
            data_scheduler : Optional[Any]
            data_scheduler_args
            data_scheduler_class
            data_sparsifier : Optional[Any]
            data_sparsifier_args
            data_sparsifier_class
            data_sparsifier_state_dict : Optional[Any]
            sparsified : Optional[torch.nn.Module]
            on_train_end(trainer, pl_module)
            on_train_epoch_end(trainer, pl_module)
            on_train_epoch_start(trainer, pl_module)
            on_train_start(trainer, pl_module) None
          }
          class TrainingIRVerifier {
            dialect : str
          }
          class TrainingState {
            name
          }
          class TrainingState {
            name
          }
          class Transform {
            bijective : bool
            codomain
            domain
            event_dim
            inv
            sign
            forward_shape(shape)
            inverse_shape(shape)
            log_abs_det_jacobian(x, y)*
            with_cache(cache_size)
          }
          class Transform {
            diagnostic_context
            fake_mode : fake_tensor.FakeTensorMode | None
            module
            run() torch.fx.GraphModule
          }
          class TransformGetItemToIndex {
          }
          class TransformedDistribution {
            arg_constraints : Dict[str, constraints.Constraint]
            base_dist : NoneType
            has_rsample
            transforms : list
            cdf(value)
            expand(batch_shape, _instance)
            icdf(value)
            log_prob(value)
            rsample(sample_shape: _size) torch.Tensor
            sample(sample_shape)
            support()
          }
          class Transformer {
            new_graph
            tracer
            call_function(target: 'Target', args: Tuple[Argument, ...], kwargs: Dict[str, Any]) Any
            call_module(target: 'Target', args: Tuple[Argument, ...], kwargs: Dict[str, Any]) Any
            get_attr(target: 'Target', args: Tuple[Argument, ...], kwargs: Dict[str, Any]) Proxy
            placeholder(target: 'Target', args: Tuple[Argument, ...], kwargs: Dict[str, Any]) Proxy
            transform() GraphModule
          }
          class Transformer {
            checkpoint_activations
            dropout
            layers
            max_seq_len
            model_args
            norm
            output
            pos_embeddings
            tok_embeddings
            forward(tokens)
            parallelize(module: 'Transformer', device_mesh: DeviceMesh, use_seq_parallel: bool, local_output_for_attn: bool) nn.Module
          }
          class Transformer {
            batch_first : bool
            d_model : int
            decoder
            encoder
            nhead : int
            forward(src: Tensor, tgt: Tensor, src_mask: Optional[Tensor], tgt_mask: Optional[Tensor], memory_mask: Optional[Tensor], src_key_padding_mask: Optional[Tensor], tgt_key_padding_mask: Optional[Tensor], memory_key_padding_mask: Optional[Tensor], src_is_causal: Optional[bool], tgt_is_causal: Optional[bool], memory_is_causal: bool) Tensor
            generate_square_subsequent_mask(sz: int, device: Optional[torch.device], dtype: Optional[torch.dtype]) Tensor
          }
          class TransformerBlock {
            attention
            attention_norm
            feed_forward
            ffn_norm
            forward(x)
          }
          class TransformerDecoder {
            layers
            norm : Optional[Module]
            num_layers : int
            forward(tgt: Tensor, memory: Tensor, tgt_mask: Optional[Tensor], memory_mask: Optional[Tensor], tgt_key_padding_mask: Optional[Tensor], memory_key_padding_mask: Optional[Tensor], tgt_is_causal: Optional[bool], memory_is_causal: bool) Tensor
          }
          class TransformerDecoderLayer {
            activation
            dropout
            dropout1
            dropout2
            dropout3
            linear1
            linear2
            multihead_attn
            norm1
            norm2
            norm3
            norm_first : bool
            self_attn
            forward(tgt: Tensor, memory: Tensor, tgt_mask: Optional[Tensor], memory_mask: Optional[Tensor], tgt_key_padding_mask: Optional[Tensor], memory_key_padding_mask: Optional[Tensor], tgt_is_causal: bool, memory_is_causal: bool) Tensor
          }
          class TransformerEncoder {
            enable_nested_tensor : bool
            layers
            mask_check : bool
            norm : Optional[Module]
            num_layers : int
            use_nested_tensor : bool
            forward(src: Tensor, mask: Optional[Tensor], src_key_padding_mask: Optional[Tensor], is_causal: Optional[bool]) Tensor
          }
          class TransformerEncoderLayer {
            activation : Union[str, Callable[[Tensor], Tensor]]
            activation_relu_or_gelu : int
            dropout
            dropout1
            dropout2
            linear1
            linear2
            norm1
            norm2
            norm_first : bool
            self_attn
            forward(src: Tensor, src_mask: Optional[Tensor], src_key_padding_mask: Optional[Tensor], is_causal: bool) Tensor
          }
          class TransformerTracer {
            graph
            root
            tensor_attrs : Dict[torch.Tensor, str]
            is_leaf_module(_, __) bool
          }
          class TransformerWithSharedParams {
            bn
            bs : int
            embed_tokens
            output_proj
            rank
            transformer
            world_size
            forward(src_ids, tgt_ids)
            get_ignored_modules()
            get_input(device)
            get_loss(input, output)
            init(group: dist.ProcessGroup, fsdp_init_mode: FSDPInitMode, device_init_mode: DEVICEInitMode, fsdp_kwargs: Optional[Dict[str, Any]], deterministic: bool, add_bn: bool) Union[nn.Module, FSDP]
            run_backward(loss)
          }
          class TranslationMetadata {
            download_uri : Optional[str]
            full_description : Optional[_multiformat_message_string.MultiformatMessageString]
            full_name : Optional[str]
            information_uri : Optional[str]
            name : str
            properties : Optional[_property_bag.PropertyBag]
            short_description : Optional[_multiformat_message_string.MultiformatMessageString]
          }
          class TranslationValidator {
            symbols : Dict[sympy.Symbol, z3.ExprRef]
            add_assertion(e: Union[z3.BoolRef, sympy.Basic]) None
            add_source_expr(e: z3.BoolRef) None
            add_target_expr(e: 'sympy.logic.boolalg.Boolean') None
            add_var(symbol: sympy.Symbol, type: Type) z3.ExprRef
            to_z3_boolean_expr(e: sympy.Basic) z3.BoolRef
            validate() None
            z3var(symbol: sympy.Symbol) z3.ExprRef
          }
          class Transpose {
            index1
            index2
            input_var
            output
            tensor_size
          }
          class TreeManagerContainer {
            device_index : int
            graph : NoneType, Optional[torch.cuda.CUDAGraph]
            live_cudagraphify_fns : int
            live_storages_count : int
            lock : lock
            tree_manager : NoneType, Optional[CUDAGraphTreeManager]
            add_strong_reference(fn: Callable[..., Any]) None
            finalize_cudagraphify_fn() None
            get_tree_manager() CUDAGraphTreeManager
          }
          class TreeSpec {
            children_specs : List['TreeSpec']
            context : Any
            num_children : int
            num_leaves : int
            num_nodes : int
            type : Any
            flatten_up_to(tree: PyTree) List[PyTree]
            is_leaf() bool
            unflatten(leaves: Iterable[Any]) PyTree
          }
          class Trie {
            root
            add(word)
            dump()
            export_to_regex()
            pattern()
            quote(char)
            search(word)
          }
          class TrieNode {
            children : dict
          }
          class TripletMarginLoss {
            eps : float
            margin : float
            p : float
            swap : bool
            forward(anchor: Tensor, positive: Tensor, negative: Tensor) Tensor
          }
          class TripletMarginWithDistanceLoss {
            distance_function : Optional[Callable[[Tensor, Tensor], Tensor]]
            margin : float
            swap : bool
            forward(anchor: Tensor, positive: Tensor, negative: Tensor) Tensor
          }
          class TritonBenchmarkRequest {
            grid : List[int]
            matrix_instr_nonkdim : int
            module_cache_key : str
            module_path : str
            num_stages : int
            num_warps : int
            workspace_arg : Optional[WorkspaceArg]
            make_run_fn() Callable[[], None]
            precompile()
          }
          class TritonBenchmarker {
            triton_do_bench
            benchmark_gpu(_callable: Callable[[], Any]) float
          }
          class TritonBundleEntry {
            device : int
            directory : str
            kernel_hash : str
          }
          class TritonBundler {
            begin_compile() None
            collect() Tuple[List[TritonKernelArtifacts], Optional[TritonBundlerMetadata]]
            end_compile() None
            is_enabled() bool
            put(kernel_hash: str, device: int) None
            read_and_emit(bundle: List[TritonKernelArtifacts]) Optional[TritonBundlerMetadata]
          }
          class TritonBundlerMetadata {
            cached_kernel_names : List[str]
          }
          class TritonCPUBenchmarkRequest {
          }
          class TritonCSE {
            augment_key(cache_key: object) object
          }
          class TritonCSEVariable {
            mask_vars
            update_on_args(name, args, kwargs)
          }
          class TritonCodeCache {
            load(kernel_name: str, source_code: str) ModuleType
          }
          class TritonFuture {
            future : NoneType, Optional[Future[Any]]
            kernel : module
            result() ModuleType
          }
          class TritonGPUBenchmarkRequest {
          }
          class TritonHOPifier {
            call_HOP(variable, grids, combined_args: Dict[str, Any], tx)* Optional['ConstantVariable']
            call_getitem(variable: Union['TritonKernelVariable', 'TraceableTritonKernelWrapper'], args: Sequence[Any]) Union['TritonKernelVariable', 'TraceableTritonKernelWrapper']
            call_grid(grid, meta, tx)* Union[Tuple[Union[int, sympy.Expr, SymInt], ...], Tuple['Proxy', ...]]
            call_run(variable: Union['TritonKernelVariable', 'TraceableTritonKernelWrapper'], args: Sequence[Any], kwargs: Dict[str, Any], tx: Optional['InstructionTranslator']) Optional['ConstantVariable']
            call_triton_kernel(variable: Union['TritonKernelVariable', 'TraceableTritonKernelWrapper'], args: Sequence[Any], kwargs: Dict[str, Any], tx: Optional['InstructionTranslator']) Optional['ConstantVariable']
            check_grid(grid)* Union[Tuple[Union[int, sympy.Expr, SymInt], ...], Tuple['Proxy', ...]]
            get_value(val: Any)* Any
            init_variable(variable: Union['TraceableTritonKernelWrapper', 'TritonKernelVariable'], kernel: 'TritonKernelType', kernel_idx: Optional[int], grid: Optional['TritonGridType']) None
            is_callable(maybe_callable: Any)* bool
            raise_unsupported(msg: str)* Never
          }
          class TritonKernel {
            allow_block_ptr : bool
            assert_function
            autotune_hints
            block_ptr_id : count
            cooperative_reduction_workspace_cache
            cse
            fixed_config : Optional[FixedTritonConfig]
            helper_functions
            inside_reduction : bool
            kexpr : Callable[[sympy.Expr], str]
            min_elem_per_thread : int
            optimize_mask : bool
            outside_loop_vars
            overrides
            post_loop_combine
            post_loop_store
            semaphores_name : str
            triton_meta : Optional[Dict[str, Any]], dict
            add_numel_to_call_args_and_grid(name, call_args, arg_types, grid)
            bucketize(values: CSEVariable, boundaries: Tuple[str, sympy.Expr, sympy.Expr, sympy.Expr], boundary_indices: CSEVariable, indexing_dtype: torch.dtype, right: bool, sorter: Optional[Tuple[str, sympy.Expr]], sorter_indices: Optional[CSEVariable]) CSEVariable
            call_kernel(name: str, node: Optional[IRNode])
            check_bounds(expr: sympy.Expr, size: sympy.Expr, lower: bool, upper: bool)
            codegen_block_ptr(name: str, var: str, indexing: BlockPtrOptions, other) Tuple[str, Optional[DeferredLine], str]
            codegen_block_ptr_store_line(name, indexing, block_ptr, value, other)
            codegen_body()
            codegen_cooperative_reduction_peer_combine(result_var, dtype)
            codegen_iteration_ranges_entry(entry: IterationRangesEntry)
            codegen_kernel(name)
            codegen_kernel_benchmark(num_gb, grid)
            codegen_nan_check()
            codegen_range_tree()
            codegen_reduction_indices(buffer) None
            codegen_reduction_numels(buffer) None
            codegen_static_numels(code)
            create_cse_var()
            dtype_to_str(dtype: torch.dtype) str
            filter_masks(mask_vars)
            get_load_buffer(indexing)
            get_reduction_prefixes() List[str]
            guard_cooperative_store(name, buffer)
            has_persistent_RBLOCK(rnumel)
            imports_for_benchmark_kernel()
            indexing(index: sympy.Expr)
            inductor_meta_common()
            init_cooperative_reduction()
            iteration_ranges_codegen_header(entry, code)
            iteration_ranges_get_pid(entry)
            iteration_ranges_ranges_code(entry)
            iteration_ranges_scalar_code(entry, value)
            load(name: str, index: sympy.Expr)
            max_block(prefix)
            max_rsplit()
            need_numel_args()
            reduction(dtype: torch.dtype, src_dtype: torch.dtype, reduction_type: ReductionType, value: Union[CSEVariable, Tuple[CSEVariable, ...]]) Union[CSEVariable, Tuple[CSEVariable, ...]]
            reduction_resize(value)
            scan(dtypes: Tuple[torch.dtype, ...], combine_fn: Callable[[Tuple[CSEVariable, ...], Tuple[CSEVariable, ...]], Tuple[CSEVariable, ...]], values: Tuple[CSEVariable, ...]) Tuple[CSEVariable, ...]
            should_use_cooperative_reduction() bool
            should_use_persistent_reduction() bool
            sort(dtypes: Tuple[torch.dtype, ...], values: Tuple[CSEVariable, ...], stable: bool, descending: bool) Tuple[CSEVariable, ...]
            store(name: str, index: sympy.Expr, value: CSEVariable, mode: StoreMode) None
            store_reduction(name: str, index: sympy.Expr, value: CSEVariable)
            want_no_x_dim()
            welford_reduce(result_var, reduction_type, value, where_cond, acc_type, dtype)
            welford_reduce_final_reduction(buf, result_mean, result_m2, result_weight, accumulator, accumulator_m2, accumulator_weight, dim)
          }
          class TritonKernelArtifact {
            filename : str
            payload : bytes
          }
          class TritonKernelArtifacts {
            artifacts : List[TritonKernelArtifact]
            device : int
            kernel_hash : str
          }
          class TritonKernelOverrides {
            constant(value, dtype)
            frexp(x)
            index_expr(expr, dtype)
            load_seed(name, offset)
            masked(mask, body, other)
          }
          class TritonKernelVariable {
            grid : str
            kernel : str
            kernel_idx : Optional[int]
            call_function(tx: 'InstructionTranslator', args: 'List[VariableTracker]', kwargs: 'Dict[str, VariableTracker]') 'VariableTracker'
            call_method(tx, name, args: 'List[VariableTracker]', kwargs: 'Dict[str, VariableTracker]') 'VariableTracker'
            specialize_symbolic(arg: Any) Any
          }
          class TritonKernelWrapperFunctional {
          }
          class TritonKernelWrapperMutation {
          }
          class TritonOverrides {
            abs(x)
            acos(x)
            acosh(x)
            asin(x)
            asinh(x)
            atan(x)
            atan2(x, y)
            atanh(x)
            bitwise_and(a, b)
            bitwise_left_shift(a, b)
            bitwise_not(a)
            bitwise_or(a, b)
            bitwise_right_shift(a, b)
            bitwise_xor(a, b)
            ceil(x)
            constant(value, dtype)
            copysign(x, y)
            cos(x)
            cosh(x)
            erf(x)
            erfc(x)
            erfinv(x)
            exp(x)
            exp2(x)
            expm1(x)
            floor(x)
            floordiv(a, b)
            fmod(a, b)
            hypot(x, y)
            index_expr(expr, dtype)*
            inline_asm_elementwise()
            isinf(x)
            isnan(x)
            lgamma(x)
            libdevice_abs(x)
            libdevice_cos(x)
            libdevice_exp(x)
            libdevice_log(x)
            libdevice_sin(x)
            libdevice_sqrt(x)
            load_seed(name, offset)*
            log(x)
            log10(x)
            log1p(x)
            log2(x)
            logical_and(a, b)
            logical_not(a)
            logical_or(a, b)
            logical_xor(a, b)
            masked(mask, body, other)*
            maximum(a, b)
            minimum(a, b)
            nextafter(x, y)
            pow(a, b)
            rand(seed, offset)
            randint64(seed, offset, low, high)
            randn(seed, offset)
            relu(x)
            round(x)
            rsqrt(x)
            sigmoid(x)
            sign(x)
            signbit(x)
            sin(x)
            sinh(x)
            sqrt(x)
            tan(x)
            tanh(x)
            to_dtype(x, dtype: torch.dtype, src_dtype: Optional[torch.dtype], use_compute_types)
            to_dtype_bitcast(x, dtype: torch.dtype, src_dtype: torch.dtype)
            trunc(x)
            truncdiv(a, b)
            where(a, b, c)
          }
          class TritonPrinter {
          }
          class TritonScheduling {
            backend_features : dict
            kernel_type : Type[Any]
            add_multi_kernel_choices(kernel: SIMDKernel, kernel_args: List[Any], kernel_kwargs: Dict[str, Any]) List[SIMDKernel]
            benchmark_combo_kernel(node_list)
            benchmark_fused_nodes(nodes, n_spills_threshold)
            codegen_comment(node_schedule)
            create_kernel_choices(kernel_features, kernel_args, kernel_kwargs) List[SIMDKernel]
            define_kernel(src_code, node_schedule, kernel)
            get_backend_features(device: torch.device)
          }
          class TritonSplitScanKernel {
            no_x_dim : bool
            initialize_range_tree(pid_cache)
            reduction(dtype, src_dtype, reduction_type, value)*
            scan(dtypes, combine_fn, values)
            should_use_cooperative_reduction() bool
            should_use_persistent_reduction() bool
          }
          class TritonSymbols {
            block_offsets
            block_sizes
            block_types
            reduction_types
            get_block_offset(tree: IterationRanges) sympy.Symbol
            get_block_size(tree: IterationRanges) sympy.Symbol
          }
          class TritonTemplate {
            all_templates : Dict[str, 'TritonTemplate']
            debug : bool
            grid : Any
            index_counter : count
            template : NoneType, Template
            generate(input_nodes, layout, num_stages, num_warps, prefix_args, suffix_args, epilogue_fn, subgraphs, mutated_inputs, call_sizes, workspace_arg: Optional[WorkspaceArg])
          }
          class TritonTemplateBuffer {
            allowed_prologue_inps
            mutated_inputs : Optional[Iterable[IRNode]]
            outputs : List[Buffer]
            get_allowed_prologue_inps() OrderedSet[str]
            get_outputs() List[Buffer]
          }
          class TritonTemplateCaller {
            allowed_prologue_inps : NoneType
            bmreq
            log_info : Optional[Dict[str, Any]]
            make_kernel_render
            mutated_inputs : NoneType
            workspace_arg : Optional[WorkspaceArg]
            autoheuristic_id()
            benchmark()
            call_name()
            get_make_kernel_render()
            hash_key()
            info_dict() Dict[str, Union[PrimitiveInfoType, List[PrimitiveInfoType]]]
            output_node()
            precompile()
          }
          class TritonTemplateCallerBase {
            get_make_kernel_render()* Any
          }
          class TritonTemplateKernel {
            body
            call_sizes
            compute
            defines
            epilogue_fn
            grid_fn
            indexing_code
            input_nodes
            kernel_name
            loads
            meta
            name
            named_input_nodes : dict
            num_stages
            num_warps
            numels
            ops_handler : Optional[V.WrapperHandler]
            output_node
            prefix_args : int
            prologue_fused_inputs : OrderedSet[str]
            prologue_supported_inputs : OrderedSet[str]
            range_trees : list
            render_hooks : dict
            stores
            subgraph_bodies : Dict[str, SubgraphInfo]
            subgraphs : Optional[List[ir.ComputedBuffer]]
            suffix_args : int
            template_indices : list
            template_mask : NoneType, Optional[str]
            template_out : Optional[str]
            triton_meta : Optional[Dict[str, object]], dict
            use_jit : bool
            workspace_arg : Optional[WorkspaceArg]
            call_kernel(name: str, node: Optional[ir.IRNode])
            codegen_range_tree()*
            create_subgraph_body(body_name: str)
            def_kernel()
            estimate_kernel_num_bytes()
            gen_argdefs()
            gen_defines()
            indexing(index: sympy.Expr)
            jit_lines()
            load_input(input_name: str, output_name: str, indices: Union[List[Any], Tuple[Any]], mask: Optional[str], other: Optional[Union[float, int]], indent_width: int)
            make_load(name, indices, mask)
            modification(subgraph_number: int, output_name: Optional[str], mask: Optional[str]) str
            need_numel_args()
            render(template, kwargs)
            set_subgraph_body(body_name: str)
            size(name: str, index: int)
            store_output(indices: Union[List[Any], Tuple[Any]], val: str, mask: Optional[str], indent_width: int)
            stride(name, index)
            template_env()
          }
          class TrivialLossWrapper {
            loss_spec : bool
            forward(x, targets)
          }
          class TruncToFloat {
            is_real : bool
            eval(number)
          }
          class TruncToInt {
            is_integer : bool
            eval(number)
          }
          class TuningProcess {
            device : Optional[int]
            process : Optional[BaseProcess]
            request_queue : Optional[Queue[Any]]
            response_queue : Optional[Queue[Any]]
            clear() None
            get(result_timeout, graceful_timeout, terminate_timeout) Any
            initialize() None
            kill(graceful_timeout, terminate_timeout) None
            process_main(request_queue: Queue[Any], response_queue: Queue[Any]) None
            put(obj: Any) None
            terminate() None
            valid() bool
            wait() None
            workloop(request_queue: Queue[Any], response_queue: Queue[Any]) None
          }
          class TuningProcessPool {
            executor : Optional[ThreadPoolExecutor]
            processes : Optional[queue.Queue[TuningProcess]]
            benchmark(choices: List[TritonTemplateCaller]) Dict[TritonTemplateCaller, float]
            get_device_list() Sequence[Optional[int]]
            initialize() None
            target(choice: TritonTemplateCaller) float
            terminate() None
          }
          class TupleIteratorGetItemSource {
            name()
            reconstruct(codegen)
          }
          class TupleIteratorVariable {
          }
          class TupleStrategy {
            childs : Sequence[StrategyType]
          }
          class TupleVariable {
            class_type
            has_grad_fn : bool
            call_hasattr(tx: 'InstructionTranslator', name: str) 'VariableTracker'
            call_method(tx, name, args: List['VariableTracker'], kwargs: Dict[str, 'VariableTracker']) 'VariableTracker'
            debug_repr()
            python_type()
            reconstruct(codegen: 'PyCodegen') None
            var_getattr(tx, name)
          }
          class TwoLayerConvModel {
            conv1
            conv2
            forward(x)
            get_example_inputs() Tuple[Any, ...]
          }
          class TwoLayerFunctionalConvModel {
            conv1
            conv2
            forward(x)
            get_example_inputs() Tuple[Any, ...]
          }
          class TwoLayerFunctionalLinearModel {
            linear1
            linear2
            forward(x)
            get_example_inputs() Tuple[Any, ...]
          }
          class TwoLayerLinearModel {
            fc1
            fc2
            qconfig
            forward(x)
            get_example_inputs() Tuple[Any, ...]
          }
          class TwoLinLayerNet {
            a
            b
            forward(x)
          }
          class TwoLinearModule {
            linear1
            linear2
            example_inputs()
            forward(x)
          }
          class TwoTensor {
            a
            b
          }
          class TwoTensorMode {
          }
          class Type {
            backward(ctx, grad_output)
            forward(ctx, i, dest_type)
          }
          class Type {
            name
          }
          class TypeConstraintParam {
            allowed_types : set[ir.TypeProtocol]
            description : str
            name : str
            any_tensor(name: str, description: str) TypeConstraintParam
            any_value(name: str, description: str) TypeConstraintParam
          }
          class TypePair {
            CLS : type
          }
          class TypePromotionRule {
            namespace : str
            op_name : str
            is_valid() bool
            preview_type_promotion(args: tuple, kwargs: dict)* TypePromotionSnapshot
          }
          class TypePromotionSnapshot {
            args_dtypes : Mapping[int, torch.dtype]
            kwargs_dtypes : Mapping[str, torch.dtype]
            out_dtype
          }
          class TypePromotionTable {
            add_rule(rule: TypePromotionRule) None
            get_rule(py_op: torch._ops.OpOverloadPacket) TypePromotionRule | None
          }
          class TypeReflectionMethod {
            forward(x)
          }
          class TypeSafeSerializer {
            default(o)
          }
          class TypeSource {
            guard_source()
            name()
            reconstruct(codegen)
          }
          class TypedExpr {
            dtype
            expr : Union
            is_constant()
          }
          class TypedStorage {
            device
            dtype
            filename
            is_cuda
            is_hpu
            is_sparse : bool
            bfloat16()
            bool()
            byte()
            char()
            clone()
            complex_double()
            complex_float()
            copy_(source: T, non_blocking: _Optional[bool])
            cpu()
            cuda(device, non_blocking) Self
            data_ptr()
            double()
            element_size()
            fill_(value)
            float()
            float8_e4m3fn()
            float8_e4m3fnuz()
            float8_e5m2()
            float8_e5m2fnuz()
            from_buffer()
            from_file(filename, shared, size)
            get_device() _int
            half()
            hpu(device, non_blocking) Self
            int()
            is_pinned(device: Union[str, torch.device])
            is_shared()
            long()
            nbytes()
            pickle_storage_type()
            pin_memory(device: Union[str, torch.device])
            resizable()
            resize_(size)
            share_memory_()
            short()
            size()
            to() Self
            tolist()
            type(dtype: _Optional[str], non_blocking: bool) Union[_StorageBase, TypedStorage, str]
            untyped()
          }
          class TypedStoragePair {
            atol
            rtol
          }
          class TypingVariable {
            value
            as_python_constant()
            call_method(tx, name, args: 'List[VariableTracker]', kwargs: 'Dict[str, VariableTracker]') 'VariableTracker'
            var_getattr(tx: 'InstructionTranslator', name: str)
          }
          class UFuncTypeError {
          }
          class UnBatcherIterDataPipe {
            datapipe
            unbatch_level : int
          }
          class UnaryAttr {
            algorithm_attr : str
            op_name : str
            scalars_attr : list
          }
          class UnaryAttr {
            algorithm_attr : str
            op_name : str
            scalars_attr : list
          }
          class UnaryOpFuzzer {
          }
          class UnaryOpSparseFuzzer {
          }
          class UnaryUfuncInfo {
            domain : tuple
            handles_complex_extremal_values : bool
            handles_large_floats : bool
            reference_numerics_filter : NoneType
            sample_kwargs
            supports_complex_to_float : bool
          }
          class UnbindCatRemover {
            get_simplified_split_ranges(split_sections: List[int], next_users: List[torch.fx.Node], user_inputs_list: List[List[Union[torch.fx.Node, _Range]]]) Optional[List[_Range]]
            get_transform_params(split_node: torch.fx.Node, next_users: List[torch.fx.Node], user_inputs_list: List[List[Union[torch.fx.Node, _Range]]]) Optional[List[List[_TransformParam]]]
            remove_unbind(graph: torch.fx.Graph, unbind_node: torch.fx.Node)
          }
          class UncapturedHigherOrderOpError {
          }
          class Unflatten {
            NamedShape : Tuple
            dim : Union[int, str]
            unflattened_size : Union[_size, NamedShape]
            extra_repr() str
            forward(input: Tensor) Tensor
          }
          class UnflattenedModule {
            adapted : bool
            check_input_constraints : bool
            equality_constraints : List
            flat_args_adapter : Optional[FlatArgsAdapter]
            graph
            graph_signature
            input_placeholders
            ivals
            module_call_graph
            range_constraints
            forward()
            print_readable(print_output, include_stride, include_device, colored)
            process_forward_inputs()
          }
          class Unfold {
            dilation : Union
            kernel_size : Union
            padding : Union
            stride : Union
            extra_repr() str
            forward(input: Tensor) Tensor
          }
          class Uniform {
            arg_constraints : dict
            has_rsample : bool
            high
            low
            mean
            mode
            stddev
            variance
            cdf(value)
            entropy()
            expand(batch_shape, _instance)
            icdf(value)
            log_prob(value)
            rsample(sample_shape: _size) torch.Tensor
            support()
          }
          class UniformQuantizationObserverBase {
            eps
            has_customized_qrange
            qscheme
            quant_max : int
            quant_min : int
            reduce_range : bool
            reset_min_max_vals()*
          }
          class UniformValueConstantFolder {
            constant_data_ptrs : Dict[torch.fx.Node, StorageWeakRef]
            indexing_op_packets
            node_replacements_shapes : Dict[torch.fx.Node, List[int]]
            node_storages_ptrs : Dict[torch.fx.Node, int]
            symint_nodes
            view_op_packets : list
            add_node_replacement(node: torch.fx.Node, tensor: torch.Tensor) None
            insert_placerholder_values(env: Dict[torch.fx.Node, Any]) None
            insertable_tensor_check(t: torch.Tensor) bool
          }
          class UninitializedBuffer {
            cls_to_become
          }
          class UninitializedParameter {
            cls_to_become
          }
          class UninitializedTensorMixin {
            data
            shape
            materialize(shape, device, dtype)
            share_memory_()
          }
          class UnionFind {
            parent : List[Optional[int]]
            size : List[int]
            find(v: int) int
            join(a: int, b: int)
            make_set(v: int)
          }
          class UnitModule {
            l1
            l2
            seq
            forward(x)
          }
          class UnitParamModule {
            l
            p
            seq
            forward(x)
          }
          class UnittestPair {
            CLS : Union[Type, Tuple[Type, ...]]
            TYPE_NAME : Optional[str]
            compare()
          }
          class UnknownVariable {
          }
          class UnpackedDualTensor {
          }
          class Unpickler {
            append
            encoding : str
            memo : Dict[int, Any]
            metastack : list
            proto : int
            read
            readline
            stack : List[Any], list
            load()
            persistent_load(pid)
            pop_mark()
          }
          class UnpicklerWrapper {
            persistent_load
            find_class(mod_name, name)
          }
          class UnpicklerWrapper {
            persistent_load
            find_class(mod_name, name)
          }
          class UnsafeScriptObjectError {
          }
          class UnsatisfiedDependencyError {
            package_name : str
          }
          class Unserializable {
            inner : NoneType
            get()
          }
          class Unset {
            name
          }
          class UnshardHandle {
            wait()
          }
          class UnshardHandle {
            wait() None
          }
          class UnspecializeRestartAnalysis {
          }
          class UnspecializedBuiltinNNModuleSource {
            guard_source()
          }
          class UnspecializedBuiltinNNModuleVariable {
          }
          class UnspecializedNNModuleSource {
            guard_source()
          }
          class UnspecializedNNModuleVariable {
            is_state_mutated : bool
            nn_module_stack_source : NoneType
            value_type
            call_function(tx: 'InstructionTranslator', args: 'List[VariableTracker]', kwargs: 'Dict[str, VariableTracker]') 'VariableTracker'
            call_method(tx, name, args: 'List[VariableTracker]', kwargs: 'Dict[str, VariableTracker]') 'VariableTracker'
            get_nn_module_stack_source()
            getattr_helper(tx: 'InstructionTranslator', field, name_vt)
            manually_trace_nn_module_getattr(tx: 'InstructionTranslator', name)
            set_nn_module_stack_source(source)
            unpack_var_sequence(tx)
            var_getattr(tx: 'InstructionTranslator', name)
          }
          class UnspecializedParamBufferSource {
          }
          class UnspecializedPythonVariable {
            need_unwrap : bool
            raw_value : NoneType
            from_tensor_variable(tensor_variable, raw_value, need_unwrap)
          }
          class Unsupported {
            case_name : Optional[str]
            category : Optional[str]
            msg
            real_stack : StackSummary
            add_to_stats(category)
            remove_from_stats()
          }
          class Unsupported {
          }
          class UnsupportedAliasMutationException {
            reason : str
          }
          class UnsupportedFakeTensorException {
            reason : str
          }
          class UnsupportedFxNodeDiagnostic {
            level : WARNING
            unsupported_fx_node : torch.fx.Node | None
          }
          class UnsupportedFxNodesAnalysis {
            analyze(diagnostic_level: diagnostics.infra.Level) UnsupportedFxNodesAnalysisResult
          }
          class UnsupportedFxNodesAnalysisResult {
            unsupported_op_to_target_mapping : dict[str, dict[str, None]]
          }
          class UnsupportedInputs {
          }
          class UnsupportedNodeError {
          }
          class UnsupportedOperatorError {
          }
          class UnsupportedOperatorException {
            func
          }
          class UnsynchronizedAccessError {
            allocation_stack_trace : Optional[traceback.StackSummary]
            current_access
            data_ptr : int
            previous_access
          }
          class UntypedStorage {
            filename
            is_cuda
            is_hpu
            share_memory_()
          }
          class UntypedStorageVariable {
            example_value
            from_tensor
            call_method(tx, name, args: List[VariableTracker], kwargs: Dict[str, VariableTracker]) VariableTracker
            reconstruct(codegen)
          }
          class UnusedParamModule {
            t0
            t1
            unused_params_rank
            forward(x, rank)
            task_parameters()
          }
          class UnusedParamTwoLinLayerNet {
            a
            b
            c
            forward(x)
          }
          class UnwrapTensorSubclass {
            rebuild_stack : list
            forward() torch.Tensor
            right_inverse(tensor: torch.Tensor) List[torch.Tensor]
          }
          class Upsample {
            align_corners : Optional[bool]
            mode : str
            name : str
            recompute_scale_factor : Optional[bool]
            scale_factor : Optional[_ratio_any_t]
            size : Optional[_size_any_t]
            extra_repr() str
            forward(input: Tensor) Tensor
          }
          class UpsampleBilinear2DDecompSkip {
            new_op_name : str
            new_op_schema : str
            onnxscript_function
            op_callable
            abstract(input, output_size, align_corners, scale_factors)
            register(export_options: torch.onnx.ExportOptions)
            unregister()
          }
          class UpsampleTrilinear3DDecompSkip {
            new_op_name : str
            new_op_schema : str
            onnxscript_function
            op_callable
            abstract(input, output_size, align_corners, scale_factors)
            register(export_options: torch.onnx.ExportOptions)
            unregister()
          }
          class UpsamplingBilinear2d {
          }
          class UpsamplingNearest2d {
          }
          class UserDefinedClassVariable {
            value
            as_proxy()
            as_python_constant()
            call_function(tx: 'InstructionTranslator', args: 'List[VariableTracker]', kwargs: 'Dict[str, VariableTracker]') 'VariableTracker'
            call_hasattr(tx: 'InstructionTranslator', name: str) 'VariableTracker'
            call_method(tx, name, args: 'List[VariableTracker]', kwargs: 'Dict[str, VariableTracker]') 'VariableTracker'
            can_constant_fold_through()
            const_getattr(tx: 'InstructionTranslator', name)
            has_key_in_generic_dict(tx: 'InstructionTranslator', key)
            is_standard_new()
            var_getattr(tx: 'InstructionTranslator', name: str) 'VariableTracker'
          }
          class UserDefinedObjectVariable {
            class_type
            cls_source : NoneType
            has_grad_fn : bool
            value
            value_type
            call_function(tx: 'InstructionTranslator', args: 'List[VariableTracker]', kwargs: 'Dict[str, VariableTracker]') 'VariableTracker'
            call_hasattr(tx: 'InstructionTranslator', name: str) 'VariableTracker'
            call_method(tx, name, args: 'List[VariableTracker]', kwargs: 'Dict[str, VariableTracker]') 'VariableTracker'
            call_torch_function(tx: 'InstructionTranslator', fn, types, args, kwargs)
            get_source_by_walking_mro(name)
            get_torch_fn(tx)
            guard_as_python_constant()
            has_key_in_generic_dict(tx: 'InstructionTranslator', key)
            is_supported_random()
            method_setattr_standard(tx: 'InstructionTranslator', name, value)
            needs_slow_setattr()
            next_variable(tx)
            odict_getitem(tx: 'InstructionTranslator', key)
            python_type()
            torch_function_check()
            unpack_var_sequence(tx)
            var_getattr(tx: 'InstructionTranslator', name)
          }
          class UserDefinedTritonKernel {
            device
            grid
            kernel_idx
            mutable_args
            mutation_outputs
            ordered_kwargs_for_cpp_kernel
            codegen(wrapper) None
            get_device() Optional[torch.device]
            get_kernel_and_metadata()
            get_outputs() List[Buffer]
            get_unbacked_symbol_defs() OrderedSet[sympy.Symbol]
            get_unbacked_symbol_uses() OrderedSet[sympy.Symbol]
          }
          class UserDefinedVariable {
          }
          class UserError {
            error_type
            message
          }
          class UserErrorType {
            name
          }
          class UserFunctionVariable {
            fn : function
            is_constant : bool
            as_python_constant()
            bind_args(parent, args, kwargs) Dict[str, VariableTracker]
            call_function(tx: 'InstructionTranslator', args: 'List[VariableTracker]', kwargs: 'Dict[str, VariableTracker]') 'VariableTracker'
            call_hasattr(tx: 'InstructionTranslator', name: str) VariableTracker
            create_with_source(value, source)
            get_code()
            get_function()
            get_globals()
            has_self()
            python_type()
            self_args()
            var_getattr(tx: 'InstructionTranslator', name: str)
          }
          class UserInputMutation {
            forward(x)
          }
          class UserInputMutationSpec {
            arg : Annotated[TensorArgument, 10]
            user_input_name : Annotated[str, 20]
          }
          class UserInputSpec {
            arg : Annotated[Argument, 10]
          }
          class UserMethodVariable {
            obj
            call_function(tx: 'InstructionTranslator', args: 'List[VariableTracker]', kwargs: 'Dict[str, VariableTracker]') 'VariableTracker'
            inspect_parameter_names()
            python_type()
            self_args()
          }
          class UserOutputSpec {
            arg : Annotated[Argument, 10]
          }
          class VFModule {
            vf : module
          }
          class ValidationException {
            details : str
            msg : str
          }
          class ValueMutationExisting {
            is_modified : bool
          }
          class ValueMutationNew {
          }
          class ValueRangeAnalysis {
            name : str
            bool_handler()
            default_handler()
            index_expr(index, dtype)
            load(name: str, index: sympy.Expr)
            neg(x)
            reduction(name, dtype, src_dtype, reduction_type, index, value)
            square(x)
            store(name, index, value, mode)
            sub(a, b)
            to_dtype(x, dtype: torch.dtype, src_dtype: Optional[torch.dtype])
            truncdiv(a, b)
          }
          class ValueRangeError {
          }
          class ValueRanges {
            AllVR : Union
            BoolVR
            ExprVR
            is_bool : bool
            is_float : bool
            is_int : bool
            lower : _T
            upper : _T
            boolify() ValueRanges[SympyBoolean]
            convex_min_zero_map(x: Union[ExprIn, ExprVR], fn: ExprFn) ExprVR
            coordinatewise_increasing_map(x: Union[ExprIn, ExprVR], y: Union[ExprIn, ExprVR], fn: ExprFn2) ExprVR
            coordinatewise_monotone_map(x, y, fn)
            decreasing_map(x: Union[ExprIn, ExprVR], fn: ExprFn) ExprVR
            increasing_map(x: Union[ExprIn, ExprVR], fn: ExprFn) ExprVR
            is_singleton() bool
            issubset(other)
            monotone_map(x: Union[ExprIn, ExprVR], fn: ExprFn) ExprVR
            tighten(other) ValueRanges
            unknown() ValueRanges[sympy.Expr]
            unknown_bool() ValueRanges[SympyBoolean]
            unknown_int() ValueRanges[sympy.Expr]
            wrap(arg: Union[ExprIn, ExprVR]) ExprVR
          }
          class ValueRangesSLoc {
            lower
            upper
          }
          class Var {
            token : str, tuple
          }
          class VarDispatcher {
          }
          class Variable {
          }
          class VariableBuilder {
            name
            source
            tx
            assert_not_wrapped_by_this_graph(value: torch.Tensor)
            get_source()
            install_guards()
            mark_static_input(value: torch.Tensor, guard: bool)
            wrap_jit_function(value)
            wrap_listlike(value: Union[tuple, list, odict_values, NamedTuple])
            wrap_literal(value)
            wrap_module(value: torch.nn.Module)
            wrap_numpy_ndarray(value)
            wrap_range_iterator(value: range_iterator)
            wrap_regex_pattern(value: re.Pattern)
            wrap_removable_handle(value)
            wrap_slice_range(value: Union[slice, range])
            wrap_symfloat(value)
            wrap_symint(value)
            wrap_tensor(value: torch.Tensor)
            wrap_tuple_iterator(value: tuple_iterator)
            wrap_unspecialized_primitive(value)
            wrap_user_defined(value: Any)
            wrap_weakref(value: weakref.ReferenceType)
          }
          class VariableMeta {
          }
          class VariableTracker {
            mutation_type : NoneType
            source : NoneType
            as_proxy()*
            as_python_constant()*
            build(tx: 'InstructionTranslatorBase', value: Any, source: Optional[Source]) Any
            call_function(tx: 'InstructionTranslator', args: 'List[VariableTracker]', kwargs: 'Dict[str, VariableTracker]') 'VariableTracker'
            call_hasattr(tx: 'InstructionTranslator', name: str) 'VariableTracker'
            call_method(tx, name, args: 'List[VariableTracker]', kwargs: 'Dict[str, VariableTracker]') 'VariableTracker'
            clone()
            const_getattr(tx: 'InstructionTranslator', name: str)* Any
            debug_repr()
            force_unpack_var_sequence(tx) List['VariableTracker']
            guard_as_python_constant()
            has_force_unpack_var_sequence(tx) bool
            has_unpack_var_sequence(tx) bool
            inspect_parameter_names() List[str]
            is_immutable()
            is_mutable()
            is_proxy()
            is_python_constant()
            is_realized()
            is_strict_mode(tx)
            make_guard(fn)
            maybe_fx_node()
            next_variable(tx)
            python_type()
            realize() 'VariableTracker'
            reconstruct(codegen)*
            set_name_hint(name)*
            unpack_var_sequence(tx)* List['VariableTracker']
            unwrap() 'VariableTracker'
            var_getattr(tx: 'InstructionTranslator', name: str) 'VariableTracker'
            visit(fn: Callable[['VariableTracker'], None], value: Any, cache: Optional[Dict[int, Any]]) None
          }
          class VariableTrackerCache {
            cache : dict
            add(value, source, vt)
            clear()
            clone()
            lookup(value, source)
          }
          class VariableTrackerCacheKey {
            source
            vt_id : int
          }
          class VariableTrackerMeta {
            all_subclasses : list
          }
          class Variadic {
          }
          class VariadicSignatureMeta {
          }
          class VariadicSignatureType {
          }
          class VecAMX {
          }
          class VecAVX2 {
          }
          class VecAVX512 {
          }
          class VecISA {
            bit_width() int
            build_arch_flags() str
            build_macro() List[str]
            check_build(code: str) bool
            nelements(dtype: torch.dtype) int
          }
          class VecNEON {
          }
          class VecSVE {
          }
          class VecVSX {
          }
          class VecZVECTOR {
          }
          class VerificationInfo {
            abs_diff_hist : tuple[torch.Tensor, torch.Tensor]
            actual_dtype
            expected_dtype
            max_abs_diff : float
            max_rel_diff : float
            name : str
            rel_diff_hist : tuple[torch.Tensor, torch.Tensor]
          }
          class VerificationOptions {
            acceptable_error_percentage : float | None
            atol : float
            backend
            check_dtype : bool
            check_shape : bool
            flatten : bool
            ignore_none : bool
            remained_onnx_input_idx : Sequence[int] | None
            rtol : float
          }
          class Verifier {
            dialect : str
            allowed_builtin_ops() List
            allowed_getattr_types() Tuple[Type[Any], ...]
            allowed_op_types() Tuple[Type[Any], ...]
            check(ep: 'ExportedProgram') None
            check_additional(gm: GraphModule)* None
            check_valid_op(op)*
          }
          class VerifyStateDictMixin {
          }
          class Version {
            base_version
            dev
            epoch
            is_devrelease
            is_postrelease
            is_prerelease
            local
            major
            micro
            minor
            post
            pre
            public
            release
          }
          class VersionControlDetails {
            as_of_time_utc : Optional[str]
            branch : Optional[str]
            mapped_to : Optional[_artifact_location.ArtifactLocation]
            properties : Optional[_property_bag.PropertyBag]
            repository_uri : str
            revision_id : Optional[str]
            revision_tag : Optional[str]
          }
          class View {
            create(x, new_size)
            dynamic_reshape_indexer(old_size, new_size)
            handle_negative_index(idx, size)
            resolve_negative_size(old_size, new_size)
          }
          class ViewAndMutationMeta {
            aliased_out_indices
            bw_donated_idxs : Optional[List[int]]
            deterministic : Optional[bool]
            dynamic_outputs
            grad_enabled_mutation : Optional[bool]
            indices_of_inputs_that_requires_grad_with_mutations_in_bw : List[int]
            input_info : List[InputAliasInfo]
            is_rng_op_functionalized : bool
            is_train : bool
            keep_input_mutations : bool
            mutated_graph_handled_indices
            mutated_graph_handled_indices_seen_by_autograd
            mutated_inp_runtime_indices
            num_backward_tokens : int
            num_forward
            num_forward_returns
            num_intermediate_bases : int
            num_mutated_graph_handled_indices
            num_mutated_graph_handled_indices_seen_by_autograd
            num_mutated_inp_runtime_indices
            num_outputs
            num_outputs_aliased
            num_outputs_aliased_to_inputs
            num_outputs_aliased_to_intermediates
            num_outputs_non_aliased
            num_outputs_rng_offset : int
            num_symints_saved_for_bw : Optional[int]
            num_unsafe_view_outputs
            output_info : List[OutputAliasInfo]
            output_types
            static_input_indices : List[int]
            subclass_fw_graph_out_meta : List[Union[PlainTensorMeta, SubclassCreationMeta]]
            subclass_inp_meta : List[Union[PlainTensorMeta, SubclassCreationMeta]]
            subclass_tangent_meta : List[Union[PlainTensorMeta, SubclassCreationMeta]]
            symints_saved_for_backwards_slice
            tensors_saved_for_backwards_slice
            tokens : Dict[Any, torch.Tensor]
            traced_tangent_metas : Optional[List[Any]]
            traced_tangents : List[Any]
            unsafe_view_out_indices
            make_runtime_safe()
          }
          class ViewBufferFromNested {
            backward(ctx, gO: torch.Tensor)
            forward(ctx, x: NestedTensor)
          }
          class ViewFunc {
            apply(t: _TensorT, new_base: _TensorT, symint_visitor_fn: Optional[Callable[[int], int]], tensor_visitor_fn: Optional[Callable[[torch.Tensor], _TensorT]])* _TensorT
            from_tensor(t: torch.Tensor) ViewFunc
          }
          class ViewInfo {
            base_index : int
            regenerate_view(bases_list: List[Tensor])*
          }
          class ViewNestedFromBuffer {
            backward(ctx, gO: NestedTensor)
            forward(ctx, values: torch.Tensor, offsets: torch.Tensor, metadata_cache: Optional[Dict[str, Any]])
          }
          class ViewOp {
            args : Tuple[Any, ...]
            kwargs : Dict[str, Any]
            target
          }
          class Virtualized {
          }
          class VmapIncrementNestingCtxManagerVariable {
            create(tx: 'InstructionTranslator', target_values)
            enter(tx)
            exit(tx: 'InstructionTranslator')
          }
          class VmapInfo {
            batch_size : int
            randomness : str
          }
          class VmapInterpreter {
            batch_size()
            get_state()
            process(op, args, kwargs)
            randomness()
          }
          class VonMises {
            arg_constraints : dict
            concentration
            has_rsample : bool
            loc
            mean
            mode
            support
            expand(batch_shape, _instance)
            log_prob(value)
            sample(sample_shape)
            variance()
          }
          class Warning {
            enabled
            getter
            setter
            get_enabled()
            set_enabled(value)
          }
          class WeakDep {
            index
            mutating_buf : str
            name : str
            get_numel() sympy.Expr
            has_unbacked_symbols()
            is_contiguous() bool
            numbytes_hint()
            rename(renames: Dict[str, str]) 'WeakDep'
          }
          class WeakIdKeyDictionary {
            data : dict
            ref_type
            copy()
            get(key, default)
            items()
            keyrefs()
            keys()
            pop(key)
            popitem()
            setdefault(key, default)
            update(dict)
            values()
          }
          class WeakIdRef {
          }
          class WeakRefCallSource {
            guard_source()
            name()
            reconstruct(codegen)
          }
          class WeakRefVariable {
            referent_vt
            build(tx, weakref_value)
            call_function(tx: 'InstructionTranslator', args: 'List[VariableTracker]', kwargs: 'Dict[str, VariableTracker]') 'VariableTracker'
          }
          class WebRequest {
            body : Optional[_artifact_content.ArtifactContent]
            headers : Optional[Any]
            index : int
            method : Optional[str]
            parameters : Optional[Any]
            properties : Optional[_property_bag.PropertyBag]
            protocol : Optional[str]
            target : Optional[str]
            version : Optional[str]
          }
          class WebResponse {
            body : Optional[_artifact_content.ArtifactContent]
            headers : Optional[Any]
            index : int
            no_response_received : Optional[bool]
            properties : Optional[_property_bag.PropertyBag]
            protocol : Optional[str]
            reason_phrase : Optional[str]
            status_code : Optional[int]
            version : Optional[str]
          }
          class Weibull {
            arg_constraints : dict
            concentration
            concentration_reciprocal
            mean
            mode
            scale
            support
            variance
            entropy()
            expand(batch_shape, _instance)
          }
          class WeightNorm {
            dim : int
            name : str
            apply(module, name: str, dim: int) 'WeightNorm'
            compute_weight(module: Module) Any
            remove(module: Module) None
          }
          class WeightNormSparsifier {
            norm_fn : NoneType, int
            update_mask(module, tensor_name, sparsity_level, sparse_block_shape, zeros_per_block)
          }
          class WeightedQuantizedModule {
            from_reference(ref_module, output_scale, output_zero_point)*
          }
          class WeightedRandomSampler {
            generator : NoneType
            num_samples : int
            replacement : bool
            weights
          }
          class WelfordReduction {
            output_index : int
            create(device: torch.device, dtype: torch.dtype, inner_fns: Sequence[Callable[..., Any]], ranges: List[Integer], reduction_ranges: List[Integer], reduction_type: str, reduction_hint: ReductionHint) Sequence[TensorBox]
            create_multilayer(device: torch.device, dtype: torch.dtype, inner_fns: Sequence[Callable[..., Any]], ranges: List[Integer], reduction_ranges: List[Integer], reduction_type: str, split: _IntLike, reduction_hint: ReductionHint) Sequence[TensorBox]
            default_value(reduction_type: str, dtype: torch.dtype) Union[_NumLike, Sequence[_NumLike]]
            store_reduction(output_name: Optional[str], indexer: Callable[[Sequence[Expr]], Never], vars: Sequence[Expr], reduction_vars: Sequence[Symbol]) OpsValue
          }
          class Where {
            nargs : Tuple[int, ...]
            precedence : int
            eval(c: sympy.Basic, p: sympy.Basic, q: sympy.Basic) Optional[sympy.Basic]
          }
          class WhileLoop {
            additional_inputs : Optional[List[TensorBox]]
            body_subgraph : Optional[Subgraph]
            carried_inputs : Optional[List[TensorBox]]
            cond_subgraph : Optional[Subgraph]
            name
            outputs : Optional[List[MultiOutput]]
            codegen(wrapper) None
            create(cond_fn: Subgraph, body_fn: Subgraph, carried_inputs: List[TensorBox], additional_inputs: List[TensorBox])
          }
          class WhileLoopHigherOrderVariable {
            call_function(tx: 'InstructionTranslator', args: List[VariableTracker], kwargs: Dict[str, VariableTracker]) VariableTracker
          }
          class WhileLoopOp {
          }
          class WhyNoFuse {
            args : Tuple[Any, ...]
            node1
            node2
            reason : str
          }
          class Wishart {
            arg_constraints : dict
            covariance_matrix
            df
            has_rsample : bool
            mean
            mode
            precision_matrix
            scale_tril
            support
            variance
            covariance_matrix()
            entropy()
            expand(batch_shape, _instance)
            log_prob(value)
            precision_matrix()
            rsample(sample_shape: _size, max_try_correction) torch.Tensor
            scale_tril()
          }
          class WithEffects {
          }
          class WithExitFunctionVariable {
            ctx : Union[ContextWrappingVariable, GenericContextWrappingVariable]
            target
            call_function(tx: 'InstructionTranslator', args: 'List[VariableTracker]', kwargs: 'Dict[str, VariableTracker]') 'VariableTracker'
            reconstruct(codegen)
          }
          class WithItemBuilder {
            build_withitem(ctx, item)
          }
          class Work {
            event
            wait(timeout: timedelta) bool
          }
          class WorkSharing {
            code
            in_parallel : bool
            num_threads : NoneType
            stack : ExitStack
            close()
            parallel(threads)
            single()
          }
          class Worker {
            current_device()* int
            get_device_properties(device: _device_t)*
            set_device(device: int)*
          }
          class Worker {
            current_device() int
            get_device_properties(device: _device_t)
            set_device(device: int)
          }
          class Worker {
            current_device() int
            get_device_properties(device: _device_t)
            set_device(device: int)
          }
          class Worker {
            get_device_properties(device: _device_t)
          }
          class Worker {
            global_rank : int
            id : Optional[Any]
            local_rank : int
            role_rank : int
            role_world_size : int
            world_size : int
          }
          class WorkerGroup {
            group_rank : NoneType
            group_world_size : NoneType
            master_addr : NoneType
            master_port : NoneType
            spec
            state : FAILED, INIT
            store : NoneType
            workers
          }
          class WorkerInfo {
            dataset : str
            id : int
            num_workers : int
            seed : int
          }
          class WorkerSpec {
            args : Tuple
            entrypoint : Optional[Union[Callable, str, None]]
            fn : Optional[Callable]
            local_addr : Optional[str]
            local_world_size : int
            master_addr : Optional[str]
            master_port : Optional[int]
            max_restarts : int
            monitor_interval : float
            rdzv_handler
            role : str
            get_entrypoint_name()
          }
          class WorkerState {
            name
            is_running(state: 'WorkerState') bool
          }
          class WorkspaceArg {
            count : Expr
            device
            dtype
            get_device_or_error
            get_output_spec
            inner_name : str
            layout
            maybe_get_layout
            maybe_get_output_spec
            outer_name : str
            zero_mode
            can_join(a, b) bool
            get_device()
            get_dtype()
            get_inputs_that_alias_output()
            get_layout()
            get_name()
            get_size()
            get_stride()
            join(a, b)
            maximum(a, b)
            unique_name(prefix)
          }
          class WorkspaceZeroMode {
            name
            combine(a, b)
            from_bool(zero_fill)
          }
          class WorldData {
            default_pg
            group_count : int
            pg_backend_config : Dict[dist.ProcessGroup, str]
            pg_coalesce_state : Dict[dist.ProcessGroup, List[Union[_CollOp, P2POp]]]
            pg_group_ranks : Dict[dist.ProcessGroup, Dict[int, int]]
            pg_map : Dict[dist.ProcessGroup, Tuple[str, Optional[Store]]]
            pg_names : Dict[dist.ProcessGroup, str]
            pg_to_tag : Dict[dist.ProcessGroup, str]
            tags_to_pg : Dict[str, List[dist.ProcessGroup]]
          }
          class WorldMetaClassVariable {
            is_group_member_type(value)
            var_getattr(tx: 'InstructionTranslator', name: str) VariableTracker
          }
          class Wrap {
          }
          class WrapActivationCheckpoint {
          }
          class WrapBackendDebug {
            get_compiler_config
          }
          class WrapHigherOrderVariable {
            call_function(tx: 'InstructionTranslator', args: 'List[VariableTracker]', kwargs: 'Dict[str, VariableTracker]') 'VariableTracker'
            create_wrapped_node(tx: 'InstructionTranslator', fn_vt, fn_args_vt, kwargs, description, under_activation_checkpoint)
            install_subgraph_in_output_graph(tx, fn_vt, fn_args_vt, kwargs, body_gmod, attr_name)
          }
          class WrapWithAutocast {
          }
          class WrapWithAutocastHigherOrderVariable {
            call_function(tx: 'InstructionTranslator', args: 'List[VariableTracker]', kwargs: 'Dict[str, VariableTracker]') 'VariableTracker'
          }
          class WrapWithSetGradEnabled {
          }
          class WrapWithSetGradEnabledHigherOrderVariable {
            call_function(tx: 'InstructionTranslator', args: 'List[VariableTracker]', kwargs: 'Dict[str, VariableTracker]') 'VariableTracker'
          }
          class WrappedCtx {
          }
          class WrappedFunction {
            constants : Tuple[torch.Tensor, ...]
            id
            model : Callable[..., Any]
            mutated_input_idxs : Sequence[int]
            placeholders : Sequence[PlaceholderInfo]
            static_input_idxs : Sequence[int]
          }
          class WrappedModel {
            model
            forward()
          }
          class WrappedUserFunctionVariable {
            context
            wrapped
            call_function(tx: 'InstructionTranslator', args: 'List[VariableTracker]', kwargs: 'Dict[str, VariableTracker]') 'VariableTracker'
          }
          class WrappedUserMethodVariable {
            context
            wrapped
            call_function(tx: 'InstructionTranslator', args: 'List[VariableTracker]', kwargs: 'Dict[str, VariableTracker]') 'VariableTracker'
          }
          class Wrapper {
            forward()
          }
          class WrapperBackend {
            backend : Callable
            candidate
            gm
            restore
          }
          class WrapperHandler {
          }
          class WrapperLine {
          }
          class WrapperModule {
            submod
            unwrap_singleton_tuple
            forward()
          }
          class WrapperModule {
            model
            forward()
            gradients(ctx_id)
          }
          class WrapperSubclass {
            a
          }
          class WrapperTensor {
            get_wrapper_properties()*
          }
          class WrapperTensorWithCustomSizes {
            t
            get_wrapper_properties(t, requires_grad)
          }
          class WrapperTensorWithCustomStrides {
            t
            get_wrapper_properties(t, requires_grad)
          }
          class WrapperUserFunctionVariable {
            attr_to_trace
            wrapper_obj
            call_function(tx: 'InstructionTranslator', args: 'List[VariableTracker]', kwargs: 'Dict[str, VariableTracker]') 'VariableTracker'
            var_getattr(tx: 'InstructionTranslator', name)
          }
          class WriteItem {
            index
            tensor_data : Optional[TensorWriteData]
            type
            tensor_storage_size() Optional[int]
          }
          class WriteItemType {
            name
          }
          class WriteResult {
            index
            size_in_bytes : int
            storage_data : Any
          }
          class WriterInterp {
            subdir
            run_node(n)
          }
          class X86InductorQuantizer {
            global_config : Optional[QuantizationConfig]
            module_function_to_aten_operator_type : dict
            module_name_qconfig : Dict[str, Optional[QuantizationConfig]]
            operator_type_qconfig : Dict[torch._ops.OpOverloadPacket, Optional[QuantizationConfig]]
            annotate(model: torch.fx.GraphModule) torch.fx.GraphModule
            get_global_quantization_config()
            set_function_type_qconfig(function_type: Callable, quantization_config: Optional[QuantizationConfig]) 'X86InductorQuantizer'
            set_global(quantization_config: QuantizationConfig)
            set_module_name_qconfig(module_name: str, quantization_config: Optional[QuantizationConfig])
            set_module_type_qconfig(module_type: torch.nn.Module, quantization_config: Optional[QuantizationConfig]) 'X86InductorQuantizer'
            validate(model: torch.fx.GraphModule)* None
          }
          class XFailRule {
            error_msg : str
            error_type : TypeVar
            type
            get_context(test_case)
          }
          class XMLTestResultVerbose {
            addSkip(test, reason)
            printErrors() None
          }
          class XNNPACKEngine {
            enabled
            m
          }
          class XNNPACKQuantizer {
            DYNAMIC_OPS : list
            STATIC_OPS : list
            STATIC_QAT_ONLY_OPS : list
            global_config : Optional[QuantizationConfig]
            module_name_config : Dict[str, Optional[QuantizationConfig]]
            module_type_config : Dict[Callable, Optional[QuantizationConfig]]
            operator_type_config : Dict[torch._ops.OpOverloadPacket, Optional[QuantizationConfig]]
            supported_config_and_operators : list
            annotate(model: torch.fx.GraphModule) torch.fx.GraphModule
            get_supported_operator_for_quantization_config(quantization_config: Optional[QuantizationConfig]) List[OperatorPatternType]
            get_supported_operators() List[OperatorConfig]
            get_supported_quantization_configs() List[QuantizationConfig]
            set_global(quantization_config: QuantizationConfig) XNNPACKQuantizer
            set_module_name(module_name: str, quantization_config: Optional[QuantizationConfig])
            set_module_type(module_type: Callable, quantization_config: QuantizationConfig)
            set_operator_type(operator_type: torch._ops.OpOverloadPacket, quantization_config: QuantizationConfig) XNNPACKQuantizer
            transform_for_annotation(model: torch.fx.GraphModule) torch.fx.GraphModule
            validate(model: torch.fx.GraphModule)* None
          }
          class XPUDeviceOpOverrides {
            abi_compatible_header()
            aoti_get_stream()
            cpp_aoti_device_guard()
            cpp_aoti_stream_guard()
            cpp_device_guard()
            cpp_device_ptr()
            cpp_getStreamFromExternal()
            cpp_kernel_type()
            cpp_stream_guard()
            cpp_stream_type()
            device_guard(device_idx)
            import_get_raw_stream_as(name)
            kernel_driver()
            kernel_header()
            set_device(device_idx)
            synchronize()
          }
          class XPUInductorQuantizer {
          }
          class XPUTestBase {
            device_type : str
            primary_device : ClassVar[str]
            get_all_devices()
            get_primary_device()
            setUpClass()
          }
          class XpuInterface {
            Event
            Stream
            current_device : staticmethod
            current_stream : staticmethod
            device
            device_count : staticmethod
            exchange_device : staticmethod
            get_device_properties : staticmethod
            get_raw_stream : staticmethod
            maybe_exchange_device : staticmethod
            memory_allocated : staticmethod
            set_device : staticmethod
            set_stream : staticmethod
            stream : staticmethod
            synchronize : staticmethod
            get_compute_capability(device: _device_t)
            is_available() bool
            is_bf16_supported(including_emulation: bool) bool
          }
          class ZeroGradientsGenVmap {
            generate_vmap_rule : bool
            backward(ctx, gx, gy)
            forward(x, y)
            jvp(ctx, gx, gy)
            setup_context(ctx, inputs, outputs)*
          }
          class ZeroPad1d {
            padding : Tuple[int, int]
            extra_repr() str
          }
          class ZeroPad2d {
            padding : Tuple[int, int, int, int]
            extra_repr() str
          }
          class ZeroPad3d {
            padding : Tuple[int, int, int, int, int, int]
            extra_repr() str
          }
          class ZeroRedundancyOptimizer {
            functional_optim_map : Any
            global_rank : int
            initialized : bool
            join_device
            optim : Any
            parameters_as_bucket_view : bool
            process_group : Any
            rank : int
            world_size : int
            add_param_group(param_group: dict[str, Any]) None
            consolidate_state_dict(to: int) None
            join_hook()
            join_process_group() Any
            load_state_dict(state_dict: dict[str, Any]) None
            state_dict() dict[str, Any]
            step(closure: None) None
          }
          class ZipVariable {
            index : int
            iterables : List[Union[List[VariableTracker], VariableTracker]]
            strict : bool
            has_unpack_var_sequence(tx) bool
            next_variable(tx)
            python_type()
            reconstruct(codegen)
            reconstruct_items(codegen)
            unpack_var_sequence(tx) List['VariableTracker']
          }
          class ZipperIterDataPipe {
            datapipes : Tuple[IterDataPipe]
          }
          class ZipperMapDataPipe {
            datapipes : Tuple[MapDataPipe[_T_co], ...]
          }
          class _Action {
            name
          }
          class _Action {
            computation_type
            microbatch_index : Optional[int]
            stage_index : int
            from_str(action_string: str)
          }
          class _AdaptiveAvgPoolNd {
            output_size : Union
            extra_repr() str
          }
          class _AdaptiveMaxPoolNd {
            output_size : Union
            return_indices : bool
            extra_repr() str
          }
          class _AddRuntimeAssertionsForInlineConstraintsPass {
            counter : int
            existing_inline_assertions : dict
            range_constraints : Dict[sympy.Symbol, ValueRanges]
            call(graph_module) PassResult
          }
          class _AllGather {
            backward(ctx)
            forward(ctx, group, tensor)
          }
          class _AllGatherBase {
            backward(ctx, grad_output)
            forward(ctx, output_tensor, input_tensor, group)
          }
          class _AllGatherMatch {
            ag_node
            gather_dim : int
            group_name : str
            match
            res_node
            shard_node
            erase() None
            replace_with(new_node: torch.fx.Node) None
          }
          class _AllGatherRotater {
            exchange_buffers(curr_buffer: torch.Tensor) None
            next_buffer() torch.Tensor
          }
          class _AllReduce {
            backward(ctx, grad_output)
            forward(ctx, op, group, tensor)
          }
          class _AllToAllRotater {
            exchange_buffers(curr_buffer: torch.Tensor) None
            next_buffer() torch.Tensor
          }
          class _AllowMutationOnSavedContext {
            cloned : MutableMapping[_Handle, torch.Tensor]
            original : MutableMapping[_Handle, torch.Tensor]
            sid_to_tid : Dict[_SID, Set[_TID]]
            tid_to_weakhandle : MutableMapping[_TID, _Handle]
            clear() None
          }
          class _AllreduceUpcastHookState {
            ddp_weakref : Any
            upcast_stream
            wait_for_stream_enqueued : bool
          }
          class _AlltoAll {
            backward(ctx)
            forward(ctx, group, out_tensor_list)
          }
          class _AlltoAllSingle {
            backward(ctx, grad_output)
            forward(ctx, group, output, output_split_sizes, input_split_sizes, input)
          }
          class _Anchors {
            left_end_lineno : int
            left_end_offset : int
            right_start_lineno : int
            right_start_offset : int
          }
          class _AssertRaisesRegexWithHighlightContext {
            exception_type
            highlight
            regex
            test_case
          }
          class _Asterisk {
          }
          class _AttentionContextParallel {
          }
          class _AttentionOp {
          }
          class _AttrKind {
            name
          }
          class _AttrProxy {
            reset_proxy_mapping(base: Module, path: str)* None
          }
          class _AutotuneCacheBundlerImpl {
            end_compile() None
            put(basename: str, data: JsonDataTy) None
            sync()* None
          }
          class _AvgPoolNd {
            extra_repr() str
          }
          class _Await {
          }
          class _BackendRendezvousStateHolder {
            state
            mark_dirty() None
            sync() Optional[bool]
          }
          class _BaseDataLoaderIter {
          }
          class _BaseDatasetFetcher {
            auto_collation
            collate_fn
            dataset
            drop_last
            fetch(possibly_batched_index)*
          }
          class _BaseVersion {
          }
          class _BatchNorm {
            bias
            running_mean
            running_var
            scale
            weight
            zero_point
            from_float(cls, mod, use_precomputed_fake_quant)
            from_reference(bn, output_scale, output_zero_point)
          }
          class _BatchNorm {
            forward(input: Tensor) Tensor
          }
          class _Boolean {
            is_discrete : bool
            check(value)
          }
          class _Broadcast {
            backward(ctx, grad_output)
            forward(ctx, src, group, tensor)
          }
          class _BufferCommHook {
            buffer_comm_hook : Callable
            buffer_comm_hook_location
            buffer_comm_hook_state : Any
          }
          class _BufferCommHookLocation {
            name
          }
          class _BufferMeta {
          }
          class _BypassDispatchCache {
            reason : str
          }
          class _CPUinfo {
            cpuinfo : list
            logical_core_node_map : dict
            node_logical_cores : List[List[int]]
            node_nums
            node_physical_cores : List[List[int]]
            physical_core_node_map : dict
            get_all_logical_cores()
            get_all_physical_cores()
            get_node_logical_cores(node_id)
            get_node_physical_cores(node_id)
            numa_aware_check(core_list)
          }
          class _CUDAAllocator {
            allocator()
          }
          class _CacheKeyState {
            shape_env : Optional[ShapeEnv]
            sym_node_lookup : Dict[int, int]
            cache_on_shape_env() bool
            convert_output(arg: _MetadataIntLike) _MetadataIntLike
            convert_sym_int(result: List[object], arg: SymInt) None
          }
          class _CacheStat {
            exception : int
            hit : int
            miss : int
            put : int
          }
          class _CacheStats {
            exception(name: str, count: int) None
            get(name: str, value: Optional[object]) None
            hit(name: str, count: int) None
            miss(name: str, count: int) None
            put(name: str, count: int) None
          }
          class _CachedForward {
          }
          class _CachedTorchDispatchMode {
            allow_cache_entry_mutation
            policy_fn
            storage
          }
          class _CachingTorchDispatchMode {
            policy_fn
            storage
          }
          class _CanonicalizeC2rReturn {
            dim : Tuple[int, ...]
            last_dim_size : int
            shape : Tuple[int, ...]
          }
          class _Cat {
            cseq : list
            dim : int
            event_dim
            is_discrete
            lengths : list
            check(value)
          }
          class _CausalBehavior {
            name
          }
          class _CheckpointFrame {
            early_stop
            forward_completed : bool
            ignore_saved_mismatch : bool
            input_saver : NoneType
            is_recomputed : DefaultDict[int, bool]
            metadata_fn
            recomp_counter : DefaultDict[int, int]
            recompute_fn
            recomputed : DefaultDict[int, weakref.WeakKeyDictionary[_Handle, torch.Tensor]]
            unpack_error_cb
            weak_holders : List[ReferenceType]
            x_metadatas : list
            check_recomputed_tensors_match(gid)
          }
          class _Checkpointable {
          }
          class _Checkpointer {
            coordinator_rank : int
            load_planner : NoneType
            no_dist : bool
            process_group : NoneType
            save_planner : NoneType
            storage_reader
            storage_writer
            async_save(state_dict: STATE_DICT_TYPE) Future
            load(state_dict: Dict[str, Any]) None
            save(state_dict: STATE_DICT_TYPE) Metadata
          }
          class _ChildDataPipe {
            instance_id : int
            main_datapipe
          }
          class _CircularPadNd {
            padding : Sequence[int]
            extra_repr() str
            forward(input: Tensor) Tensor
          }
          class _ClassNamespace {
            name
          }
          class _ClassPropertyDescriptor {
            fget
          }
          class _Classes {
            loaded_libraries
            load_library(path)
          }
          class _CloneArgBeforeMutateMode {
            ctx : str
          }
          class _CoalescingManager {
            works : List[Work]
            append(work: Work)
            wait()
          }
          class _CodeOnlyModule {
          }
          class _CodeParser {
            function_body
            function_name
            function_params
            return_type
            template_params
          }
          class _CollOp {
            dst_tensor : Optional[torch.Tensor]
            op : Callable
            redop : Optional[ReduceOp]
            root : Optional[int]
            tensor
          }
          class _CollectiveKernel {
            cpp_kernel_name
            ordered_kwargs_for_cpp_kernel
            outputs : list
            create_inplace(kernel, inputs: Union[TensorBox, List[TensorBox]]) None
            create_out_of_place(kernel, inputs: Union[TensorBox, List[TensorBox]])
            has_side_effects() bool
            set_cpp_kernel_name(cpp_kernel_name: Optional[str]) None
            should_allocate() bool
          }
          class _Column {
            get_results_for(group)
            num_to_str(value: Optional[float], estimated_sigfigs: int, spread: Optional[float])
          }
          class _CommModeModuleTracker {
            activation_checkpointing : bool
            module_helper_dict : dict
            module_parameters_dict : dict
            module_parents_dict : dict
            name : str
            parent_dict : dict
            parent_list : list
            register_forward_hook_handles : dict
            sharding_dict : dict
            print_paramater_info()
            print_sharding_info()
          }
          class _Commit {
            additions : Dict[str, Any]
            base : Dict[str, Any]
            checksum_head : Optional[str]
            checksum_next : str
            cpp_header : str
            cpp_header_path : str
            result : Dict[str, Any]
            subtractions : Dict[str, Any]
            thrift_checksum_head : Optional[str]
            thrift_checksum_next : str
            thrift_checksum_real : Optional[str]
            thrift_schema : str
            thrift_schema_path : str
            yaml_path : str
          }
          class _CompileFxCallable {
          }
          class _CompileFxKwargs {
            aot_mode : bool
            boxed_forward_device_index : Optional[BoxedDeviceIndex]
            cpp_wrapper : bool
            cudagraphs : Optional[BoxedBool]
            extern_node_serializer : Optional[Callable[[List[ExternKernelNode]], Any]]
            graph_id : Optional[int]
            is_backward : bool
            is_inference : bool
            layout_opt : Optional[bool]
            static_input_idxs : Sequence[int]
          }
          class _Comptime {
            assert_static(val)
            breakpoint()
            force_static(val)
            graph_break()
            print(e)
            print_bt()
            print_disas()
            print_graph()
            print_guards()
            print_locals()
            print_value_stack()
            print_value_stack_and_return(e)
            sleep(sec)
          }
          class _ComputationType {
            name
            from_str(action)
          }
          class _ConfigAutoWrap {
            in_autowrap_context : bool
            kwargs : Dict[str, Any]
            wrapper_cls : Optional[Callable]
            disable_autowrap_context() None
            enable_autowrap_context(kwargs: Any) None
          }
          class _ConfigEntry {
            default : Any
            env_value_default : Any
            env_value_force : Any
            hide : bool
            justknob : Optional[str]
            user_override : Any
            value_type : type
          }
          class _ConsolidatedOptimState {
            non_tensor_state : Dict[str, Any]
            tensor_state : Dict[str, torch.Tensor]
            zero_dim_tensor_state : Dict[str, torch.Tensor]
          }
          class _ConstantPadNd {
            padding : Sequence[int]
            value : float
            extra_repr() str
            forward(input: Tensor) Tensor
          }
          class _Constraint {
            constraint_range : str
            name : str
            serializable_spec
          }
          class _ConstraintTarget {
            dim : int
            t_id : int
          }
          class _Container {
          }
          class _ContainerTemplate {
            get_length_by_instance(instance_id: int)*
            get_next_element_by_instance(instance_id: int)*
            is_every_instance_exhausted()* bool
            reset()* None
          }
          class _Context {
            args : Tuple[Any, ...]
            function : Callable[..., None]
            interval : float
            kwargs : Dict[str, Any]
            stop_event : Event
          }
          class _ContextParallelOptions {
            convert_to_f32 : bool
            enable_load_balance : bool
            rotate_method
          }
          class _ConvBnNd {
            bias
            bn
            freeze_bn : bool
            qconfig : NoneType
            training : bool
            weight
            weight_fake_quant
            extra_repr()
            forward(input)
            freeze_bn_stats()
            from_float(mod, use_precomputed_fake_quant)
            reset_bn_parameters()
            reset_parameters()
            reset_running_stats()
            to_float()
            train(mode)
            update_bn_stats()
          }
          class _ConvNd {
            from_float(cls, float_conv, weight_qparams)
          }
          class _ConvNd {
            dilation
            groups
            in_channels
            kernel_size
            out_channels
            output_padding
            padding
            padding_mode : str
            scale : float
            stride
            training
            transposed
            zero_point : int
            bias()*
            extra_repr()
            from_float(cls, mod, use_precomputed_fake_quant)
            from_reference(ref_qconv, output_scale, output_zero_point)
            get_qconv(mod, activation_post_process, weight_post_process)
            set_weight_bias(qweight, bias_float)*
          }
          class _ConvNd {
            qconfig : NoneType
            weight_fake_quant
            forward(input)
            from_float(cls, mod, use_precomputed_fake_quant)
            to_float()
          }
          class _ConvNd {
            bias : Optional[Tensor]
            dilation : Tuple[int, ...]
            groups : int
            in_channels : int
            kernel_size : Tuple[int, ...]
            out_channels : int
            output_padding : Tuple[int, ...]
            padding : Union[str, Tuple[int, ...]]
            padding_mode : str
            stride : Tuple[int, ...]
            transposed : bool
            weight
            extra_repr()
            reset_parameters() None
          }
          class _ConvTransposeMixin {
          }
          class _ConvTransposeNd {
            from_float(cls, float_conv, weight_qparams)
          }
          class _ConvTransposeNd {
            scale : float
            zero_point : int
            from_float(mod, use_precomputed_fake_quant)
            from_reference(cls, ref_qconvt, output_scale, output_zero_point)
          }
          class _ConvTransposeNd {
          }
          class _CorrCholesky {
            event_dim : int
            check(value)
          }
          class _CudaBase {
            is_cuda : bool
            is_sparse : bool
            type()
          }
          class _CudaLegacyStorage {
            from_buffer()
          }
          class _CurrentQuantizationMode {
            dynamic_state : Optional[bool]
            qat_state : Optional[bool]
          }
          class _CustomBuiltin {
            import_str : str
            obj : Any
          }
          class _CustomReducer {
            init_value
            reduce_fn
          }
          class _CustomViewFunc {
            func : Callable[[torch.Tensor, Optional[Callable[[int], int]], Optional[Callable[[torch.Tensor], _TensorT]]], _TensorT]
            apply(t: torch.Tensor, new_base: torch.Tensor, symint_visitor_fn: Optional[Callable[[int], int]], tensor_visitor_fn: Optional[Callable[[torch.Tensor], _TensorT]]) _TensorT
          }
          class _DDPBucketAssignment {
            bucket_index : int
            device
            offset : int
            parameters : list[torch.Tensor]
            tensor : torch.Tensor | None
          }
          class _DDPJoinHook {
            ddp
            main_hook()
            post_hook(is_last_joiner: bool)
          }
          class _DDPSink {
            backward(ctx)
            forward(ctx, ddp_weakref)
          }
          class _DataPipeMeta {
            type
          }
          class _DataPipeSerializationWrapper {
          }
          class _DataPipeType {
            param
            issubtype(other)
            issubtype_of_instance(other)
          }
          class _DatasetKind {
            Iterable : int
            Map : int
            create_fetcher(kind, dataset, auto_collation, collate_fn, drop_last)
          }
          class _DeconstructedSymNode {
            constant : Optional[Union[int, float, bool]]
            fx_node
            pytype : type
            extract(shape_env: ShapeEnv) SymNode
            from_node(node: SymNode) _DeconstructedSymNode
          }
          class _DeconstructedSymType {
            node
            ty : Type[PySymType]
            extract(shape_env: ShapeEnv) PySymType
            from_sym_type(value: PySymType) _DeconstructedSymType
          }
          class _DecoratorContextManager {
            clone()
          }
          class _DemultiplexerIterDataPipe {
            buffer_size : int
            child_buffers : List[Deque[_T_co]]
            classifier_fn : Callable[[_T_co], Optional[int]]
            current_buffer_usage : int
            drop_none : bool
            main_datapipe : IterDataPipe[_T_co]
            main_datapipe_exhausted : bool
            num_instances : int
            get_length_by_instance(instance_id: int) int
            get_next_element_by_instance(instance_id: int)
            is_every_instance_exhausted() bool
            reset() None
          }
          class _DependencyViewer {
            downstreams : defaultdict
            upstreams : defaultdict
            downstreams_of(node: Node) Set[Node]
            upstreams_of(node: Node) Set[Node]
          }
          class _Dependent {
            event_dim
            is_discrete
            check(x)
          }
          class _DependentProperty {
          }
          class _DerivedConstraint {
            constraint_range : str
            fn : Callable
            name : str
            root : Union[_ConstraintTarget, _PhantomRoot]
            serializable_spec
          }
          class _DerivedDim {
            max
            min
          }
          class _DerivedObserverOrFakeQuantize {
            ch_axis : Optional[int]
            derive_qparams_fn : Callable[[List[ObserverOrFakeQuantize]], Tuple[Tensor, Tensor]]
            obs_or_fqs : List[ObserverOrFakeQuantize]
            qscheme : Optional[torch.qscheme]
            quant_max : Optional[int]
            quant_min : Optional[int]
            calculate_qparams()
            forward(x: Tensor) Tensor
          }
          class _Deterministic {
            fill_uninitialized_memory
          }
          class _DeviceGuard {
            idx : int
            prev_idx : int
          }
          class _DeviceGuard {
            idx : int
            prev_idx : int
          }
          class _DeviceMeshStub {
          }
          class _DictMock {
            mock_value
          }
          class _Dim {
            readable(name, min_, max_)
          }
          class _DimHint {
            name
          }
          class _DimRange {
            dim : int
            max : int
            min : int
          }
          class _Dirichlet {
            backward(ctx, grad_output)
            forward(ctx, concentration)
          }
          class _DispatchCacheEntry {
            is_output_tuple : bool
            output_infos : Tuple[_DispatchCacheEntryOutputInfo]
          }
          class _DispatchCacheEntryOutputInfo {
            inplace_idx : Optional[int]
            metadata : Optional[TensorMetadata]
            view_idx : Optional[int]
          }
          class _DispatchCacheKey {
            hashvalue : int
            key : Tuple[object, ...]
            strip_shape_env() None
          }
          class _DistTestBase {
            call_dist_op(profiling_title_postfix, is_async, op)
            test_1_level_hierarchical_model_averager_equivalent_to_periodic_model_averager()
            test_3_level_hierarchical_model_averager()
            test_Backend_enum_class()
            test_DistributedDataParallel()
            test_DistributedDataParallelCPU()
            test_DistributedDataParallelCPU_grad_is_view()
            test_DistributedDataParallel_SyncBatchNorm()
            test_DistributedDataParallel_SyncBatchNorm_2D_Input()
            test_DistributedDataParallel_SyncBatchNorm_Channels_Last()
            test_DistributedDataParallel_SyncBatchNorm_Diff_Input_Sizes_Running_Value()
            test_DistributedDataParallel_SyncBatchNorm_Diff_Input_Sizes_gradient()
            test_DistributedDataParallel_SyncBatchNorm_No_Affine()
            test_DistributedDataParallel_SyncBatchNorm_Single_Input_Per_Process()
            test_DistributedDataParallel_SyncBatchNorm_half()
            test_DistributedDataParallel_non_default_stream()
            test_DistributedDataParallel_requires_grad()
            test_DistributedDataParallel_with_amp_and_grad_is_view()
            test_DistributedSampler_padding()
            test_SyncBatchNorm_process_group()
            test_accumulate_gradients_no_sync()
            test_accumulate_gradients_no_sync_allreduce_hook()
            test_accumulate_gradients_no_sync_allreduce_with_then_hook()
            test_accumulate_gradients_no_sync_grad_is_view()
            test_all_gather()
            test_all_gather_coalesced_complex()
            test_all_gather_coalesced_full_group()
            test_all_gather_coalesced_group()
            test_all_gather_coalesced_simple()
            test_all_gather_coalesced_with_empty()
            test_all_gather_complex()
            test_all_gather_cuda()
            test_all_gather_cuda_complex()
            test_all_gather_full_group()
            test_all_gather_group()
            test_all_gather_into_cat_tensor_cuda()
            test_all_gather_into_stack_tensor_cuda()
            test_all_gather_object_default_pg()
            test_all_gather_object_subgroup()
            test_all_gather_v_cuda()
            test_all_reduce_coalesced_full_group_max()
            test_all_reduce_coalesced_full_group_min()
            test_all_reduce_coalesced_full_group_product()
            test_all_reduce_coalesced_full_group_sum()
            test_all_reduce_coalesced_group_max()
            test_all_reduce_coalesced_group_min()
            test_all_reduce_coalesced_group_product()
            test_all_reduce_coalesced_group_sum()
            test_all_reduce_coalesced_max()
            test_all_reduce_coalesced_max_complex_unsupported()
            test_all_reduce_coalesced_min()
            test_all_reduce_coalesced_product()
            test_all_reduce_coalesced_sum()
            test_all_reduce_complex_unsupported_ops()
            test_all_reduce_full_group_max()
            test_all_reduce_full_group_min()
            test_all_reduce_full_group_product()
            test_all_reduce_full_group_sum()
            test_all_reduce_group_max()
            test_all_reduce_group_min()
            test_all_reduce_group_product()
            test_all_reduce_group_sum()
            test_all_reduce_max()
            test_all_reduce_min()
            test_all_reduce_product()
            test_all_reduce_sum()
            test_all_reduce_sum_async()
            test_all_reduce_sum_complex()
            test_all_reduce_sum_cuda()
            test_all_reduce_sum_cuda_async()
            test_all_reduce_sum_cuda_complex()
            test_all_to_all()
            test_all_to_all_complex()
            test_all_to_all_cuda()
            test_all_to_all_cuda_complex()
            test_all_to_all_full_group()
            test_all_to_all_full_group_cuda()
            test_all_to_all_group()
            test_all_to_all_group_cuda()
            test_all_to_all_single_equal_split()
            test_all_to_all_single_equal_split_complex()
            test_all_to_all_single_equal_split_cuda()
            test_all_to_all_single_equal_split_cuda_complex()
            test_all_to_all_single_equal_split_full_group()
            test_all_to_all_single_equal_split_full_group_cuda()
            test_all_to_all_single_equal_split_group()
            test_all_to_all_single_equal_split_group_cuda()
            test_all_to_all_single_unequal_split()
            test_all_to_all_single_unequal_split_complex()
            test_all_to_all_single_unequal_split_cuda()
            test_all_to_all_single_unequal_split_cuda_complex()
            test_all_to_all_single_unequal_split_full_group()
            test_all_to_all_single_unequal_split_full_group_cuda()
            test_all_to_all_single_unequal_split_group()
            test_all_to_all_single_unequal_split_group_cuda()
            test_average_parameters()
            test_barrier()
            test_barrier_cuda()
            test_barrier_full_group()
            test_barrier_full_group_cuda()
            test_barrier_group()
            test_barrier_group_cuda()
            test_barrier_timeout_full_group()
            test_barrier_timeout_global()
            test_barrier_timeout_group()
            test_batch_isend_irecv_gloo()
            test_batch_isend_irecv_gloo_tags()
            test_batch_isend_irecv_mixed_backend_err()
            test_batch_isend_irecv_nccl()
            test_batch_isend_irecv_no_rank_zero_nccl()
            test_batch_isend_irecv_op_err()
            test_batch_isend_irecv_op_list_err()
            test_batch_isend_irecv_ring_exchange_nccl()
            test_batch_isend_irecv_self_nccl()
            test_broadcast()
            test_broadcast_cuda()
            test_broadcast_full_group()
            test_broadcast_group()
            test_broadcast_object_list()
            test_coalescing_manager()
            test_coalescing_manager_async()
            test_compute_bucket_assignment_by_size_sparse_error_with_logger()
            test_compute_bucket_assignment_by_size_sparse_error_without_logger()
            test_ddp_apply_optim_in_backward()
            test_ddp_apply_optim_in_backward_grad_as_bucket_view_false()
            test_ddp_apply_optim_in_backward_ignored_params()
            test_ddp_broadcast_buffer()
            test_ddp_broadcast_buffer_via_hook()
            test_ddp_buffer_hook_allreduce()
            test_ddp_buffer_hook_allreduce_return_future()
            test_ddp_build_debug_param_to_name_mapping()
            test_ddp_build_debug_param_to_name_mapping_requires_grad()
            test_ddp_comm_hook_logging()
            test_ddp_compile_static_graph()
            test_ddp_control_flow_different_across_ranks()
            test_ddp_control_flow_same_across_ranks()
            test_ddp_create_graph()
            test_ddp_device()
            test_ddp_device_mesh_initialization()
            test_ddp_forward_backward_hook()
            test_ddp_grad_div_uneven_inputs()
            test_ddp_has_finalized()
            test_ddp_hook_parity_allreduce()
            test_ddp_hook_parity_allreduce_process_group()
            test_ddp_hook_parity_post_localSGD()
            test_ddp_hook_parity_powerSGD()
            test_ddp_hook_pickling_powerSGD()
            test_ddp_ignore_params_arg()
            test_ddp_inference()
            test_ddp_join_model_equivalence()
            test_ddp_logging_data_cpu()
            test_ddp_logging_data_gpu()
            test_ddp_model_diff_num_params_across_ranks()
            test_ddp_model_diff_shape_across_ranks()
            test_ddp_multiple_nested_unused_params_err_ignore_params()
            test_ddp_multiple_nested_unused_params_error()
            test_ddp_namedtuple()
            test_ddp_native_mixed_precision_grad_as_bucket_view_no_set_grad_none()
            test_ddp_native_mixed_precision_grad_as_bucket_view_set_grad_to_none()
            test_ddp_native_mixed_precision_ignored_params()
            test_ddp_native_mixed_precision_no_grad_as_bucket_view_no_set_grad_none()
            test_ddp_native_mixed_precision_no_grad_as_bucket_view_set_grad_to_none()
            test_ddp_new_tensor_in_fwd()
            test_ddp_new_tensor_in_fwd_static_graph()
            test_ddp_profiling_autograd_profiler()
            test_ddp_profiling_execution_trace()
            test_ddp_profiling_torch_profiler()
            test_ddp_python_error_logged()
            test_ddp_remove_autograd_hooks()
            test_ddp_returns_tensor_with_no_grad()
            test_ddp_shared_grad_acc_unused_params()
            test_ddp_sink_noclone()
            test_ddp_static_graph_nested_types()
            test_ddp_sync_bn_training_vs_eval()
            test_ddp_sync_module_states()
            test_ddp_uneven_input_exception()
            test_ddp_uneven_input_join_disable()
            test_ddp_uneven_inputs()
            test_ddp_uneven_inputs_stop_iteration_sync_bn()
            test_ddp_unused_params_rebuild_buckets_exception()
            test_ddp_zero_output_features()
            test_destroy_full_group()
            test_destroy_group()
            test_detect_ddp_is_actually_static()
            test_different_graph_across_ranks()
            test_dump_DDP_relevant_env_vars()
            test_gather()
            test_gather_checks()
            test_gather_cuda()
            test_gather_full_group()
            test_gather_group()
            test_gather_object()
            test_gather_object_subgroup()
            test_get_backend()
            test_get_data_parallel_params()
            test_get_future()
            test_get_rank()
            test_get_rank_size_full_group()
            test_get_rank_size_group()
            test_grads_same_across_ranks_with_no_sync()
            test_invalid_static_graph()
            test_irecv()
            test_isend()
            test_isend_autograd_profiler()
            test_isend_torch_profiler()
            test_monitored_barrier_allreduce_hang()
            test_monitored_barrier_allreduce_hang_wait_all_ranks()
            test_monitored_barrier_failure_order()
            test_monitored_barrier_gloo()
            test_monitored_barrier_gloo_rank_0_timeout()
            test_monitored_barrier_gloo_subgroup()
            test_monitored_barrier_wait_all_ranks()
            test_nccl_backend_bool_allgather()
            test_nccl_backend_bool_allreduce()
            test_nccl_backend_bool_broadcast()
            test_nccl_backend_bool_reduce()
            test_nccl_high_priority_stream()
            test_new_subgroups()
            test_new_subgroups_by_enumeration()
            test_new_subgroups_by_enumeration_input_rank_exceeds_world_size()
            test_new_subgroups_by_enumeration_negative_input_rank()
            test_new_subgroups_group_size_exceeds_world_size()
            test_new_subgroups_overlap_not_allowed()
            test_new_subgroups_world_size_not_divisible_by_group_size()
            test_output_unused_in_loss_dict_module()
            test_output_unused_in_loss_tuple_module()
            test_periodic_model_averager()
            test_periodic_model_averager_param_group()
            test_post_localSGD_optimizer_parity()
            test_post_localSGD_optimizer_parity_grad_is_view()
            test_post_localSGD_optimizer_parity_with_hierarchical_sgd()
            test_post_localSGD_optimizer_parity_with_hierarchical_sgd_grad_is_view()
            test_post_localSGD_optimizer_step_reload()
            test_reduce_full_group_max()
            test_reduce_full_group_min()
            test_reduce_full_group_product()
            test_reduce_full_group_sum()
            test_reduce_group_max()
            test_reduce_group_min()
            test_reduce_group_product()
            test_reduce_group_sum()
            test_reduce_max()
            test_reduce_min()
            test_reduce_product()
            test_reduce_scatter_tensor_cuda()
            test_reduce_scatter_v_cuda()
            test_reduce_sum()
            test_reduce_sum_cuda()
            test_reduce_sum_cuda_twice()
            test_reduce_sum_twice()
            test_scatter()
            test_scatter_checks()
            test_scatter_complex()
            test_scatter_cuda()
            test_scatter_cuda_complex()
            test_scatter_full_group()
            test_scatter_group()
            test_scatter_object_list()
            test_send_recv()
            test_send_recv_any_source()
            test_send_recv_any_source_autograd_profiler()
            test_send_recv_any_source_torch_profiler()
            test_send_recv_autograd_profiler()
            test_send_recv_nccl()
            test_send_recv_nccl_autograd_profiler()
            test_send_recv_nccl_torch_profiler()
            test_send_recv_torch_profiler()
            test_send_recv_with_tag()
            test_send_recv_with_tag_autograd_profiler()
            test_send_recv_with_tag_torch_profiler()
            test_sparse_all_reduce_sum()
            test_sparse_all_reduce_sum_cuda()
            test_stateless_api_with_ddp()
            test_static_graph_api_cpu()
            test_static_graph_multi_forward()
            test_sync_bn_logged()
            test_undefined_grad_parity_unused_parameters()
            test_verify_model_across_rank_with_logger()
            test_verify_model_across_rank_without_logger()
            validate_net_equivalence(net)
          }
          class _DistWrapper {
            coordinator_rank : int
            group : Optional[dist.ProcessGroup]
            is_coordinator : bool
            rank : int
            use_dist : bool
            all_gather(step: str, map_fun: Callable[[], T]) List[T]
            all_gather_object(object: T) List[T]
            all_reduce(step: str, map_fun: Callable[[], T], reduce_fun: Callable[[List[T]], R]) R
            broadcast(step: str, map_fun: Callable[[], T]) T
            broadcast_object(object: Optional[T]) T
            gather_object(object: T) Optional[List[T]]
            get_rank() int
            get_world_size() int
            reduce_scatter(step: str, map_fun: Callable[[], T], reduce_fun: Callable[[List[T]], List[R]]) R
            scatter_object(object_list: Optional[List[T]]) T
          }
          class _DistributedPdb {
            interaction()
          }
          class _DistributedRendezvousOpExecutor {
            run(state_handler: Callable[[_RendezvousContext, float], _Action], deadline: float, update_deadline: Optional[Callable[[timedelta], float]]) None
          }
          class _DropoutNd {
            inplace : bool
            p : float
            extra_repr() str
          }
          class _Dummy {
            nop()*
          }
          class _DummyLeaf {
          }
          class _DummyLeaf {
          }
          class _DummyLeaf {
          }
          class _DynType {
          }
          class _EXTRA_STATE {
          }
          class _EffectType {
            name
          }
          class _Empty {
          }
          class _EmptyStateDictLoadPlanner {
            keys : NoneType
            set_up_planner(state_dict: STATE_DICT_TYPE, metadata: Optional[Metadata], is_coordinator: bool) None
          }
          class _EvalCacheLoader {
            eval_cache : dict
            next_id : int
            cache(src: str, globals: Dict[str, Any], co_fields)
            get_source(module_name) Optional[str]
          }
          class _EventBase {
          }
          class _ExecOrderData {
            all_handles : List[FlatParamHandle]
            current_order_index : int
            handles_post_forward_order : List[Optional[FlatParamHandle]]
            handles_pre_forward_order : List[FlatParamHandle]
            is_first_iter
            param_to_fqn : Dict[nn.Parameter, List[str]]
            process_group : Optional[dist.ProcessGroup]
            rank
            warn_status : NONE, WARNED, WARNING
            world_size : Optional[int]
            get_handle_to_backward_prefetch(current_handle: FlatParamHandle) Optional[FlatParamHandle]
            get_handle_to_forward_prefetch(current_handle: FlatParamHandle) Optional[FlatParamHandle]
            init(state: _FSDPState, root_module: nn.Module, process_group: dist.ProcessGroup) None
            next_iter()
            record_post_forward(handle: Optional[FlatParamHandle]) None
            record_pre_forward(handle: Optional[FlatParamHandle], is_training: bool) None
          }
          class _ExecOrderTracer {
            exec_info : Optional[_ExecutionInfo]
            patch_tracer(tracer: torch.fx.Tracer, root_module: nn.Module)
          }
          class _ExecOrderWarnStatus {
            name
          }
          class _ExecutionInfo {
            curr_module
            module_forward_order : List[nn.Module]
            module_to_param_usage_infos : Dict[nn.Module, List[_ParamUsageInfo]]
            param_forward_order : List[nn.Parameter]
            visited_params : Set[nn.Parameter]
          }
          class _ExportOutcome {
            name
          }
          class _ExportPassBaseDeprecatedDoNotUse {
            fake_tensor_mode : NoneType, Optional[FakeTensorMode]
            interpreter
            node_debug_str : Optional[typing.Optional[str]]
            tracer
            call(graph_module: fx.GraphModule) PassResult
            call_cond(pred: ProxyValue, true_fn: torch.fx.GraphModule, false_fn: torch.fx.GraphModule, inputs: List[Argument], meta: NodeMetadata) ProxyValue
            call_getitem(value: ProxyValue, key: int, meta: NodeMetadata) ProxyValue
            call_map(f: torch.fx.GraphModule, mapped_args: List[ProxyValue], operands: List[ProxyValue], meta: NodeMetadata) ProxyValue
            call_operator(op, args: Tuple[Argument, ...], kwargs: Dict[str, Argument], meta: NodeMetadata) ProxyValue
            call_submodule(graph_module: fx.GraphModule, inputs: Tuple[Argument, ...]) PassResult
            call_sym(target: Fn, args: Tuple[Argument, ...], meta: NodeMetadata) ProxyValue
            inputs(graph_module: torch.fx.GraphModule) List[Argument]
            on_attr(attr: ProxyValue)* None
            output(results: List[Argument], meta: NodeMetadata) ProxyValue
            placeholder(name: str, arg: Argument, meta: NodeMetadata) ProxyValue
          }
          class _ExportType {
            name
          }
          class _ExternNode {
          }
          class _ExtractModuleReferences {
            package
            references : dict
            run(src: str, package: str) List[Tuple[str, Optional[str]]]
            visit_Call(node)
            visit_Import(node)
            visit_ImportFrom(node)
          }
          class _FC2 {
            fc
            forward(x)
          }
          class _FSDPDeviceHandle {
            from_device(device: torch.device) '_FSDPDeviceHandle'
          }
          class _FSDPModMemStats {
            local_peak : Dict[torch.device, int]
            mod_fqn : str
            snapshots : Dict[_FSDPModState, List[Dict[torch.device, Dict[str, int]]]]
          }
          class _FSDPModState {
            name
          }
          class _FSDPRefType {
            name
          }
          class _FSDPState {
            compute_device : Optional[torch.device]
            process_group : Optional[dist.ProcessGroup]
            rank : int
            sharding_strategy : FULL_SHARD
            training_state : IDLE
            world_size : int
          }
          class _FakeGlobalNamespace {
          }
          class _FakeTensorViewFunc {
            apply(t: torch.Tensor, new_base: torch.Tensor, symint_visitor_fn: Optional[Callable[[int], int]], tensor_visitor_fn: Optional[Callable[[torch.Tensor], FakeTensor]]) FakeTensor
          }
          class _Faketqdm {
            disable : bool
            n : int
            total : NoneType
            close()
            set_description()*
            update(n)
            write(s)
          }
          class _FileSystemWriter {
            checkpoint_id
            fs
            metadata_path
            overwrite : bool
            path : NoneType, Path
            per_thread_copy_ahead : int
            save_id : str
            single_file_per_rank : bool
            sync_files : bool
            thread_count : int
            finish(metadata: Metadata, results: List[List[WriteResult]]) None
            prepare_global_plan(plans: List[SavePlan]) List[SavePlan]
            prepare_local_plan(plan: SavePlan) SavePlan
            reset(checkpoint_id: Union[str, os.PathLike, None]) None
            set_up_storage_writer(is_coordinator: bool)* None
            storage_meta() Optional[StorageMeta]
            validate_checkpoint_id(checkpoint_id: Union[str, os.PathLike]) bool
            write_data(plan: SavePlan, planner: SavePlanner) Future[List[WriteResult]]
          }
          class _FindNodesLookupTable {
            table : Dict[Tuple[str, Optional[Target]], Dict[Node, None]]
            find_nodes()
            insert(node: Node) None
            remove(node: Node) None
          }
          class _FindOperatorOverloadsInOnnxRegistry {
            format(level: infra.Level, node) Tuple[infra.Rule, infra.Level, str]
            format_message(node) str
          }
          class _FindOpschemaMatchedSymbolicFunction {
            format(level: infra.Level, symbolic_fn, node) Tuple[infra.Rule, infra.Level, str]
            format_message(symbolic_fn, node) str
          }
          class _FlatParameterMeta {
          }
          class _FlopCounterMode {
            counter
          }
          class _ForkerIterDataPipe {
            buffer : Deque, deque
            buffer_size : int
            child_pointers : List[int]
            copy_fn
            end_ptr : NoneType, Optional[int]
            leading_ptr : int
            main_datapipe
            num_instances : int
            slowest_ptr : int
            get_length_by_instance(instance_id: int) int
            get_next_element_by_instance(instance_id: int)
            is_every_instance_exhausted() bool
            reset() None
          }
          class _Formatter {
            floating_dtype
            int_mode : bool
            max_width : int
            sci_mode : bool
            format(value)
            width()
          }
          class _FreeEventQueue {
            dequeue_if_needed() Optional[torch.Event]
            enqueue(free_event: torch.Event) None
          }
          class _FromTorchTensor {
            backward(ctx, grad_output: torch.Tensor) torch.Tensor
            forward(ctx, input: torch.Tensor) torch.Tensor
          }
          class _FromTorchTensor {
            backward(ctx, grad_output: 'DTensor')
            forward(ctx, input: torch.Tensor, device_mesh: DeviceMesh, placements: Tuple[Placement, ...], run_check: bool, shape: Optional[torch.Size], stride: Optional[Tuple[int, ...]]) 'DTensor'
          }
          class _FunctionalAdadelta {
            defaults : dict
            foreach : bool
            maximize : bool
            param_group : dict
            state : dict
            step(gradients: List[Optional[Tensor]])
          }
          class _FunctionalAdagrad {
            coalesce_grad : bool
            defaults : dict
            foreach : bool
            fused : bool
            maximize : bool
            param_group : dict
            state : dict
            step(gradients: List[Optional[Tensor]])
          }
          class _FunctionalAdam {
            amsgrad : bool
            defaults : dict
            foreach : bool
            fused : bool
            maximize : bool
            param_group : dict
            state : dict
            step(gradients: List[Optional[Tensor]])
            step_param(param: Tensor, grad: Optional[Tensor])
          }
          class _FunctionalAdamW {
            amsgrad : bool
            defaults : dict
            foreach : bool
            fused : bool
            maximize : bool
            param_group : dict
            state : dict
            step(gradients: List[Optional[Tensor]])
            step_param(param: Tensor, grad: Optional[Tensor])
          }
          class _FunctionalAdamax {
            defaults : dict
            foreach : bool
            maximize : bool
            param_group : dict
            state : dict
            step(gradients: List[Optional[Tensor]])
          }
          class _FunctionalRMSprop {
            centered : bool
            defaults : dict
            foreach : bool
            maximize : bool
            param_group : dict
            state : dict
            step(gradients: List[Optional[Tensor]])
          }
          class _FunctionalRprop {
            defaults : dict
            etas : Tuple[float, float]
            foreach : bool
            maximize : bool
            param_group : dict
            state : dict
            step_sizes : Tuple[float, float]
            step(gradients: List[Optional[Tensor]])
          }
          class _FunctionalSGD {
            defaults : dict
            foreach : bool
            fused : bool
            maximize : bool
            nesterov : bool
            param_group : dict
            state : dict
            step(gradients: List[Optional[Tensor]])
            step_param(param: Tensor, grad: Optional[Tensor])
          }
          class _FunctionalizationMetadataProp {
            multi_output_view_nodes : dict
            node_counter : int
            propagate()
            run_node(node: Node)
          }
          class _FunctionalizeSideEffectfulOpsPass {
            call(graph_module: torch.fx.GraphModule) PassResult
            call_operator(op: OpOverload, args: Tuple[Argument, ...], kwargs: Dict[str, Argument], meta: NodeMetadata) ProxyValue
            output(results: List[Argument], meta: NodeMetadata) ProxyValue
          }
          class _FusedModule {
          }
          class _FxGraphToOnnx {
            format(level: infra.Level, graph_name) Tuple[infra.Rule, infra.Level, str]
            format_message(graph_name) str
          }
          class _FxNodeInsertTypePromotion {
            format(level: infra.Level, target) Tuple[infra.Rule, infra.Level, str]
            format_message(target) str
          }
          class _FxNodeToOnnx {
            format(level: infra.Level, node_repr) Tuple[infra.Rule, infra.Level, str]
            format_message(node_repr) str
          }
          class _FxPass {
            format(level: infra.Level, pass_name) Tuple[infra.Rule, infra.Level, str]
            format_message(pass_name) str
          }
          class _Gather {
            backward(ctx)
            forward(ctx, dst, group, tensor)
          }
          class _GdsFile {
            fd
            filename : str
            flags : int
            handle : NoneType, Optional[int]
            deregister_handle() None
            load_storage(storage: Storage, offset: int) None
            register_handle() None
            save_storage(storage: Storage, offset: int) None
          }
          class _GeneralMultiDeviceReplicator {
            master
          }
          class _GlobalItemStats {
            cache : Dict[str, object]
            reset() None
          }
          class _GlobalStats {
            aot_autograd
            autotune_local
            autotune_remote
            bundled_autotune
            dynamo_pgo
            fx_graph
            triton
            get_stat(name: str) _GlobalItemStats
            report()
            reset() None
          }
          class _GraphAppendingTracerEx {
            enable_thunkify : bool
            script_object_tracker : MutableMapping[_AnyScriptObjectType, Proxy]
            symnode_tracker : MutableMapping[PySymType, _PySymProxyType]
            sympy_expr_tracker : Dict[sympy.Symbol, object]
            tensor_tracker : MutableMapping[Tensor, _ProxyTensor]
            torch_fn_counts : Dict[OpOverload, int]
            torch_fn_metadata : Optional[OpOverload]
          }
          class _GraphDiff {
            graph_a
            graph_b
            diff_report() str
          }
          class _GreaterThan {
            lower_bound
            check(value)
          }
          class _GreaterThanEq {
            lower_bound
            check(value)
          }
          class _HalfOpenInterval {
            lower_bound
            upper_bound
            check(value)
          }
          class _Handle {
          }
          class _Handle {
          }
          class _HasMeta {
            meta : Dict[str, PySymType]
          }
          class _HasStorage {
            storage()
          }
          class _HashableTracker {
            underlying_value
            vt
          }
          class _Holder {
            handles : Dict[int, Optional[_Handle]]
          }
          class _HookMixin {
          }
          class _INITIAL_MISSING {
          }
          class _IRNode {
            stack_meta
            stack_trace
          }
          class _ITTStub {
            mark
            rangePop
            rangePush
            is_available()
          }
          class _ITraceObserver {
            cleanup()*
            start()*
            stop()*
          }
          class _IVals {
            fqns : defaultdict
            storage : dict
            create(partitions, root_module)
            read(fqn, graph, node)
            update(fqn, graph, node)
          }
          class _IgnoreContextManager {
          }
          class _IllegalWork {
          }
          class _InProcessFxCompile {
            codegen_and_compile(gm: GraphModule, example_inputs: Sequence[InputType], inputs_to_check: Sequence[int], graph_kwargs: _CompileFxKwargs) OutputCode
          }
          class _IncompatibleKeys {
          }
          class _IndependentConstraint {
            base_constraint
            event_dim
            is_discrete
            reinterpreted_batch_ndims
            check(value)
          }
          class _InfiniteConstantSampler {
          }
          class _InputBackref {
            value : int
          }
          class _InputEqualizationObserver {
            dtype
            equalization_scale
            equalization_shape : List[int]
            input_obs
            qscheme
            with_args : classmethod
            calculate_scaled_minmax()
            forward(x_orig)
            get_input_minmax()
            set_equalization_scale(equalization_scale)
          }
          class _InsertPoint {
            graph
            orig_insert
          }
          class _InsertPoint {
            g
            guard : NoneType
            insert_point : Union[torch._C.Node, torch._C.Block]
            prev_insert_point
          }
          class _InstanceNorm {
            forward(input: Tensor) Tensor
          }
          class _IntegerGreaterThan {
            is_discrete : bool
            lower_bound
            check(value)
          }
          class _IntegerInterval {
            is_discrete : bool
            lower_bound
            upper_bound
            check(value)
          }
          class _IntegerLessThan {
            is_discrete : bool
            upper_bound
            check(value)
          }
          class _InternalGlobals {
            autograd_inlining
            export_onnx_opset_version
            export_training : bool
            in_onnx_export
            onnx_shape_inference : bool
            operator_export_type
            training_mode
          }
          class _InternalRPCPickler {
            deserialize(binary_data, tensor_table)
            serialize(obj)
          }
          class _Interval {
            lower_bound
            upper_bound
            check(value)
          }
          class _InverseTransform {
            bijective
            inv
            sign
            codomain()
            domain()
            forward_shape(shape)
            inverse_shape(shape)
            log_abs_det_jacobian(x, y)
            with_cache(cache_size)
          }
          class _IterDataPipeMeta {
          }
          class _IterDataPipeSerializationWrapper {
          }
          class _IterableDatasetFetcher {
            dataset_iter
            ended : bool
            fetch(possibly_batched_index)
          }
          class _IterableDatasetStopIteration {
            worker_id : int
          }
          class _JittedFunction {
            code_string : str
            is_cuda_available : bool
            kernel_name
            kwargs_dict : dict
            num_outputs : int
            return_by_ref : bool
          }
          class _JoinConfig {
            enable : bool
            is_first_joinable : bool
            throw_on_early_termination : bool
            construct_disabled_join_config()
          }
          class _KinetoProfile {
            acc_events : bool
            activities
            custom_trace_id_callback : NoneType
            execution_trace_observer : NoneType
            experimental_config : NoneType
            mem_tl : Optional[MemoryProfileTimeline]
            preset_metadata : Dict[str, str]
            profile_memory : bool
            profiler : Optional[prof.profile]
            record_shapes : bool
            use_device : NoneType, str
            with_flops : bool
            with_modules : bool
            with_stack : bool
            add_metadata(key: str, value: str)
            add_metadata_json(key: str, value: str)
            events()
            export_chrome_trace(path: str)
            export_memory_timeline(path: str, device: Optional[str]) None
            export_stacks(path: str, metric: str)
            key_averages(group_by_input_shape: bool, group_by_stack_n: int)
            prepare_trace()
            preset_metadata_json(key: str, value: str)
            start()
            start_trace()
            stop()
            stop_trace()
            toggle_collection_dynamic(enable: bool, activities: Iterable[ProfilerActivity])
          }
          class _LPPoolNd {
            ceil_mode : bool
            kernel_size : Union
            norm_type : float
            stride : Optional[_size_any_t]
            extra_repr() str
          }
          class _LRScheduler {
          }
          class _LSTMLayer {
            batch_first : bool
            bidirectional : bool
            layer_bw
            layer_fw
            qconfig
            forward(x: Tensor, hidden: Optional[Tuple[Tensor, Tensor]])
            from_float(other, layer_idx, qconfig)
          }
          class _LSTMSingleLayer {
            cell
            forward(x: Tensor, hidden: Optional[Tuple[Tensor, Tensor]])
            from_params()
          }
          class _Launcher {
            cpuinfo
            msg_lib_notfound : str
            add_lib_preload(lib_type)
            is_numactl_available()
            launch(args)
            log_env_var(env_var_name)
            set_env(env_name, env_value)
            set_memory_allocator(enable_tcmalloc, enable_jemalloc, use_default_allocator)
            set_multi_thread_and_allocator(ncores_per_instance, disable_iomp, set_kmp_affinity, enable_tcmalloc, enable_jemalloc, use_default_allocator)
          }
          class _LazyConvXdMixin {
            bias
            groups : int
            in_channels : int
            kernel_size : Tuple[int, ...]
            out_channels : int
            transposed : bool
            weight
            initialize_parameters(input: Tensor) None
            reset_parameters() None
          }
          class _LazyGraphModule {
            code
            forward
            force_recompile(gm)
            from_graphmodule(gm: GraphModule)
            real_recompile()
            recompile()
          }
          class _LazyModule {
          }
          class _LazyNormBase {
            affine : bool
            bias
            num_batches_tracked
            num_features
            running_mean
            running_var
            track_running_stats : bool
            weight
            initialize_parameters(input) None
            reset_parameters() None
          }
          class _LazyProtocol {
            register_forward_pre_hook(hook)
          }
          class _LazySeedTracker {
            call_order : list
            manual_seed_all_cb : NoneType, tuple
            manual_seed_cb : NoneType, tuple
            get_calls() List
            queue_seed(cb, traceback)
            queue_seed_all(cb, traceback)
          }
          class _LeafNode {
            fx_node
            fx_op
            stack_meta
            stack_trace
          }
          class _LearnableFakeQuantize {
            activation_post_process
            bitwidth : int
            ch_axis : int
            dtype
            qscheme
            quant_max : int
            quant_min : int
            scale
            use_grad_scaling : bool
            zero_point
            calculate_qparams()
            enable_observer(enabled)
            enable_param_learning()
            enable_static_estimate()
            enable_static_observation()
            forward(X)
            observe_quant_params()
            toggle_fake_quant(enabled)
            toggle_observer_update(enabled)
            toggle_qparam_learning(enabled)
          }
          class _LegacyStorage {
          }
          class _LegacyStorageMeta {
            dtype
          }
          class _LessThan {
            upper_bound
            check(value)
          }
          class _LinearNodeList {
            serialize_node_list : list
            to_graph()
          }
          class _LoadBalancer {
            shard(buffer: torch.Tensor, mesh: DeviceMesh, seq_dim: int)* torch.Tensor
            unshard(buffer: torch.Tensor, mesh: DeviceMesh, seq_dim: int)* torch.Tensor
          }
          class _LocalAutotuneCacheBackend {
          }
          class _LocalOptimizer {
            global_lock : lock
            optim
            step(autograd_ctx_id)
          }
          class _Loss {
            reduction : str
          }
          class _LossReducer {
          }
          class _LowerCholesky {
            event_dim : int
            check(value)
          }
          class _LowerTriangular {
            event_dim : int
            check(value)
          }
          class _MEM_FORMAT_ENCODING {
            name
          }
          class _MakefxTracer {
            decomposition_table : Dict[OpOverload, Callable]
            fake_tensor_mode : NoneType, Optional[FakeTensorMode]
            fx_tracer : Optional[PythonKeyTracer]
            pre_dispatch : bool
            proxy_function_mode : Union[nullcontext, PreDispatchTorchFunctionMode]
            proxy_mode : Union[nullcontext, ProxyTorchDispatchMode]
            python_dispatcher_mode : Union[nullcontext, Any]
            record_module_stack : bool
            torch_fn_metadata_mode : Union[nullcontext, TorchFunctionMetadataMode]
            tracing_mode : str
            trace(f: Callable) fx.GraphModule
            trace_subgraph(f: Callable) GraphModule
          }
          class _MapDataPipeSerializationWrapper {
          }
          class _MapDatasetFetcher {
            fetch(possibly_batched_index)
          }
          class _MaskPartial {
            mask_buffer
            offset_dim : int
            offset_shape : Optional[torch.Size]
          }
          class _MaskedContiguous {
            backward(ctx, grad_output)
            forward(ctx, input)
          }
          class _MaskedToDense {
            backward(ctx, grad_output)
            forward(ctx, input)
          }
          class _MaskedToSparse {
            backward(ctx, grad_output)
            forward(ctx, input)
          }
          class _MaskedToSparseCsr {
            backward(ctx, grad_output)
            forward(ctx, input)
          }
          class _MaskedWhere {
            backward(ctx, grad_output)
            forward(ctx, cond, self, other)
          }
          class _Match {
            types : tuple
          }
          class _Matmul {
            A_node
            B_node
            arg_ancestor_nodes : OrderedSet[torch.fx.Node]
            nodes : List[torch.fx.Node]
            erase() None
            from_match(match: List[torch.fx.Node]) '_Matmul'
            replace_with(new_node: torch.fx.Node) None
          }
          class _MaxPoolNd {
            ceil_mode : bool
            dilation : Union
            kernel_size : Union
            padding : Union
            return_indices : bool
            stride : NoneType
            extra_repr() str
          }
          class _MaxUnpoolNd {
            extra_repr() str
          }
          class _MemRefType {
            name
          }
          class _MeshEnv {
            child_to_root_mapping : Dict[DeviceMesh, DeviceMesh]
            flatten_name_to_root_dims : Dict[DeviceMesh, Dict[str, Tuple[int, ...]]]
            mesh_dim_group_options : Dict[int, Tuple[str, Optional[C10dBackend.Options]]]
            mesh_stack : List[DeviceMesh]
            root_to_flatten_mapping : Dict[DeviceMesh, Dict[str, DeviceMesh]]
            create_flatten_mesh(device_mesh: 'DeviceMesh', mesh_dim_name: Optional[str]) 'DeviceMesh'
            create_sub_mesh(device_mesh: 'DeviceMesh', submesh_dim_names: Tuple[str, ...], submesh_dims: List[Tuple[int, ...]]) 'DeviceMesh'
            get_current_mesh() 'DeviceMesh'
            get_mesh_dim_by_name(device_mesh: 'DeviceMesh', mesh_dim_name: str) int
            get_root_mesh(device_mesh: 'DeviceMesh') 'DeviceMesh'
            get_root_mesh_dim(device_mesh: 'DeviceMesh') Optional[int]
            num_devices_per_host(device_type: str) int
            num_hosts(device_type: str) int
          }
          class _MetaTensorCallback {
          }
          class _MetaTensorCallbackKwargs {
            device : Union[torch.device, str]
          }
          class _MinimizerBase {
            a_outputs : Dict[str, Any]
            b_outputs : Dict[str, Any]
            compare_fn : Callable[[TensorOrTensors, TensorOrTensors, Names], Tuple[float, bool]]
            exclusion_fn : Optional[Callable[[NodeList, int, int], None]]
            fusions : dict
            iteration : int
            module
            module_exporter : Optional[Callable[[Tensors, torch.fx.GraphModule, str], None]]
            reports : List[List[str]]
            results : Dict[Any, Any]
            sample_input : Union
            settings
            minimize(start: Optional[str], end: Optional[str], skip_nodes: Optional[List], find_last_node: Optional[bool]) NodeSet
            print_report(report: List[str])
            print_reports()
            run_a(mod: torch.fx.GraphModule, inputs: Tensors, report_idx: int) TensorOrTensors
            run_b(mod: torch.fx.GraphModule, inputs: Tensors, report_idx: int) TensorOrTensors
            run_nodes(start: Optional[str], end: Optional[str])
          }
          class _MinimizerSettingBase {
            accumulate_error : bool
            all_outputs : bool
            find_all : bool
            return_intermediate : bool
            traverse_method : str
          }
          class _MissingCustomSymbolicFunction {
            format(level: infra.Level, op_name) Tuple[infra.Rule, infra.Level, str]
            format_message(op_name) str
          }
          class _MissingStandardSymbolicFunction {
            format(level: infra.Level, op_name, opset_version, issue_url) Tuple[infra.Rule, infra.Level, str]
            format_message(op_name, opset_version, issue_url) str
          }
          class _MixedPrecision {
            buffer_dtype : Optional[torch.dtype]
            param_dtype : Optional[torch.dtype]
            reduce_dtype : Optional[torch.dtype]
          }
          class _MkldnnConvNd {
            dilation
            groups
            padding
            stride
            forward(x)
          }
          class _ModMemStats {
            buffer_mem : int
            input_mem : int
            local_peak : Dict[torch.device, int]
            mod_fqn : str
            output_mem : int
            parameter_mem : int
            snapshots : Dict[_ModState, List[Dict[torch.device, Dict[str, int]]]]
          }
          class _ModState {
            name
          }
          class _ModeStackStateForPreDispatch {
            count()
            get(index)
            set(index, mode)
          }
          class _ModificationType {
            name
          }
          class _ModuleFrame {
            child_fqn : str
            created_modules
            flat_graph
            fqn
            graph
            ivals : dict
            module : Union[torch.fx.GraphModule, UnflattenedModule, InterpreterModule]
            module_call_graph : Dict[str, ModuleCallSignature]
            module_id
            module_stack : List[Tuple[str, int]]
            node_map : Dict[torch.fx.Node, torch.fx.Node]
            node_to_placeholder : dict
            nodes : Tuple[torch.fx.Node, ...]
            parent
            parent_call_module : Optional[torch.fx.Node]
            seen_attrs
            seen_modules
            seen_nodes
            verbose : bool
            add_placeholder(x)
            copy_node(node)
            copy_sym_call_function(x)
            finalize_outputs()
            print()
            remap_input(x)
            run_from(node_idx)
            run_outer()
          }
          class _ModuleMeta {
            module_class_name
            module_display_name
            module_name
            qualified_module_class_name
            raw_meta
            create_root() _ModuleMeta
            from_dynamo_produced_raw_meta(raw_meta: _DYNAMO_NN_MODULE_META_TYPE) _ModuleMeta
            from_fx_tracer_produced_raw_meta(raw_meta: _FX_TRACER_NN_MODULE_META_TYPE) _ModuleMeta
            from_raw_meta(raw_meta: _FX_TRACER_NN_MODULE_META_TYPE | _DYNAMO_NN_MODULE_META_TYPE) _ModuleMeta
          }
          class _ModuleNode {
            stack_meta
            stack_trace
            add_leaf_node(leaf_node: _LeafNode) None
            build_module(module_names: dict[str, int]) torch.fx.GraphModule
            fx_nodes() Generator[torch.fx.Node, None, None]
            is_parent_module_of(node: _IRNode) bool
            is_same_module_as(node: _IRNode) bool
            module_inputs() Sequence[torch.fx.Node]
            module_outputs() Sequence[torch.fx.Node]
          }
          class _ModuleNode {
            source_file : str
          }
          class _ModuleNotInstalledAsSubmoduleError {
          }
          class _ModuleProviderAction {
            name
          }
          class _ModuleStackMeta {
            module_class
            module_display_name
            qualified_module_class_name
            raw_meta
            is_empty_or_root() bool
            is_superset_of(module_stack: _ModuleStackMeta) bool
            push(module_meta: _ModuleMeta) None
            top() _ModuleMeta
          }
          class _ModuleStackTracer {
            attr_proxy_map : WeakKeyDictionary[Module, _AttrProxy]
            counter : int
            enable_attr_proxy : bool
            module_id_cache : defaultdict
            proxy_modules : WeakKeyDictionary[_AttrProxy, Module]
            proxy_paths : WeakKeyDictionary[_AttrProxy, str]
            proxy_type
            scope_root
            submodule_paths : dict
            call_module(m: Module, forward: Callable, args: Tuple[object, ...], kwargs: Dict[str, object]) None
            create_node() fx.node.Node
            getattr(attr: str, attr_val: object, parameter_proxy_cache: Dict[str, Proxy]) object
            is_leaf_module(m: Module, module_qualified_name: str) bool
            path_of_module(mod: Module) str
            trace(root: Union[Module, Callable], concrete_args: Optional[Dict[str, object]]) fx.Graph
          }
          class _MultiDeviceReplicator {
            master
            get(device: torch.device) torch.Tensor
          }
          class _MultiHandle {
            handles : Tuple[RemovableHandle, ...]
            remove() None
          }
          class _MultiProcessingDataLoaderIter {
          }
          class _Multinomial {
            event_dim : int
            is_discrete : bool
            upper_bound
            check(x)
          }
          class _NSGraphMatchableSubgraphsIterator {
            gm
            non_matchable_functions : Set[NSNodeTargetType]
            non_matchable_methods : Set[NSNodeTargetType]
            non_matchable_modules : Set[NSNodeTargetType]
            seen_nodes : Set[Node]
            stack : List[Node]
          }
          class _NVTXStub {
            markA
            rangePop
            rangePushA
          }
          class _NamedOptimizer {
            module : Optional[nn.Module]
            named_parameters : dict
            ordered_param_keys : list
            param_groups : Optional[Collection[Mapping[str, Any]]]
            state
            add_param_group(param_group: Mapping[str, Any]) None
            init_state() None
            load_state_dict(state_dict: Mapping[str, Any]) None
            state_dict() Dict[str, Any]
            step(closure: None) None
          }
          class _Namespace {
            associate_name_with_obj(name: str, obj: Any)
            create_name(candidate: str, obj: Optional[Any]) str
          }
          class _NnapiSerializer {
            ADDER_MAP : dict
            cached_immediates : dict
            constants : dict
            flexible_shape_computation_lines : list
            inputs : list
            jitval_operand_map : dict
            modules : dict
            operands : list
            operation_args : list
            operations : list
            outputs : list
            tensor_sequences : dict
            use_int16_for_qint16 : bool
            used_weights : list
            value_data : list
            values : list
            weight_offset : int
            add_adaptive_avg_pool2d(node)
            add_add_sub_op(node, opcode, fuse_code)
            add_addmm(node)
            add_addmm_or_linear(node, transpose_weight, jit_input, jit_weight, jit_bias)
            add_anonymous_tensor_operand(oper)
            add_avg_pool2d(node)
            add_cat(node)
            add_constant_node(node)
            add_constant_value(jitval, ctype, value)
            add_conv2d(node)
            add_conv2d_common(jit_out, out_scale, out_zero_point, jit_image, weight_tensor, bias_id, args, transpose, fuse_code)
            add_conv_underscore(node)
            add_dequantize(node)
            add_flatten(node)
            add_getattr(node)
            add_hardtanh(node)
            add_immediate_bool_scalar(value)
            add_immediate_float_scalar(value)
            add_immediate_int_scalar(value)
            add_immediate_int_vector(value)
            add_immediate_operand(code, value, dims)
            add_linear(node)
            add_list_construct(node)
            add_log_softmax(node)
            add_mean(node)
            add_node(node)
            add_operation(opcode, inputs, outputs)
            add_pointwise_simple_binary_broadcast_op(node, opcode, fuse_code)
            add_pointwise_simple_unary_op(node, opcode)
            add_pool2d_node(node, opcode)
            add_prelu_op(node)
            add_qadd(node, opcode, fuse_code)
            add_qconv2d(node, fuse_code, transpose)
            add_qlinear(node)
            add_quantize(node)
            add_reshape(node)
            add_size(node)
            add_slice(node)
            add_softmax(node)
            add_tensor_operand(jitval, oper)
            add_tensor_operand_for_input(arg_idx, jitval, tensor)
            add_tensor_operand_for_weight(tensor, dim_order)
            add_tensor_sequence(jitval, values)
            add_to(node)
            add_tuple_construct(node)
            add_unsqueeze(node)
            add_upsample_nearest2d(node)
            compute_operand_shape(op_id, dim, expr)
            forward_operand_shape(out_op_id, out_dim, in_op_id, in_dim)
            get_constant_value(jitval, typekind)
            get_conv_pool_args_2d_common(kernel_size, strides, paddings, dilations, group_num)
            get_conv_pool_args_2d_from_jit(kernel_size, stride, padding, dilation, group)
            get_conv_pool_args_2d_from_pack(kernel_size, packed_config)
            get_next_operand_id()
            get_optional_bias(jit_bias, weight_tensor, transpose)
            get_size_arg(jitval)
            get_tensor_operand_by_jitval(jitval)
            get_tensor_operand_by_jitval_fixed_size(jitval)
            get_tensor_operand_for_weight(jitval)
            get_tensor_operand_or_constant(jitval, dim_order)
            has_operand_for_jitval(jitval)
            operand_to_template_torchscript(op_id, oper, shape)
            serialize_ints(ints)
            serialize_model(model, inputs, return_shapes)
            serialize_values()
            torch_tensor_to_operand(tensor, dim_order)
            transpose_for_broadcast(in0_id, in0_oper, in1_id, in1_oper)
            transpose_to_nhwc(in_id, oper)
          }
          class _NoDefault {
          }
          class _NoParamDecoratorContextManager {
          }
          class _NoSymbolicFunctionForCallFunction {
            format(level: infra.Level, target) Tuple[infra.Rule, infra.Level, str]
            format_message(target) str
          }
          class _NodeDesc {
            addr : str
            local_id : int
            pid : int
          }
          class _NodeDescGenerator {
            generate(local_addr: Optional[str]) _NodeDesc
          }
          class _NodeMissingOnnxShapeInference {
            format(level: infra.Level, op_name) Tuple[infra.Rule, infra.Level, str]
            format_message(op_name) str
          }
          class _NodeReference {
            name : str
          }
          class _NonStrictTorchFunctionHandler {
          }
          class _NoopSaveInputs {
            backward(ctx)
            forward()
            setup_context(ctx: Any, inputs: Tuple[Any, ...], output: Any) None
          }
          class _NormBase {
            affine : bool
            bias
            eps : float
            momentum : Optional[float]
            num_batches_tracked : Optional[Tensor]
            num_features : int
            running_mean : Optional[Tensor]
            running_var : Optional[Tensor]
            track_running_stats : bool
            weight
            extra_repr()
            reset_parameters() None
            reset_running_stats() None
          }
          class _NormPartial {
            norm_type : Union[int, float, str]
          }
          class _NotProvided {
          }
          class _NullDecorator {
          }
          class _NumpyEncoder {
            default(obj)
          }
          class _OneHot {
            event_dim : int
            is_discrete : bool
            check(value)
          }
          class _OnnxSchemaChecker {
            attributes
            match_score
            onnxfunction : onnxscript.OnnxFunction | onnxscript.TracedOnnxFunction
            op_schema
            param_schema
            type_constraints
            perfect_match_inputs(diagnostic: diagnostics.Diagnostic, args: Sequence[fx_type_utils.TensorLike | str | int | float | bool | list | complex | None], kwargs: dict[str, fx_type_utils.Argument]) bool
          }
          class _OpLevelDebugging {
            format(level: infra.Level, node, symbolic_fn) Tuple[infra.Rule, infra.Level, str]
            format_message(node, symbolic_fn) str
          }
          class _OpNamespace {
            name
          }
          class _OpTraceDispatchMode {
            traced_ops : list
          }
          class _OperatorSupportedInNewerOpsetVersion {
            format(level: infra.Level, op_name, opset_version, supported_opset_version) Tuple[infra.Rule, infra.Level, str]
            format_message(op_name, opset_version, supported_opset_version) str
          }
          class _Ops {
            loaded_libraries : set
            import_module(module)
            load_library(path)
          }
          class _OptimInBackwardHookState {
            optim_stream
            wait_for_optim_stream_enqueued : bool
          }
          class _OptimStateKey {
            is_fsdp_managed : bool
            unflat_param_names : Tuple[str, ...]
          }
          class _OptimizerHookState {
            functional_optimizer
            params_to_optimize : set
          }
          class _OrderedSet {
            rep : OrderedDict
            append(o)
          }
          class _OrthMaps {
            name
          }
          class _Orthogonal {
            base
            orthogonal_map
            shape
            forward(X: torch.Tensor) torch.Tensor
            right_inverse(Q: torch.Tensor) torch.Tensor
          }
          class _OverlapInfo {
            assigned_ranks_per_bucket : list[set[int]]
            broadcast_handles : Any
            bucket_index_to_bucket : Any
            bucket_index_to_future : Any
            bucket_indices_seen : Any
            offsets : Any
            params_per_bucket : Any
            params_per_rank : Any
            shard_buckets : bool
            status : Any
            total_size : int
            clear_per_iter_info() None
            wait_for_broadcasts() None
          }
          class _OverlapStatus {
            name
          }
          class _OverlappedStandardOptimizer {
            register_ddp(ddp_inst: DistributedDataParallel)
            register_fsdp(fsdp: FullyShardedDataParallel)* None
          }
          class _OverlappingCpuLoader {
            current_items : deque
            device_module
            device_type : NoneType, str
            idx : int
            in_flight_data : int
            inflight_threshhold : int
            items : List[Tuple[int, object]]
            resolve_fun : Callable
            started : bool
            stream
            add(size: int, obj: object) None
            start_loading() None
            values() Iterator[Tuple[torch.Tensor, object]]
          }
          class _POERules {
            find_operator_overloads_in_onnx_registry
            find_opschema_matched_symbolic_function
            fx_graph_to_onnx
            fx_node_insert_type_promotion
            fx_node_to_onnx
            fx_pass
            missing_custom_symbolic_function
            missing_standard_symbolic_function
            no_symbolic_function_for_call_function
            node_missing_onnx_shape_inference
            op_level_debugging
            operator_supported_in_newer_opset_version
            unsupported_fx_node_analysis
          }
          class _PackageNode {
            children : Dict[str, _PathNode]
            source_file : Optional[str]
          }
          class _PackageResourceReader {
            fullname
            importer
            contents()
            is_resource(name)
            open_resource(resource)
            resource_path(resource)
          }
          class _ParamUsageInfo {
            module
            named_params : List[Tuple[str, nn.Parameter]]
          }
          class _ParameterMeta {
          }
          class _ParsedStackTrace {
            code : str
            file : str
            lineno : str
            name : str
            get_summary_str()
          }
          class _PartialWrapper {
            callable_args : dict
            p
            with_args()
            with_callable_args()
          }
          class _PassDictsType {
          }
          class _PatchedFn {
            fn_name : str
            frame_dict : Any
            new_fn : Any
            orig_fn : Any
            patch()*
            revert()*
          }
          class _PatchedFnDel {
            patch()
            revert()
          }
          class _PatchedFnSetAttr {
            patch()
            revert()
          }
          class _PatchedFnSetItem {
            patch()
            revert()
          }
          class _Patcher {
            patches_made : List[_PatchedFn]
            visited : Set[int]
            patch(frame_dict: Dict[str, Any], name: str, new_fn: Callable, deduplicate: bool)
            patch_method(cls: type, name: str, new_fn: Callable, deduplicate: bool)
            reapply_all_patches()
            revert_all_patches()
            visit_once(thing: Any)
          }
          class _PathNode {
          }
          class _PatternInfo {
            action
            allow_empty : bool
            was_matched : bool
          }
          class _PeriodicTimer {
            name
            cancel() None
            set_name(name: str) None
            start() None
          }
          class _PhantomRoot {
            constraint_range : str
            name : str
            val : int
          }
          class _PipelineSchedule {
            step()*
          }
          class _PipelineScheduleRuntime {
            pipeline_order_with_comms : Dict[int, List[_Action]]
          }
          class _PipelineStage {
            act_send_info : dict
            name
            node
            pipe_info
            submod_to_stage_index : Dict[str, int]
            find_dst_rank(user: fx.Node) Optional[int]
            get_stage_index_of_submod(submod_name: str)
          }
          class _PipelineStageBase {
            act_send_info : Dict[int, List]
            args_recv_info : Dict[int, Tuple[InputInfo, ...]]
            backward_state : Dict[int, Tuple[Any, ...]]
            bwd_cache : Dict[int, Tuple[Optional[torch.Tensor], ...]]
            chunks : Optional[int]
            device
            dw_builder : Optional[Callable[[], Callable[..., None]]]
            dw_runner : Dict[int, Callable[..., None]]
            fwd_cache : Dict[int, Tuple[Any, List[torch.Tensor]]]
            grad_recv_info : Dict
            grad_send_info : Optional[List], list
            group : Optional[dist.ProcessGroup]
            group_rank : int
            group_size : int
            has_backward
            is_first
            is_last
            log_prefix : str
            num_stages : int
            output_chunks : List[Any]
            stage_index : int
            stage_index_to_group_rank : Dict[int, int]
            submod
            backward_maybe_with_nosync(backward_type, bwd_kwargs: Dict, last_backward) Tuple[Tuple[Optional[torch.Tensor], ...], Optional[List[Dict[str, Any]]]]
            backward_one_chunk(bwd_chunk_id: int, loss, full_backward: bool, last_backward)
            backward_weight_one_chunk(bwd_chunk_id: int, last_backward)
            clear_runtime_states() None
            forward_maybe_with_nosync()
            forward_one_chunk(fwd_chunk_id: int, args: Tuple[Any, ...], kwargs: Optional[Dict[str, Any]])
            get_bwd_recv_ops(bwd_chunk_id: int) List[dist.P2POp]
            get_bwd_send_ops(bwd_chunk_id: int) List[dist.P2POp]
            get_fwd_recv_ops(fwd_chunk_id: int) List[dist.P2POp]
            get_fwd_send_ops(fwd_chunk_id: int) List[dist.P2POp]
            get_local_bwd_output(mb_index)
            get_outputs_meta() Tuple[torch.Tensor, ...]
            set_local_bwd_input(next_stage_bwd_outputs: Tuple[Optional[torch.Tensor], ...], mb_index: int) None
            set_local_fwd_input(prev_stage_outputs: Any, mb_index: int) None
          }
          class _Policy {
          }
          class _PosDimTensorInfo {
            dtype
            shape
          }
          class _PositiveDefinite {
            check(value)
          }
          class _PositiveSemidefinite {
            check(value)
          }
          class _PrefetchMode {
            name
          }
          class _ProcessGroupStub {
          }
          class _ProfilerStats {
            function_events_build_tree_call_duration_us : int
            number_of_events : int
            parse_kineto_call_duration_us : int
            profiler_disable_call_duration_us : int
            profiler_enable_call_duration_us : int
            profiler_prepare_call_duration_us : int
            profiling_window_duration_sec : float
          }
          class _ProtocolFn {
            json_to_treespec : Callable[[DumpableContext], TreeSpec]
            treespec_to_json : Callable[[TreeSpec], DumpableContext]
          }
          class _ProxyTensor {
            constant : Optional[Tensor]
            proxy
          }
          class _PyAwaitMeta {
          }
          class _PyFutureMeta {
          }
          class _PyOpNamespace {
          }
          class _PySymInputStub {
            value : Union[PySymType, _DeconstructedSymType, _InputBackref]
            extract(shape_env: ShapeEnv) PySymType
            strip_shape_env() None
          }
          class _PyTorchLegacyPickler {
            persistent_id
            persistent_id(obj)
          }
          class _PyTreeCodeGen {
            pytree_info : _PyTreeInfo
            gen_fn_def(free_vars, maybe_return_annotation)
            generate_output(output_args)
            process_inputs() Any
            process_outputs(out: Any) Any
          }
          class _PyTreeExtensionContext {
            register_pytree_node(class_type: type, flatten_func: pytree.FlattenFunc, unflatten_func: pytree.UnflattenFunc)
          }
          class _PyTreeInfo {
            in_spec
            orig_args : List[str]
            out_spec : Optional[pytree.TreeSpec]
          }
          class _PythonMsgPrinter {
            src_map : Dict[str, List[str]]
          }
          class _QEngineProp {
          }
          class _RNGStateTracker {
            distribute_region_enabled
            rng_states
            get_seed(name: str) int
            rng_state_is_sync(name) bool
            set_seed(name: str, seed: int) None
          }
          class _ReaderView {
            base_stream : IOBase
            len : int
            offset : int
            read(size)
            readable() bool
            readinto(b)
            seek(__offset: int, __whence: int) int
            seekable() bool
            tell() int
          }
          class _ReaderWithOffset {
            fqn_to_offset : Dict[str, Sequence[int]]
            metadata
            state_dict : Dict
            translation : Dict[MetadataIndex, MetadataIndex]
            create_local_plan() LoadPlan
            lookup_tensor(index: MetadataIndex) torch.Tensor
          }
          class _Real {
            check(value)
          }
          class _RecordLoadStoreInner {
            bucketize(values: T, boundaries: Tuple[str, sympy.Expr, sympy.Expr, sympy.Expr], boundary_indices: T, indexing_dtype: torch.dtype, right: bool, sorter: Optional[Tuple[str, sympy.Expr]], sorter_indices: Optional[T]) None
            canonicalize(index: sympy.Expr) Tuple[sympy.Expr, Tuple[sympy.Symbol, ...], Tuple[sympy.Expr, ...]]
            drop_unused_symbols(index, var_names, sizes)
            index_expr(index: sympy.Expr, dtype) str
            load(name: str, index: sympy.Expr) str
            load_seed(name: str, index: int)
            store(name: str, index: sympy.Expr, value: str, mode) str
            store_reduction(name: str, index, value) str
          }
          class _RecvInfo {
            buffer
            input_name : str
            source : int
          }
          class _Reduce {
            backward(ctx, grad_output)
            forward(ctx, src, op, group, tensor)
          }
          class _ReduceScatterMatch {
            group_name : str
            input_node
            match
            reduce_op : str
            res_node
            rs_node
            scatter_dim : int
            erase() None
            replace_with(new_node: torch.fx.Node) None
          }
          class _Reduce_Scatter {
            backward(ctx, grad_output)
            forward(ctx, op, group, tensor)
          }
          class _RefType {
            name
          }
          class _ReflectionPadNd {
            padding : Sequence[int]
            extra_repr() str
            forward(input: Tensor) Tensor
          }
          class _RelaxedConstraint {
            serializable_spec
          }
          class _RemoteModule {
            device : str
            generated_methods
            is_device_map_set
            is_scriptable : bool
            module_rref
            on : NoneType, int
            add_module(name: str, module: Optional[Module]) None
            apply(fn: Callable[[Module], None]) T
            bfloat16() T
            buffers(recurse: bool) Iterator[Tensor]
            children() Iterator[Module]
            cpu() T
            cuda(device: Optional[Union[int, device]]) T
            double() T
            eval() T
            extra_repr() str
            float() T
            get_module_rref() rpc.RRef[nn.Module]
            half() T
            init_from_module_rref(remote_device: str, module_rref: rpc.RRef[nn.Module], _module_interface_cls: Any)
            ipu(device: Optional[Union[int, device]]) T
            load_state_dict(state_dict: Mapping[str, Any], strict: bool, assign: bool)
            modules() Iterator[Module]
            named_buffers(prefix: str, recurse: bool, remove_duplicate: bool) Iterator[Tuple[str, Tensor]]
            named_children() Iterator[Tuple[str, Module]]
            named_modules(memo: Optional[Set[Module]], prefix: str, remove_duplicate: bool)
            named_parameters(prefix: str, recurse: bool, remove_duplicate: bool) Iterator[Tuple[str, Parameter]]
            parameters(recurse: bool) Iterator[Parameter]
            register_backward_hook(hook: Callable[[Module, _grad_t, _grad_t], Union[None, _grad_t]]) RemovableHandle
            register_buffer(name: str, tensor: Optional[Tensor], persistent: bool) None
            register_forward_hook(hook: Union[Callable[[T, Tuple[Any, ...], Any], Optional[Any]], Callable[[T, Tuple[Any, ...], Dict[str, Any], Any], Optional[Any]]], prepend: bool, with_kwargs: bool) RemovableHandle
            register_forward_pre_hook(hook: Union[Callable[[T, Tuple[Any, ...]], Optional[Any]], Callable[[T, Tuple[Any, ...], Dict[str, Any]], Optional[Tuple[Any, Dict[str, Any]]]]], prepend: bool, with_kwargs: bool) RemovableHandle
            register_parameter(name: str, param: Optional[Parameter]) None
            remote_parameters(recurse: bool) List[rpc.RRef[Parameter]]
            requires_grad_(requires_grad: bool) T
            share_memory() T
            state_dict()
            to() T
            train(mode: bool) T
            type(dst_type: Union[dtype, str]) T
            xpu(device: Optional[Union[int, device]]) T
            zero_grad(set_to_none: bool) None
          }
          class _RemoveRuntimeAssertionsPass {
            call(graph_module) PassResult
          }
          class _RendezvousCloseOp {
          }
          class _RendezvousContext {
            node
            settings
            state
          }
          class _RendezvousExitOp {
          }
          class _RendezvousJoinOp {
          }
          class _RendezvousKeepAliveOp {
          }
          class _RendezvousOpExecutor {
            run(state_handler: Callable[[_RendezvousContext, float], _Action], deadline: float, update_deadline: Optional[Callable[[timedelta], float]])* None
          }
          class _RendezvousState {
            closed : bool
            complete : bool
            deadline : Optional[datetime]
            last_heartbeats : Dict[_NodeDesc, datetime]
            participants : Dict[_NodeDesc, int]
            redundancy_list : Set[_NodeDesc]
            round : int
            wait_list : Set[_NodeDesc]
          }
          class _RendezvousStateHolder {
            state
            mark_dirty()* None
            sync()* Optional[bool]
          }
          class _ReparametrizeModule {
            accessor
            orig_parameters_and_buffers : dict
            parameters_and_buffers : Dict[str, Tensor]
            stack_weights : bool
            untied_parameters_and_buffers
          }
          class _Replicate {
          }
          class _ReplicateState {
            has_initialized : bool
            module
            forward_post_hook(module: nn.Module, input: Tuple[torch.Tensor], output: torch.Tensor) torch.Tensor
            forward_pre_hook(module: nn.Module, args: Tuple[Any, ...], kwargs: Dict[str, Any]) Any
            init(module: nn.Module, ignored_modules: Set[nn.Module]) None
            lazy_init() None
            record_init_args() None
            register_comm_hook() None
          }
          class _ReplicationPadNd {
            padding : Sequence[int]
            extra_repr() str
            forward(input: Tensor) Tensor
          }
          class _RequiredParameter {
          }
          class _ResumeIteration {
            seed : Optional[int]
          }
          class _RewriteInfo {
            example_inputs : Tuple[Any, ...]
            pattern : Callable
            pattern_post_trans : Optional[Callable[[GraphModule], GraphModule]]
            replacement : Callable
            replacement_post_trans : Optional[Callable[[GraphModule], GraphModule]]
          }
          class _RingRotater {
            exchange_buffers(curr_buffer: torch.Tensor)* None
            next_buffer()* torch.Tensor
          }
          class _RoleInstanceInfo {
            local_world_size : int
            rank : int
            role : str
            compare(obj1, obj2) int
            deserialize(data: bytes)
            find_role_boundaries(roles_infos: List, role: str) Tuple[int, int]
            serialize() bytes
          }
          class _RootArgPlaceholder {
            meta
          }
          class _RotateMethod {
            name
          }
          class _RoundRobinLoadBalancer {
            ROUND_ROBIN_CYCLE : int
            shard(buffer: torch.Tensor, mesh: DeviceMesh, seq_dim: int) torch.Tensor
            unshard(buffer: torch.Tensor, mesh: DeviceMesh, seq_dim: int) torch.Tensor
          }
          class _Row {
            as_column_strings()
            color_segment(segment, value, best_value)
            finalize_column_strings(column_strings, col_widths)
            register_columns(columns: Tuple[_Column, ...])
            row_separator(overall_width)
          }
          class _SACMetadata {
            curr_idx : int
            func : Any
            inplace_info : Tuple[int, ...]
            is_rand_op : bool
            is_view_like : bool
            memory_used : float
            output_ids : Tuple[int, ...]
            time_taken : float
          }
          class _SACModMetadata {
            force_store_random : bool
            sac_metadata : List[_SACMetadata]
            start_idx : int
          }
          class _SDPAMerger {
            results() Tuple[torch.Tensor, torch.Tensor]
            step(out: torch.Tensor, lse: torch.Tensor, partial: bool) None
          }
          class _SavedCollectives {
            all_gather_into_tensor : Callable
            all_reduce : Callable
            barrier : Callable
            reduce_scatter_tensor : Callable
          }
          class _SavedFSDPMethods {
            post_backward : Callable
            pre_backward : Callable
          }
          class _ScaleMode {
            name
          }
          class _ScaledMatmul {
            A_node
            A_scale_node
            B_scale_node
            arg_ancestor_nodes
            bias_node : Optional[torch.fx.Node]
            out_dtype : Optional[torch.dtype]
            result_scale_node : Optional[torch.fx.Node]
            use_fast_accum : bool
            from_match(match: List[torch.fx.Node]) '_ScaledMatmul'
          }
          class _Scatter {
            backward(ctx, grad_output)
            forward(ctx, src, group)
          }
          class _ScheduleForwardOnly {
          }
          class _SchedulePhase {
            end_lr : str
            end_momentum : str
            end_step : float
            start_lr : str
            start_momentum : str
          }
          class _ScriptLocalOptimizer {
            compile_lock : lock
            optim
            step(autograd_ctx_id: int)
          }
          class _ScriptLocalOptimizerInterface {
            step(autograd_ctx_id: int)* None
          }
          class _ScriptProfile {
            profile
            disable()
            dump()
            dump_string() str
            enable()
          }
          class _ScriptProfileColumn {
            alignment : int
            header : str
            offset : int
            rows : Dict[int, Any]
            add_row(lineno: int, value: Any)
            materialize()
          }
          class _ScriptProfileTable {
            cols : List[_ScriptProfileColumn]
            source_range : List[int]
            dump_string()
          }
          class _SequentialSharder {
            shard(buffer: torch.Tensor, mesh: DeviceMesh, seq_dim: int) torch.Tensor
            unshard(buffer: torch.Tensor, mesh: DeviceMesh, seq_dim: int) torch.Tensor
          }
          class _SerialCpuLoader {
            items : List[Tuple[int, object]]
            resolve_fun : Callable
            add(size: int, obj: object) None
            start_loading()* None
            values() Iterator[Tuple[torch.Tensor, object]]
          }
          class _SerializationLocal {
            map_location : NoneType, Optional[MAP_LOCATION]
            materialize_fake_tensors : bool
            skip_data : bool
          }
          class _SerializeNodeDef {
            from_dumpable_context : Optional[FromDumpableContextFn]
            serialized_type_name : str
            to_dumpable_context : Optional[ToDumpableContextFn]
            typ : Type[Any]
          }
          class _SerializedProgram {
            constants : bytes
            example_inputs : bytes
            exported_program
            state_dict : bytes
          }
          class _ShapeAndDims {
            dims : Tuple[int, ...]
            shape : Tuple[int, ...]
          }
          class _ShapeGuardPrinter {
            source_ref : Callable[[Source], str]
            symbol_to_source : Mapping[sympy.Symbol, List[Source]]
            var_to_sources : Mapping[sympy.Symbol, List[Source]]
            print_source(source: Source)* str
          }
          class _ShardParamInfo {
            in_shard : bool
            intra_param_end_idx : Optional[int]
            intra_param_start_idx : Optional[int]
            numel_in_shard : Optional[int]
            offset_in_shard : Optional[int]
          }
          class _ShardingIterDataPipe {
            apply_sharding(num_of_instances: int, instance_id: int, sharding_group: SHARDING_PRIORITIES)*
          }
          class _Simplex {
            event_dim : int
            check(value)
          }
          class _SingleLevelFunction {
            vjp
            backward(ctx: Any)* Any
            forward()* Any
            jvp(ctx: Any)* Any
            setup_context(ctx: Any, inputs: Tuple[Any, ...], output: Any)* Any
          }
          class _SingleProcessDataLoaderIter {
          }
          class _SnapshotState {
            name
          }
          class _SpectralNorm {
            dim : int
            eps : float
            n_power_iterations : int
            forward(weight: torch.Tensor) torch.Tensor
            right_inverse(value: torch.Tensor) torch.Tensor
          }
          class _SplitterBase {
            PCIe_BW : int
            acc_nodes : set
            deps : defaultdict
            fusions : dict
            module
            non_acc_submodule_name : str
            operator_support
            sample_input : Sequence[Any]
            settings
            tags : List[str], list
            extend_acc_subgraph(tag: str)
            find_deps() Dict[torch.fx.Node, NodeSet]
            find_parent_nodes_of_subgraph(tag: str) NodeSet
            find_reverse_deps(tag_id: Optional[int]) Dict[torch.fx.Node, NodeSet]
            generate_split_results() SplitResult
            get_node_submodule_map() Dict[str, str]
            node_support_preview(dump_graph: bool)
            put_nodes_into_subgraphs() List[Subgraph]
            remove_small_acc_subgraphs(subgraphs: List[Subgraph]) List[Subgraph]
            split(remove_tag: bool) torch.fx.GraphModule
            split_preview(dump_graph: bool)
            starter_nodes() Tuple[NodeSet, NodeSet]
            tag(subgraphs: List[Subgraph])
            update_deps_for_fusions()
            update_reverse_deps_for_fusions(deps: Dict[torch.fx.Node, NodeSet])
          }
          class _SplitterSettingBase {
            allow_non_tensor : bool
            max_acc_splits : int
            min_acc_module_size : int
            skip_fusion : bool
          }
          class _Square {
            event_dim : int
            check(value)
          }
          class _Stack {
            cseq : list
            dim : int
            event_dim
            is_discrete
            check(value)
          }
          class _State {
          }
          class _State {
            name
          }
          class _StateDictInfo {
            fqn_param_mapping : Dict[Union[str, torch.Tensor], Union[FQNS_T, torch.Tensor]]
            fsdp_context : Callable
            fsdp_modules : List[nn.Module]
            handle_model : bool
            handle_optim : bool
            shared_params_mapping : Dict[Union[str, torch.Tensor], Union[FQNS_T, torch.Tensor]]
            submodule_prefixes : Set[str]
          }
          class _StatefulGraphModule {
            range_constraints : list
            validate_inputs : bool
          }
          class _StatefulGraphModuleFactory {
          }
          class _StaticDim {
            max
            min
          }
          class _StopRecomputationError {
          }
          class _Storage {
            allocation_id : int
            ptr : int
          }
          class _StorageBase {
            device
            is_cuda
            is_hpu
            is_sparse : bool
            is_sparse_csr : bool
            bfloat16()
            bool()
            byte()
            byteswap(dtype)
            char()
            clone()
            complex_double()
            complex_float()
            copy_(source: T, non_blocking: _Optional[_bool])* T
            cpu()
            cuda(device, non_blocking) Union[_StorageBase, TypedStorage]
            data_ptr()* _int
            double()
            element_size()* _int
            float()
            float8_e4m3fn()
            float8_e4m3fnuz()
            float8_e5m2()
            float8_e5m2fnuz()
            from_buffer()* T
            from_file(filename, shared, nbytes)* Union[_StorageBase, TypedStorage]
            get_device() _int
            half()
            hpu(device, non_blocking) Union[_StorageBase, TypedStorage]
            int()
            is_pinned(device: Union[str, torch.device])
            is_shared()* _bool
            long()
            mps()
            nbytes()* _int
            new()* Union[_StorageBase, TypedStorage]
            pin_memory(device: Union[str, torch.device])
            resizable()* _bool
            resize_(size: _int)*
            share_memory_()
            short()
            size() _int
            to()
            tolist()
            type(dtype: _Optional[str], non_blocking: _bool) Union[_StorageBase, TypedStorage]
            untyped()
          }
          class _StorageInfo {
            length : int
            offset : int
            relative_path : str
          }
          class _StoragePrefix {
            prefix : str
          }
          class _StreamBase {
          }
          class _StridedShard {
            split_factor : int
          }
          class _SubmoduleEntry {
            call_idx : int
            fqn : str
            module
            parent_call_module
            parent_fqn : str
            parent_module
          }
          class _SubprocExceptionInfo {
            details : str
          }
          class _SupportedQEnginesProp {
          }
          class _SymExprHash {
            sym_obj : Union[SymInt, SymFloat, SymBool]
          }
          class _SymHashingDict {
            sym_hash_dict : dict
            get(key, default)
          }
          class _SymIntOutputStub {
            value : Union[int, _DeconstructedSymNode]
            extract(key: _DispatchCacheKey, shape_env: ShapeEnv) SymInt
          }
          class _SymNodeDict {
            sym_node_dict : Dict[PySymType, _PySymProxyType]
            get(key: PySymType, default: Optional[_PySymProxyType]) _PySymProxyType
          }
          class _SymbolicFunctionGroup {
            add(func: Callable, opset: OpsetVersion) None
            add_custom(func: Callable, opset: OpsetVersion) None
            get(opset: OpsetVersion) Optional[Callable]
            get_min_supported() OpsetVersion
            remove_custom(opset: OpsetVersion) None
          }
          class _Symmetric {
            check(value)
          }
          class _SysImporter {
            import_module(module_name: str)
            whichmodule(obj: Any, name: str) str
          }
          class _TargetArgsExpr {
            args : tuple
            flat_args_kwargs : tuple
            flatten
            kwargs : dict
            find_anchor_nodes(ctx: MatchContext, searched: OrderedSet[torch.fx.Node]) Generator[Optional[torch.fx.Node], None, None]
            pattern_eq(other: Any) bool
            pretty_print(pp: PatternPrettyPrinter) str
            pytree_flatten(args: Sequence[Any], kwargs: Mapping[Any, Any]) Tuple[Sequence[Any], Union[_SimpleSpec, pytree.TreeSpec]]
            simple_flatten(args: Sequence[Any], kwargs: Mapping[Any, Any]) Tuple[Sequence[Any], Union[_SimpleSpec, pytree.TreeSpec]]
          }
          class _TargetExpr {
            fns : List[FnsType]
            fns_set : OrderedSet[FnsType]
            op
            users : Union[Multiple, int]
            find_anchor_nodes(ctx: MatchContext, searched: OrderedSet[torch.fx.Node])* Generator[Optional[torch.fx.Node], None, None]
            fns_repr() str
            has_multiple_users() bool
            pattern_eq(other: Any) bool
          }
          class _TargetExprVarArgs {
          }
          class _TensorExtractor {
            tensors
            persistent_id(obj)
          }
          class _TensorInfo {
            dtype
            size
          }
          class _TensorLoader {
            add(size: int, obj: object)* None
            start_loading()* None
            values()* Iterator[Tuple[torch.Tensor, object]]
          }
          class _TensorParallelTransformPass {
            graph_signature
            mesh
            parallel_strategies : Dict[str, ParallelStyle]
            rank : int
            state_dict : Dict[str, torch.Tensor]
            call(graph_module) PassResult
          }
          class _TensorsAccessed {
            accesses : Dict[DataPtr, TensorInfo]
            add_read(data_ptr: DataPtr, access: Access) None
            create_tensor(data_ptr: DataPtr, stack_trace: Optional[traceback.StackSummary]) None
            delete_tensor(data_ptr: DataPtr) None
            ensure_tensor_does_not_exist(data_ptr: DataPtr) None
            ensure_tensor_exists(data_ptr: DataPtr) None
            get_allocation_stack_trace(data_ptr: DataPtr) Optional[traceback.StackSummary]
            get_reads(data_ptr: DataPtr) List[Access]
            get_write(data_ptr: DataPtr) Optional[Access]
            set_write(data_ptr: DataPtr, access: Access) None
            were_there_reads_since_last_write(data_ptr: DataPtr) bool
          }
          class _TestCase {
            setUpClass() None
            tearDownClass() None
          }
          class _TestParametrizer {
          }
          class _TestParamsMaxPool1d {
          }
          class _TestParamsMaxPool2d {
          }
          class _TestParamsMaxPool3d {
          }
          class _TestParamsMaxPoolBase {
            kwargs : dict
            shapes : list
            gen_input_params()
          }
          class _ToTorchTensor {
            backward(ctx, grad_output: torch.Tensor)
            forward(ctx, input: 'DTensor', grad_placements: Optional[Sequence[Placement]])
          }
          class _TorchCompileInductorWrapper {
            compiler_name : str
            config : _Dict[str, _Any]
            dynamic
            apply_mode(mode: _Optional[str])
            apply_options(options: _Optional[_Dict[str, _Any]])
            get_compiler_config()
            reset()
          }
          class _TorchCompileWrapper {
            compiler_fn
            compiler_name : str
            dynamic
            kwargs : dict
            reset()
          }
          class _TorchDynamoContext {
            callback : Union
            cleanup_fns : List[Callable[[], Any]]
            compiler_config : NoneType
            enter_exit_hooks : list
            export : bool
            first_ctx : bool
            prior : NoneType, Union[Unset, DynamoCallback], bool, token
            prior_skip_guard_eval_unsafe
          }
          class _TorchSchema {
            arguments : List[str], list
            name : str
            opsets : List[int], list
            optional_arguments : List[str], list
            overload_name : str
            returns : List[str], list
            is_aten() bool
            is_backward() bool
          }
          class _TransformInfo {
            logical_shape : List[int]
            mesh_dim : int
            src_dst_placements : Tuple[Placement, Placement]
          }
          class _TreeSpecSchema {
            children_spec : List['_TreeSpecSchema']
            context : Any
            type : Optional[str]
          }
          class _TritonLibrary {
            lib
            ops_table : _Dict[_Tuple[str, str], _Callable]
            registerOp(op_key, full_schema, op_impl, dispatch_key)
          }
          class _TypePromotionInterpreter {
            diagnostic_context
            type_promotion_table
            run_node(node: torch.fx.Node) Any
          }
          class _UnaliasedStorage {
          }
          class _Unassigned {
          }
          class _UninitializedDeviceHandle {
          }
          class _Union {
            type
            value
            create()
          }
          class _UnionTag {
            create(t, cls)
          }
          class _UnshardHandleImpl {
            wait()
          }
          class _Unspecified {
          }
          class _UnsupportedFxNodeAnalysis {
            format(level: infra.Level, node_op_to_target_mapping) Tuple[infra.Rule, infra.Level, str]
            format_message(node_op_to_target_mapping) str
          }
          class _UpdateType {
            name
          }
          class _V {
            KernelFormatterHandler
            MockHandler
            WrapperHandler
            aot_compilation
            choices
            current_node
            debug
            fake_mode
            get_aot_compilation : Callable[[], Any]
            get_current_node : Callable[[], Any]
            get_fake_mode : Callable[[], Any]
            get_local_buffer_context : Callable[[], Any]
            get_ops_handler : Callable[[], Any]
            get_real_inputs : Callable[[], Any]
            graph
            interpreter
            kernel
            local_buffer_context
            ops
            real_inputs
            set_aot_compilation : Callable[[bool], Any]
            set_choices_handler : Callable[[Any], Any]
            set_current_node : Callable[[Any], Any]
            set_debug_handler : Callable[[Any], Any]
            set_fake_mode : Callable[[Any], Any]
            set_graph_handler : Callable[[GraphLowering], Any]
            set_interpreter_handler : Callable[[Any], Any]
            set_kernel_handler : Callable[[Any], Any]
            set_local_buffer_context : Callable[[Any], Any]
            set_ops_handler : Callable[[Any], Any]
            set_real_inputs : Callable[[Any], Any]
          }
          class _ValgrindWrapper {
            collect_callgrind(task_spec: common.TaskSpec, globals: Dict[str, Any]) Tuple[CallgrindStats, ...]
          }
          class _VerifierMeta {
          }
          class _Version {
            dev : Optional[Tuple[str, int]]
            epoch : int
            local : Optional[LocalType]
            post : Optional[Tuple[str, int]]
            pre : Optional[Tuple[str, int]]
            release : Tuple[int, ...]
          }
          class _VersionWrapper {
            val : Union[torch.Tensor, Any]
            version : Optional[int]
            get_val(allow_cache_entry_mutation)
          }
          class _ViewType {
            name
          }
          class _WaitKernel {
            create_wait(kernel, inp: TensorBox) None
            get_read_writes() dependencies.ReadWrites
            get_volatile_reads()
          }
          class _WeakHashRef {
          }
          class _WeakRefInfo {
            device
            element_size : int
            mem_consumed
            reftype
            size : int
            create_winfo(st: torch.UntypedStorage, device: torch.device, reftype: _RefType, callback: Optional[Callable[[Self, weakref.ref], Any]]) Tuple[Self, weakref.ref]
            get_untyped_storages(t: torch.Tensor) Set[torch.UntypedStorage]
            update_mem_consumed(st: torch.UntypedStorage) int
          }
          class _WeightEqualizationObserver {
            ch_axis : int
            dtype
            equalization_scale
            qscheme
            weight_col_obs
            with_args : classmethod
            forward(w_orig)
            get_weight_col_minmax()
            set_equalization_scale(equalization_scale)
          }
          class _WeightNorm {
            dim : Optional[int]
            forward(weight_g, weight_v)
            right_inverse(weight)
          }
          class _WeightedLoss {
            weight : Optional[Tensor]
          }
          class _WorksWithInt {
          }
          class _World {
            default_pg
            group_count
            pg_backend_config
            pg_coalesce_state
            pg_config_info
            pg_group_ranks
            pg_map
            pg_names
            pg_to_tag
            tags_to_pg
          }
          class _WorldMeta {
            WORLD
          }
          class _WrappedCall {
            cls
            cls_call
          }
          class _WrappedHook {
            hook : Callable
            module : weakref.ReferenceType[Module]
            with_module : bool
          }
          class _WrappedTritonKernel {
            kernel
            kernel_invoked : bool
          }
          class _WrapperModule {
            fn
            forward()
          }
          class _WrapperModule {
            f
            forward()
          }
          class _X86InductorQuantizationAnnotation {
          }
          class _XNNPACKEnabled {
          }
          class _Z3Ops {
            bitwise_and
            bitwise_or
            lshift
            rshift
            validator : str
            abs(number: z3.ArithRef) z3.ArithRef
            ceil(number: z3.ArithRef) z3.ArithRef
            div(numerator: z3.ArithRef, denominator: z3.ArithRef) z3.ArithRef
            floor(number: z3.ArithRef) z3.ArithRef
            floordiv(numerator: z3.ArithRef, denominator: z3.ArithRef) z3.ArithRef
            max(a: z3.ArithRef, b: z3.ArithRef) z3.ArithRef
            min(a: z3.ArithRef, b: z3.ArithRef) z3.ArithRef
            mod(p: z3.ArithRef, q: z3.ArithRef) z3.ArithRef
            pow(base: z3.ArithRef, exp: z3.ArithRef) z3.ArithRef
            round_to_int(number: z3.ArithRef) z3.ArithRef
            sqrt(number: z3.ArithRef) z3.ArithRef
            sym_sum(args: z3.ArithRef) z3.ArithRef
            to_int(x: z3.ArithRef) z3.ArithRef
            to_real(x: z3.ArithRef) z3.ArithRef
            trunc(number: z3.ArithRef) z3.ArithRef
          }
          class _ZeROJoinHook {
            zero : Any
            main_hook() None
          }
          class __PrinterOptions {
            edgeitems : int
            linewidth : int
            precision : int
            sci_mode : Optional[bool]
            threshold : float
          }
          class _checkpoint_hook {
          }
          class _collective {
            auto_select : bool
            one_shot_all_reduce_threshold_bytes : int
          }
          class _dispatch_dtypes {
          }
          class _dynamic_dispatch_dtypes {
          }
          class _enable_get_lr_call {
            o
          }
          class _enable_get_sl_call {
            o
          }
          class _enable_get_sp_call {
            o
          }
          class _force_original_view_tracking {
            mode : bool
            prev
            clone()
          }
          class _lazy_property_and_property {
          }
          class _missing {
          }
          class _node_list {
            direction : str
            graph : str
          }
          class _open_buffer_reader {
          }
          class _open_buffer_writer {
          }
          class _open_file {
          }
          class _open_zipfile_reader {
          }
          class _open_zipfile_writer_buffer {
            buffer
          }
          class _open_zipfile_writer_file {
            file_stream : FileIO, NoneType
            name : str
          }
          class _opener {
            file_like
          }
          class _recomputation_hook {
          }
          class _reduce_op {
          }
          class _remote_device {
            device() torch.device
            rank() Optional[int]
            worker_name() Optional[str]
          }
          class _safe_globals {
            safe_globals : List[Union[Callable, Tuple[Callable, str]]]
          }
          class _server_process_global_profile {
            entered : bool
            function_events
            process_global_function_events : list
          }
          class _set_fwd_grad_enabled {
            prev
          }
          class _swap_with_cloned {
          }
          class _unsafe_preserve_version_counter {
            prev_version
            tensor
          }
          class align {
            is_integer : bool
            nargs : tuple
            eval(value: sympy.Expr) Optional[sympy.Expr]
          }
          class aot_inductor {
            allow_stack_allocation : bool
            debug_compile
            debug_intermediate_value_printer : str
            dump_aoti_minifier : bool
            filtered_kernel_names : NoneType
            force_mmap_weights : bool
            metadata : Dict[str, str]
            output_path : str
            package : bool
            package_constants_in_so : bool
            package_cpp_only : bool
            presets : Dict[str, Any]
            raise_error_on_ignored_optimization : bool
            serialized_in_spec : str
            serialized_out_spec : str
            use_minimal_arrayref_interface : bool
            use_runtime_constant_folding : bool
          }
          class assert_raises_regex {
            exception_cls
            regex
          }
          class autocast {
            device : str
            fast_dtype
          }
          class autocast {
            device : str
            fast_dtype
          }
          class autocast {
            custom_backend_name
            custom_device_mod
            device : str
            fast_dtype : NoneType
            prev
            prev_cache_enabled
            prev_fastdtype
          }
          class bcolors {
            BOLD : str
            ENDC : str
            FAIL : str
            HEADER : str
            OKBLUE : str
            OKGREEN : str
            UNDERLINE : str
            WARNING : str
          }
          class bool_ {
            name : str
            torch_dtype
            typecode : str
          }
          class capture_stderr {
            stringio : StringIO
            sys_stderr : NoneType, StringIO, TextIOWrapper
          }
          class capture_stdout {
            stringio : StringIO
            sys_stdout : StringIO, TextIOWrapper
          }
          class check_env {
          }
          class check_sparse_tensor_invariants {
            saved_state : NoneType, Optional[bool]
            state : bool
            disable()
            enable()
            is_enabled()
          }
          class clear_and_catch_warnings {
            class_modules : tuple
            modules
          }
          class closure_lookup {
          }
          class cls_with_options {
          }
          class complex128 {
            name : str
            torch_dtype
            typecode : str
          }
          class complex64 {
            name : str
            torch_dtype
            typecode : str
          }
          class complexfloating {
            name : str
          }
          class context {
            autograd_context
          }
          class cpp {
            cxx : tuple
            descriptive_names : str
            dynamic_threads
            enable_concat_linear : bool
            enable_floating_point_contract_flag
            enable_kernel_profile
            enable_loop_tail_vec : bool
            enable_tiling_heuristics
            enable_unsafe_math_opt_flag
            fallback_scatter_reduce_sum
            gemm_cache_blocking : NoneType
            gemm_max_k_slices : int
            gemm_thread_factors : NoneType
            inject_log1p_bug_TESTING_ONLY : Optional[str]
            inject_relu_bug_TESTING_ONLY : Optional[str]
            max_horizontal_fusion_size : int
            min_chunk_size : int
            no_redundant_loops
            simdlen : Optional[int]
            threads : int
            vec_isa_ok : Optional[bool]
            weight_prepack
          }
          class cuBLASModule {
            allow_tf32 : bool
          }
          class cuFFTPlanCache {
            device_index
            max_size
            size
            clear()
          }
          class cuFFTPlanCacheAttrContextProp {
            getter
            setter
          }
          class cuFFTPlanCacheManager {
            caches : list
          }
          class cuda {
            arch : Optional[str]
            compile_opt_level : str
            cuda_cxx : Optional[str]
            cutlass_backend_min_gemm_size : int
            cutlass_dir
            cutlass_max_profiling_configs : Optional[int]
            cutlass_op_allowlist_regex : Optional[str]
            cutlass_op_denylist_regex : Optional[str]
            enable_cuda_lto : bool
            enable_debug_info : bool
            enable_ptxas_info : bool
            generate_test_runner : bool
            use_fast_math : bool
            version : Optional[str]
          }
          class cudaStatus {
            ERROR_NOT_READY : int
            SUCCESS : int
          }
          class decorateIf {
            decorator
            predicate_fn
          }
          class detect_anomaly {
            check_nan : bool
            prev
            prev_check_nan
          }
          class device {
            idx : NoneType, int
            prev_idx : int
          }
          class device {
          }
          class device {
            idx
            prev_idx : int
          }
          class device {
            idx : NoneType, int
            prev_idx : int
          }
          class deviceCountAtLeast {
            num_required_devices
          }
          class device_of {
          }
          class device_of {
          }
          class dont_convert {
          }
          class dtypes {
            args : tuple
            device_type : str
          }
          class dtypesIfCPU {
          }
          class dtypesIfCUDA {
          }
          class dtypesIfHPU {
          }
          class dtypesIfMPS {
          }
          class dtypesIfPRIVATEUSE1 {
          }
          class dual_level {
          }
          class elastic_launch {
          }
          class elementwise_type_promotion_wrapper {
            type_promoting_arg_names : NoneType
            type_promotion_kind
          }
          class emit_itt {
            enabled : bool
            entered : bool
            record_shapes : bool
          }
          class emit_nvtx {
            enabled : bool
            entered : bool
            record_shapes : bool
          }
          class enable_grad {
            prev
          }
          class env {
          }
          class env {
          }
          class expectedFailure {
            device_type
          }
          class float16 {
            name : str
            torch_dtype
            typecode : str
          }
          class float32 {
            name : str
            torch_dtype
            typecode : str
          }
          class float64 {
            name : str
            torch_dtype
            typecode : str
          }
          class floating {
            name : str
          }
          class foreach_inputs_sample_func {
            arity : int
            sample_zero_size_tensor_inputs(opinfo, device, dtype, requires_grad)
          }
          class foreach_max_sample_func {
            sample_zero_size_tensor_inputs(opinfo, device, dtype, requires_grad)
          }
          class foreach_norm_sample_func {
            sample_zero_size_tensor_inputs(opinfo, device, dtype, requires_grad)
          }
          class foreach_pointwise_sample_func {
            sample_zero_size_tensor_inputs(opinfo, device, dtype, requires_grad)
          }
          class functional_datapipe {
            enable_df_api_tracing : bool
            name : str
          }
          class generic {
            name : str
          }
          class graph {
            capture_error_mode : str
            capture_stream : NoneType
            cuda_graph
            default_capture_stream : Optional[typing.Optional['torch.cuda.Stream']]
            pool : tuple
            stream_ctx
          }
          class group {
          }
          class guaranteed_datapipes_determinism {
            prev : bool
          }
          class halide {
            asserts : bool
            cpu_target : str
            debug : bool
            gpu_target : str
            scan_kernels : bool
            scheduler_cpu : str
            scheduler_cuda : str
          }
          class inexact {
            name : str
          }
          class inference_mode {
            mode : bool
            clone() 'inference_mode'
          }
          class int16 {
            name : str
            torch_dtype
            typecode : str
          }
          class int32 {
            name : str
            torch_dtype
            typecode : str
          }
          class int64 {
            name : str
            torch_dtype
            typecode : str
          }
          class int8 {
            name : str
            torch_dtype
            typecode : str
          }
          class integer {
            name : str
          }
          class lazy_property {
            wrapped
          }
          class macros {
            local_rank : str
            substitute(args: List[Any], local_rank: str) List[str]
          }
          class modules {
            allowed_dtypes : NoneType, set
            module_info_list : list
            skip_if_dynamo : bool
            train_eval_mode
          }
          class ndarray {
            T
            conj
            conjugate
            data
            dtype
            flags
            fn
            imag
            itemsize
            ivar : str
            method : str
            name : NoneType
            nbytes
            ndim
            plain : str
            put
            real
            rvar : str
            shape
            size
            strides
            take
            tensor
            astype(dtype, order, casting, subok, copy)
            copy(order: NotImplementedType)
            fill(value: ArrayLike)
            flatten(order: NotImplementedType)
            is_integer()
            item()
            reshape()
            resize()
            sort(axis, kind, order)
            tolist()
            transpose()
            view(dtype, type)
          }
          class nested {
            e_bool : bool
          }
          class no_grad {
            prev : bool
          }
          class non_deterministic {
            cls : Optional[Type[IterDataPipe]]
            deterministic_fn : Callable[[], bool]
            deterministic_wrapper_fn() IterDataPipe
          }
          class number {
            name : str
          }
          class numpy_method_wrapper {
            method : str
          }
          class numpy_operator_wrapper {
            op : Callable[..., Any]
          }
          class numpy_to_tensor_wrapper {
            f
          }
          class onlyOn {
            device_type
          }
          class ops {
            allowed_dtypes : NoneType, set
            op_list : list
            opinfo_dtypes : supported
            skip_if_dynamo : bool
          }
          class optims {
            dtypes : NoneType, list
            optim_info_list : list
          }
          class parametrize {
            arg_names : List[str]
            arg_values
            name_fn : NoneType
          }
          class precisionOverride {
            d
          }
          class profile {
            enabled : bool
            entered : bool
            function_events : NoneType
            profile_memory : bool
            profiler_kind
            record_shapes : bool
            self_cpu_time_total
            use_cuda : bool
            with_flops : bool
            with_modules : bool
            with_stack : bool
            config()
            export_chrome_trace(path)
            export_stacks(path: str, metric: str)
            key_averages(group_by_input_shape, group_by_stack_n)
            table(sort_by, row_limit, max_src_column_width, max_name_column_width, max_shapes_column_width, header, top_level_events_only)
            total_average()
          }
          class profile {
            acc_events : bool
            custom_trace_id_callback : NoneType
            enabled : bool
            entered : bool
            experimental_config : NoneType
            function_events
            kineto_activities : set
            kineto_results : Optional[_ProfilerResult]
            profile_memory : bool
            profiler_kind
            profiling_end_time_ns : int
            profiling_start_time_ns : int
            record_shapes : bool
            self_cpu_time_total
            trace_id : str
            use_cpu : bool
            use_cuda : bool
            use_device : NoneType, Optional[str]
            with_flops : bool
            with_modules : bool
            with_stack : bool
            config(create_trace_id)
            create_trace_id()
            default_trace_id()
            export_chrome_trace(path)
            export_stacks(path: str, metric: str)
            key_averages(group_by_input_shape, group_by_stack_n)
            table(sort_by, row_limit, max_src_column_width, max_name_column_width, max_shapes_column_width, header, top_level_events_only)
            toggle_collection_dynamic(enabled: bool, activities: Iterable[ProfilerActivity])
            total_average()
          }
          class profile {
            action_map : Dict[Tuple[ProfilerAction, Optional[ProfilerAction]], List[Any]]
            current_action : RECORD
            custom_trace_id_callback
            on_trace_ready : NoneType
            record_steps : bool
            schedule : NoneType
            step_num : int
            step_rec_fn : Optional[prof.record_function]
            get_trace_id()
            set_custom_trace_id_callback(callback)
            start()
            step()
            stop()
          }
          class record_function {
            args : Optional[str]
            name : str
            record : NoneType
            run_callbacks_on_exit : bool
          }
          class reparametrize {
            adapter_fn
            parametrizer
          }
          class rocm {
            arch : List[str]
            ck_dir : NoneType
            ck_supported_arch : List[str]
            compile_opt_level : str
            flush_denormals : bool
            generate_test_runner : bool
            is_debug : bool
            n_max_profiling_configs : Optional[int]
            print_kernel_resource_usage : bool
            rocm_home : Optional[str]
            save_temps : bool
            use_fast_math : bool
            use_preselected_instances : bool
          }
          class runtime_validation_disabled {
            prev : bool
          }
          class safe_globals {
          }
          class sample_skips_and_xfails {
            rules
          }
          class save_on_cpu {
          }
          class saved_tensors_hooks {
            pack_hook : Callable[[torch.Tensor], Any]
            unpack_hook : Callable[[Any], torch.Tensor]
          }
          class set_default_mmap_options {
            prev
          }
          class set_detect_anomaly {
            prev
            prev_check_nan
          }
          class set_grad_enabled {
            mode : bool
            prev
            clone() 'set_grad_enabled'
          }
          class set_multithreading_enabled {
            mode : bool
            prev
            clone() 'set_multithreading_enabled'
          }
          class set_stance {
            prev
            stance
            clone()
          }
          class signedinteger {
            name : str
          }
          class size_bytes {
            output_size : int
            total_size : int
          }
          class skipCPUIf {
          }
          class skipCUDAIf {
          }
          class skipGPUIf {
          }
          class skipHPUIf {
          }
          class skipIf {
            dep
            device_type : NoneType
            reason
          }
          class skipLazyIf {
          }
          class skipMPSIf {
          }
          class skipMetaIf {
          }
          class skipPRIVATEUSE1If {
          }
          class skipXLAIf {
          }
          class skipXPUIf {
          }
          class skip_data {
            materialize_fake_tensors : bool
          }
          class strict_fusion {
          }
          class subtest {
            arg_values
            decorators : list
            name : NoneType
          }
          class suppress_warnings {
            log : list
            filter(category, message, module)
            record(category, message, module)
          }
          class swap {
            swap_values
          }
          class test_configs {
            force_extern_kernel_in_multi_template : bool
            max_mm_configs : Optional[int]
            runtime_triton_dtype_assert : bool
          }
          class toleranceOverride {
            d
          }
          class trace {
            compile_profile : bool
            debug_dir : Optional[str]
            debug_log : bool
            dot_graph_shape : NoneType
            draw_orig_fx_graph
            enabled
            fx_graph : bool
            fx_graph_transformed : bool
            graph_diagram
            info_log : bool
            ir_post_fusion : bool
            ir_pre_fusion : bool
            log_autotuning_results : bool
            log_url_for_graph_xform : NoneType
            output_code : bool
            save_real_tensors
            upload_tar : Optional[Callable[[str], None]]
          }
          class triton {
            autotune_at_compile_time : Optional[bool]
            autotune_cublasLt : bool
            autotune_pointwise : bool
            codegen_upcast_to_fp32 : bool
            cooperative_reductions
            cudagraph_dynamic_shape_warn_limit : Optional[int]
            cudagraph_skip_dynamic_graphs : bool
            cudagraph_support_input_mutation : bool
            cudagraph_trees : bool
            cudagraph_trees_history_recording : bool
            cudagraph_unexpected_rerecord_limit : int
            cudagraphs
            debug_sync_graph : bool
            debug_sync_kernel : bool
            dense_indexing : bool
            descriptive_names : str
            divisible_by_16 : bool
            enable_persistent_tma_matmul
            fast_path_cudagraph_asserts : bool
            force_cooperative_reductions : bool
            force_cudagraph_sync : bool
            force_cudagraphs_warmup : bool
            inject_relu_bug_TESTING_ONLY : Optional[str]
            max_tiles : int
            min_split_scan_rblock : int
            multi_kernel : int
            persistent_reductions
            prefer_nd_tiling : bool
            skip_cudagraph_warmup : bool
            slow_path_cudagraph_asserts : bool
            spill_threshold : int
            store_cubin : bool
            tiling_prevents_pointwise_fusion : bool
            tiling_prevents_reduction_fusion : bool
            unique_kernel_names
            use_block_ptr : bool
          }
          class uint16 {
            name : str
            torch_dtype
            typecode : str
          }
          class uint32 {
            name : str
            torch_dtype
            typecode : str
          }
          class uint64 {
            name : str
            torch_dtype
            typecode : str
          }
          class uint8 {
            name : str
            torch_dtype
            typecode : str
          }
          class unsignedinteger {
            name : str
          }
          class verbose {
            enable
          }
          class verbose {
            level
          }
          BFloat16Storage --|> _LegacyStorage
          BoolStorage --|> _LegacyStorage
          ByteStorage --|> _LegacyStorage
          CharStorage --|> _LegacyStorage
          ComplexDoubleStorage --|> _LegacyStorage
          ComplexFloatStorage --|> _LegacyStorage
          DoubleStorage --|> _LegacyStorage
          FloatStorage --|> _LegacyStorage
          HalfStorage --|> _LegacyStorage
          IntStorage --|> _LegacyStorage
          LongStorage --|> _LegacyStorage
          QInt32Storage --|> _LegacyStorage
          QInt8Storage --|> _LegacyStorage
          QUInt2x4Storage --|> _LegacyStorage
          QUInt4x2Storage --|> _LegacyStorage
          QUInt8Storage --|> _LegacyStorage
          ShortStorage --|> _LegacyStorage
          ModIndex --|> Function
          TraceWrapped --|> HigherOrderOperator
          TransformGetItemToIndex --|> TorchFunctionMode
          SubmodCompiler --|> Interpreter
          WrapperModule --|> Module
          TracableCreateParameter --|> Function
          set_stance --|> _DecoratorContextManager
          CpuInterface --|> DeviceInterface
          CudaInterface --|> DeviceInterface
          XpuInterface --|> DeviceInterface
          DisableContext --|> _TorchDynamoContext
          FlattenInputOutputSignature --|> Transformer
          OptimizeContext --|> _TorchDynamoContext
          OptimizedModule --|> Module
          RunOnlyContext --|> _TorchDynamoContext
          ArgsMismatchError --|> Unsupported
          AttributeMutationError --|> Unsupported
          BackendCompilerFailed --|> TorchDynamoException
          CompileCollectiveRestartAnalysis --|> RestartAnalysis
          CondOpArgsMismatchError --|> ArgsMismatchError
          InternalTorchDynamoError --|> TorchDynamoException
          InvalidBackend --|> TorchDynamoException
          ObservedAttributeError --|> ObservedException
          ObservedException --|> TorchDynamoException
          ObservedKeyError --|> ObservedException
          ObservedUserStopIteration --|> ObservedException
          RecompileError --|> TorchDynamoException
          RecompileLimitExceeded --|> Unsupported
          ResetRequired --|> TorchDynamoException
          RestartAnalysis --|> TorchDynamoException
          SkipCodeRecursiveException --|> TorchDynamoException
          SkipFrame --|> TorchDynamoException
          SpeculationRestartAnalysis --|> RestartAnalysis
          TensorifyScalarRestartAnalysis --|> RestartAnalysis
          TorchRuntimeError --|> TorchDynamoException
          UncapturedHigherOrderOpError --|> TorchDynamoException
          UnsafeScriptObjectError --|> TorchDynamoException
          UnspecializeRestartAnalysis --|> RestartAnalysis
          Unsupported --|> TorchDynamoException
          UserError --|> Unsupported
          DeletedGuardManagerWrapper --|> GuardManagerWrapper
          GuardBuilder --|> GuardBuilderBase
          IndentedBufferWithPrefix --|> IndentedBuffer
          FakeRootModule --|> Module
          SubgraphTracer --|> Tracer
          NoEnterTorchFunctionMode --|> BaseTorchFunctionMode
          ExactReaderInterp --|> Interpreter
          ReaderInterp --|> Interpreter
          WriterInterp --|> Interpreter
          AttrProxySource --|> ChainedSource
          AttrSource --|> ChainedSource
          BackwardStateSource --|> Source
          CallFunctionNoArgsSource --|> WeakRefCallSource
          CallMethodItemSource --|> ChainedSource
          ConstDictKeySource --|> GetItemSource
          ConstantSource --|> Source
          ConvertIntSource --|> ChainedSource
          DefaultsSource --|> ChainedSource
          EphemeralSource --|> Source
          FSDPNNModuleSource --|> NNModuleSource
          FlattenScriptObjectSource --|> ChainedSource
          FloatTensorSource --|> ChainedSource
          GetItemSource --|> ChainedSource
          GlobalSource --|> Source
          GlobalStateSource --|> Source
          GlobalWeakRefSource --|> Source
          GradSource --|> ChainedSource
          LocalCellSource --|> Source
          LocalSource --|> Source
          NNModuleSource --|> ChainedSource
          NegateSource --|> ChainedSource
          NumpyTensorSource --|> ChainedSource
          ODictGetItemSource --|> ChainedSource
          OptimizerSource --|> ChainedSource
          ParamBufferSource --|> AttrSource
          RandomValueSource --|> Source
          ScriptObjectQualifiedNameSource --|> ChainedSource
          ShapeEnvSource --|> Source
          SubclassAttrListSource --|> ChainedSource
          SyntheticLocalSource --|> Source
          TensorPropertySource --|> ChainedSource
          TorchFunctionModeStackSource --|> Source
          TupleIteratorGetItemSource --|> GetItemSource
          TypeSource --|> ChainedSource
          UnspecializedBuiltinNNModuleSource --|> UnspecializedNNModuleSource
          UnspecializedNNModuleSource --|> NNModuleSource
          UnspecializedParamBufferSource --|> AttrSource
          WeakRefCallSource --|> ChainedSource
          InliningGeneratorInstructionTranslator --|> InliningInstructionTranslator
          InliningInstructionTranslator --|> InstructionTranslatorBase
          InstructionTranslator --|> InstructionTranslatorBase
          TestCase --|> TestCase
          MinifierTestBase --|> TestCase
          CleanupManager --|> ExactWeakKeyDictionary
          GmWrapper --|> Module
          AttributeMutation --|> MutationType
          AttributeMutationExisting --|> AttributeMutation
          AttributeMutationNew --|> AttributeMutation
          ValueMutationExisting --|> MutationType
          ValueMutationNew --|> MutationType
          BackwardStateGraphArg --|> GraphArg
          BuiltinVariable --|> VariableTracker
          ConstantVariable --|> VariableTracker
          EnumVariable --|> VariableTracker
          AutocastModeVariable --|> ContextWrappingVariable
          CUDADeviceVariable --|> ContextWrappingVariable
          CatchWarningsCtxManagerVariable --|> ContextWrappingVariable
          ContextWrappingVariable --|> VariableTracker
          DeterministicAlgorithmsVariable --|> ContextWrappingVariable
          DisabledSavedTensorsHooksVariable --|> ContextWrappingVariable
          DualLevelContextManager --|> ContextWrappingVariable
          EventVariable --|> VariableTracker
          FSDPParamGroupUseTrainingStateVariable --|> ContextWrappingVariable
          GenericContextWrappingVariable --|> UserDefinedObjectVariable
          GradIncrementNestingCtxManagerVariable --|> ContextWrappingVariable
          GradInplaceRequiresGradCtxManagerVariable --|> ContextWrappingVariable
          GradModeVariable --|> ContextWrappingVariable
          InferenceModeVariable --|> ContextWrappingVariable
          JvpIncrementNestingCtxManagerVariable --|> ContextWrappingVariable
          NullContextVariable --|> ContextWrappingVariable
          PreserveVersionContextVariable --|> ContextWrappingVariable
          SDPAKernelVariable --|> ContextWrappingVariable
          SetFwdGradEnabledContextManager --|> ContextWrappingVariable
          StreamContextVariable --|> ContextWrappingVariable
          StreamVariable --|> VariableTracker
          TorchFunctionDisableVariable --|> ContextWrappingVariable
          VmapIncrementNestingCtxManagerVariable --|> ContextWrappingVariable
          WithExitFunctionVariable --|> VariableTracker
          ConstDictVariable --|> VariableTracker
          CustomizedDictVariable --|> ConstDictVariable
          DefaultDictVariable --|> ConstDictVariable
          DictKeys --|> DictView
          DictValues --|> DictView
          DictView --|> VariableTracker
          FrozensetVariable --|> SetVariable
          HFPretrainedConfigVariable --|> VariableTracker
          PythonSysModulesVariable --|> VariableTracker
          SetVariable --|> ConstDictVariable
          BackwardHookVariable --|> VariableTracker
          DeviceMeshVariable --|> DistributedVariable
          DistributedVariable --|> VariableTracker
          PlacementClassVariable --|> DistributedVariable
          PlacementVariable --|> DistributedVariable
          ProcessGroupVariable --|> DistributedVariable
          WorldMetaClassVariable --|> DistributedVariable
          BaseUserFunctionVariable --|> VariableTracker
          CollectiveFunctionRewriteVariable --|> UserFunctionVariable
          CreateTMADescriptorVariable --|> VariableTracker
          DynamoTritonHOPifier --|> TritonHOPifier
          FunctoolsPartialVariable --|> VariableTracker
          NestedUserFunctionVariable --|> BaseUserFunctionVariable
          PolyfilledFunctionVariable --|> VariableTracker
          SkipFunctionVariable --|> VariableTracker
          TMADescriptorVariable --|> VariableTracker
          TritonKernelVariable --|> VariableTracker
          UserFunctionVariable --|> BaseUserFunctionVariable
          UserMethodVariable --|> UserFunctionVariable
          WrappedUserFunctionVariable --|> UserFunctionVariable
          WrappedUserMethodVariable --|> UserMethodVariable
          WrapperUserFunctionVariable --|> VariableTracker
          AssociativeScanHigherOrderVariable --|> TorchHigherOrderOperatorVariable
          AutoFunctionalizeHigherOrderVariable --|> TorchHigherOrderOperatorVariable
          AutogradFunctionApplyVariable --|> VariableTracker
          CallTorchbindHigherOrderVariable --|> TorchHigherOrderOperatorVariable
          CheckpointHigherOrderVariable --|> WrapHigherOrderVariable
          CondHigherOrderVariable --|> TorchHigherOrderOperatorVariable
          ExecutorchCallDelegateHigherOrderVariable --|> TorchHigherOrderOperatorVariable
          ExportTracepointHigherOrderVariable --|> TorchHigherOrderOperatorVariable
          FlexAttentionHigherOrderVariable --|> TorchHigherOrderOperatorVariable
          FunctionalCallVariable --|> FunctorchHigherOrderVariable
          FunctorchHigherOrderVariable --|> UserFunctionVariable
          HintsWrapperHigherOrderVariable --|> TorchHigherOrderOperatorVariable
          InvokeSubgraphHigherOrderVariable --|> WrapHigherOrderVariable
          MapHigherOrderVariable --|> TorchHigherOrderOperatorVariable
          OutDtypeHigherOrderVariable --|> TorchHigherOrderOperatorVariable
          PrimHOPBaseVariable --|> WrapHigherOrderVariable
          RunWithRNGStateHigherOrderVariable --|> TorchHigherOrderOperatorVariable
          ScanHigherOrderVariable --|> TorchHigherOrderOperatorVariable
          StrictModeHigherOrderVariable --|> TorchHigherOrderOperatorVariable
          TorchHigherOrderOperatorVariable --|> VariableTracker
          TraceWrappedHigherOrderOperatorVariable --|> TorchHigherOrderOperatorVariable
          WhileLoopHigherOrderVariable --|> TorchHigherOrderOperatorVariable
          WrapHigherOrderVariable --|> TorchHigherOrderOperatorVariable
          WrapWithAutocastHigherOrderVariable --|> TorchHigherOrderOperatorVariable
          WrapWithSetGradEnabledHigherOrderVariable --|> TorchHigherOrderOperatorVariable
          CountIteratorVariable --|> IteratorVariable
          CycleIteratorVariable --|> IteratorVariable
          FilterVariable --|> IteratorVariable
          IteratorVariable --|> VariableTracker
          ItertoolsVariable --|> VariableTracker
          MapVariable --|> ZipVariable
          RepeatIteratorVariable --|> IteratorVariable
          ZipVariable --|> IteratorVariable
          LazyVariableTracker --|> VariableTracker
          BaseListVariable --|> VariableTracker
          CommonListMethodsVariable --|> BaseListVariable
          DequeVariable --|> CommonListMethodsVariable
          ListIteratorVariable --|> IteratorVariable
          ListVariable --|> CommonListMethodsVariable
          NamedTupleVariable --|> TupleVariable
          RangeVariable --|> BaseListVariable
          RestrictedListSubclassVariable --|> ListVariable
          SizeVariable --|> TupleVariable
          SliceVariable --|> BaseListVariable
          TupleIteratorVariable --|> ListIteratorVariable
          TupleVariable --|> BaseListVariable
          AutogradEngineVariable --|> UserDefinedObjectVariable
          AutogradFunctionContextVariable --|> UserDefinedObjectVariable
          AutogradFunctionVariable --|> VariableTracker
          CellVariable --|> VariableTracker
          ComptimeVariable --|> VariableTracker
          ConstantLikeVariable --|> VariableTracker
          ConstantRegexMatchVariable --|> ConstantLikeVariable
          DebuggingVariable --|> VariableTracker
          DelayGraphBreakVariable --|> UnknownVariable
          DeletedVariable --|> VariableTracker
          ExceptionVariable --|> VariableTracker
          GetAttrVariable --|> VariableTracker
          GetSetDescriptorVariable --|> VariableTracker
          InspectBoundArgumentsVariable --|> VariableTracker
          InspectParameterVariable --|> VariableTracker
          InspectSignatureVariable --|> VariableTracker
          LambdaVariable --|> VariableTracker
          LoggingLoggerVariable --|> VariableTracker
          MethodWrapperVariable --|> VariableTracker
          NewGlobalVariable --|> VariableTracker
          NullVariable --|> VariableTracker
          NumpyDTypeVariable --|> ConstantLikeVariable
          NumpyTypeInfoVariable --|> ConstantLikeVariable
          NumpyVariable --|> VariableTracker
          PythonModuleVariable --|> VariableTracker
          RandomClassVariable --|> VariableTracker
          RandomVariable --|> VariableTracker
          RegexPatternVariable --|> ConstantLikeVariable
          StringFormatVariable --|> VariableTracker
          SuperVariable --|> VariableTracker
          TorchVersionVariable --|> ConstantLikeVariable
          TypingVariable --|> VariableTracker
          UnknownVariable --|> VariableTracker
          WeakRefVariable --|> VariableTracker
          FSDPManagedNNModuleVariable --|> UnspecializedNNModuleVariable
          NNModuleVariable --|> VariableTracker
          UnspecializedBuiltinNNModuleVariable --|> UnspecializedNNModuleVariable
          UnspecializedNNModuleVariable --|> UserDefinedObjectVariable
          OptimizerVariable --|> UserDefinedObjectVariable
          TorchScriptObjectVariable --|> UserDefinedObjectVariable
          SDPAParamsVariable --|> VariableTracker
          DataPtrVariable --|> VariableTracker
          FakeItemVariable --|> TensorVariable
          NumpyNdarrayVariable --|> TensorVariable
          SymNodeVariable --|> VariableTracker
          TensorSubclassVariable --|> VariableTracker
          TensorVariable --|> VariableTracker
          UnspecializedPythonVariable --|> TensorVariable
          UntypedStorageVariable --|> VariableTracker
          BaseTorchVariable --|> VariableTracker
          TorchCtxManagerClassVariable --|> BaseTorchVariable
          TorchInGraphFunctionVariable --|> BaseTorchVariable
          TensorWithTFOverrideVariable --|> TensorVariable
          TorchFunctionModeStackVariable --|> VariableTracker
          TorchFunctionModeVariable --|> GenericContextWrappingVariable
          GetMethodMode --|> BaseTorchFunctionMode
          FrozenDataClassVariable --|> UserDefinedObjectVariable
          KeyedJaggedTensorVariable --|> UserDefinedObjectVariable
          MutableMappingVariable --|> UserDefinedObjectVariable
          RandomVariable --|> UserDefinedObjectVariable
          RemovableHandleVariable --|> VariableTracker
          SourcelessGraphModuleVariable --|> UserDefinedObjectVariable
          UserDefinedClassVariable --|> UserDefinedVariable
          UserDefinedObjectVariable --|> UserDefinedVariable
          UserDefinedVariable --|> VariableTracker
          ExplainTS2FXGraphConverter --|> TS2FXGraphConverter
          AssumeConstantResult --|> Module
          AutogradFunction --|> Module
          MyAutogradFunction --|> Function
          ClassMethod --|> Module
          CondBranchClassMethod --|> Module
          MySubModule --|> Module
          CondBranchNestedFunction --|> Module
          CondBranchNonlocalVariables --|> Module
          CondClosedOverVariable --|> Module
          CondOperands --|> Module
          CondPredicate --|> Module
          ConstrainAsSizeExample --|> Module
          ConstrainAsValueExample --|> Module
          Decorator --|> Module
          Dictionary --|> Module
          DynamicShapeAssert --|> Module
          DynamicShapeConstructor --|> Module
          DynamicShapeIfGuard --|> Module
          DynamicShapeMap --|> Module
          DynamicShapeRound --|> Module
          DynamicShapeSlicing --|> Module
          DynamicShapeView --|> Module
          FnWithKwargs --|> Module
          ListContains --|> Module
          ListUnpack --|> Module
          ModelAttrMutation --|> Module
          NestedFunction --|> Module
          NullContextManager --|> Module
          OptionalInput --|> Module
          PytreeFlatten --|> Module
          ScalarOutput --|> Module
          SpecializedAttribute --|> Module
          StaticForLoop --|> Module
          StaticIf --|> Module
          TensorSetattr --|> Module
          TypeReflectionMethod --|> Module
          TorchSymMin --|> Module
          UserInputMutation --|> Module
          _NonStrictTorchFunctionHandler --|> TorchFunctionMode
          _ExportPassBaseDeprecatedDoNotUse --|> PassBase
          ExportInterpreter --|> Interpreter
          ExportTracer --|> PythonKeyTracer
          _AddRuntimeAssertionsForInlineConstraintsPass --|> PassBase
          CollectTracepointsPass --|> PassBase
          ConstantFolder --|> Interpreter
          _FunctionalizeSideEffectfulOpsPass --|> _ExportPassBaseDeprecatedDoNotUse
          _RemoveRuntimeAssertionsPass --|> PassBase
          ReplaceViewOpsWithViewCopyOpsPass --|> _ExportPassBaseDeprecatedDoNotUse
          Argument --|> _Union
          ConstantValue --|> _Union
          InputSpec --|> _Union
          OptionalTensorArgument --|> _Union
          OutputSpec --|> _Union
          SymBool --|> _Union
          SymBoolArgument --|> _Union
          SymExprHint --|> _Union
          SymFloat --|> _Union
          SymFloatArgument --|> _Union
          SymInt --|> _Union
          SymIntArgument --|> _Union
          TrainingIRVerifier --|> Verifier
          ExportTracepoint --|> HigherOrderOperator
          AOTAutogradCacheDetails --|> FxGraphHashDetails
          AOTAutogradCachePickler --|> FxGraphCachePickler
          CompiledBackward --|> FXGraphCacheLoadable
          CompiledForward --|> FXGraphCacheLoadable
          FXGraphCacheMiss --|> BypassAOTAutogradCache
          AOTDedupeWrapper --|> CompilerWrapper
          CompiledFunction --|> Function
          CompiledFunctionBackward --|> Function
          AOTDispatchSubclassWrapper --|> CompilerWrapper
          AOTSyntheticBaseWrapper --|> CompilerWrapper
          DebugAssertWrapper --|> CompilerWrapper
          EffectTokensWrapper --|> CompilerWrapper
          FakifiedOutWrapper --|> CompilerWrapper
          FunctionalizedRngRuntimeWrapper --|> CompilerWrapper
          RuntimeWrapper --|> CompilerWrapper
          UnwrapTensorSubclass --|> Module
          SerializableAOTDispatchCompiler --|> AOTDispatchCompiler
          AOTModule --|> Module
          AutogradFunctionApply --|> HigherOrderOperator
          ApplyTemplate --|> Function
          CtxCustomSave --|> WrappedCtx
          CtxWithSavedTensors --|> WrappedCtx
          CustomFunctionHigherOrderOperator --|> HigherOrderOperator
          DebugInterpreter --|> Interpreter
          ConcreteProp --|> Interpreter
          FunctionalModule --|> Module
          FunctionalModuleWithBuffers --|> Module
          FunctionalizeInterpreter --|> FuncTorchInterpreter
          GradInterpreter --|> FuncTorchInterpreter
          JvpInterpreter --|> FuncTorchInterpreter
          VmapInterpreter --|> FuncTorchInterpreter
          ChainedSource --|> Source
          DuplicateInputs --|> GuardEnvExpr
          GlobalContext --|> Checkpointable
          GuardsContext --|> Checkpointable
          InvokeSubgraphCache --|> HopSubgraphCache
          ModuleContext --|> Checkpointable
          StorageOverlap --|> GuardEnvExpr
          AssociativeScanOp --|> HigherOrderOperator
          AliasViewInfo --|> ViewInfo
          AsStridedViewInfo --|> ViewInfo
          AutoFunctionalized --|> HigherOrderOperator
          AutoFunctionalizedV2 --|> HigherOrderOperator
          NotView --|> ViewInfo
          SliceViewInfo --|> ViewInfo
          CondAutogradOp --|> Function
          CondOp --|> HigherOrderOperator
          WithEffects --|> HigherOrderOperator
          ExecutorchCallDelegate --|> HigherOrderOperator
          FlexAttentionAutogradOp --|> Function
          FlexAttentionBackwardHOP --|> HigherOrderOperator
          FlexAttentionHOP --|> HigherOrderOperator
          ForeachMap --|> PrimHOPBase
          HintsWrapper --|> HigherOrderOperator
          InvokeSubgraphAutogradOp --|> Function
          InvokeSubgraphHOP --|> HigherOrderOperator
          MapAutogradOp --|> Function
          MapImpl --|> HigherOrderOperator
          MapWrapper --|> HigherOrderOperator
          OutDtypeOperator --|> HigherOrderOperator
          PrimHOPBase --|> HigherOrderOperator
          PrimHOPBaseFunction --|> Function
          RunConstGraph --|> HigherOrderOperator
          ScanOp --|> HigherOrderOperator
          StrictMode --|> HigherOrderOperator
          CallTorchBind --|> HigherOrderOperator
          TracingTritonHOPifier --|> TritonHOPifier
          TritonKernelWrapperFunctional --|> HigherOrderOperator
          TritonKernelWrapperMutation --|> HigherOrderOperator
          WhileLoopOp --|> HigherOrderOperator
          TagActivationCheckpoint --|> HigherOrderOperator
          Wrap --|> HigherOrderOperator
          WrapActivationCheckpoint --|> HigherOrderOperator
          WrapWithAutocast --|> HigherOrderOperator
          WrapWithSetGradEnabled --|> HigherOrderOperator
          MMRankingA100 --|> LearnedHeuristicDecision
          MMRankingH100 --|> LearnedHeuristicDecision
          MixedMMA100 --|> LearnedHeuristicDecision
          MixedMMH100 --|> LearnedHeuristicDecision
          PadMMA100 --|> LearnedHeuristicRegression
          AutoHeuristicSelectAlgorithm --|> AutoHeuristic
          LearnedHeuristicDecision --|> LearnedHeuristic
          LearnedHeuristicRegression --|> LearnedHeuristic
          CUDABenchmarkRequest --|> BenchmarkRequest
          CUDABenchmarkRequest --|> GPUDeviceBenchmarkMixin
          CppBenchmarkRequest --|> BenchmarkRequest
          CppBenchmarkRequest --|> CPUDeviceBenchmarkMixin
          TestBenchmarkRequest --|> BenchmarkRequest
          TritonBenchmarkRequest --|> BenchmarkRequest
          TritonCPUBenchmarkRequest --|> CPUDeviceBenchmarkMixin
          TritonCPUBenchmarkRequest --|> TritonBenchmarkRequest
          TritonGPUBenchmarkRequest --|> GPUDeviceBenchmarkMixin
          TritonGPUBenchmarkRequest --|> TritonBenchmarkRequest
          CppPythonBindingsCodeCache --|> CppCodeCache
          CppWrapperCodeCache --|> CppPythonBindingsCodeCache
          HalideCodeCache --|> CppPythonBindingsCodeCache
          LambdaFuture --|> CodeCacheFuture
          LocalCache --|> CacheBase
          PersistentCache --|> CacheBase
          TritonFuture --|> CodeCacheFuture
          BracesBuffer --|> IndentedBuffer
          CppWrapperKernelArgs --|> KernelArgs
          DeferredLine --|> DeferredLineBase
          Kernel --|> CodeGen
          OpOverrides --|> OpDecompositions
          PythonPrinter --|> PythonPrinter
          CppKernel --|> Kernel
          CppKernelProxy --|> CppKernel
          CppOverrides --|> OpOverrides
          CppScheduling --|> BaseScheduling
          CppTile2DKernel --|> CppVecKernel
          CppTile2DOverrides --|> CppVecOverrides
          CppVecKernel --|> CppKernel
          CppVecOverrides --|> CppOverrides
          CppWrapperKernelGroup --|> KernelGroup
          OuterLoopFusedKernel --|> CppKernel
          OuterLoopFusedSchedulerNode --|> FusedSchedulerNode
          CppBmmTemplate --|> CppGemmTemplate
          CppFlexAttentionTemplate --|> CppTemplate
          CppGemmTemplate --|> CppTemplate
          CppMicroBrgemm --|> CppMicroGemm
          CppMicroGemmAMX --|> CppMicroGemm
          CppMicroGemmFP32Vec --|> CppMicroGemm
          CppMicroGemmRef --|> CppMicroGemm
          CppTemplate --|> KernelTemplate
          CppTemplateCaller --|> ChoiceCaller
          CppTemplateKernel --|> CppKernel
          CppCSEVariable --|> CSEVariable
          CppPrinter --|> CppPrinter
          LocalizeBufferHandler --|> WrapperHandler
          CppWrapperCpu --|> PythonWrapperCodegen
          CppWrapperCpuArrayRef --|> CppWrapperCpu
          CppWrapperGpu --|> CppWrapperCpu
          DeferredGpuGridLine --|> DeferredLineBase
          DeferredGpuKernelLine --|> DeferredLineBase
          CpuDeviceOpOverrides --|> DeviceOpOverrides
          CUDACPPScheduling --|> BaseScheduling
          CUDAKernel --|> Kernel
          CUDATemplateCaller --|> ChoiceCaller
          CUDATemplateKernel --|> CUDAKernel
          CUDATemplate --|> KernelTemplate
          CUTLASSTemplate --|> CUDATemplate
          CUDADeviceOpOverrides --|> DeviceOpOverrides
          CUTLASS2xGemmTemplate --|> CUTLASSGemmTemplate
          CUTLASS3xGemmTemplate --|> CUTLASSGemmTemplate
          CUTLASSGemmTemplate --|> CUTLASSTemplate
          CUDACombinedScheduling --|> BaseScheduling
          HalideCSEVariable --|> CSEVariable
          HalideKernel --|> SIMDKernel
          HalideOverrides --|> OpOverrides
          HalidePrinter --|> PythonPrinter
          HalideScheduling --|> SIMDScheduling
          AllocFromPoolLine --|> PoolMemoryPlanningLine
          Allocation --|> AllocationTreeNode
          ClearCacheOnAllocateMixin --|> MemorySplitProtocol
          DeallocFromPoolLine --|> PoolMemoryPlanningLine
          Empty --|> AllocationTreeNode
          PoolMemoryPlanningLine --|> MemoryPlanningLine
          SpatialSplit --|> AllocationTreeNode
          SpatialSplit --|> ClearCacheOnAllocateMixin
          TemporalSplit --|> AllocationTreeNode
          TemporalSplit --|> ClearCacheOnAllocateMixin
          CKGroupedConvFwdTemplate --|> CKTemplate
          CKTemplate --|> ROCmTemplate
          CKGemmTemplate --|> CKTemplate
          ROCmBenchmarkRequest --|> BenchmarkRequest
          ROCmBenchmarkRequest --|> GPUDeviceBenchmarkMixin
          ROCmCPPScheduling --|> BaseScheduling
          ROCmKernel --|> Kernel
          ROCmTemplateCaller --|> ChoiceCaller
          ROCmTemplateKernel --|> ROCmKernel
          ROCmTemplate --|> KernelTemplate
          ROCmTemplateBuffer --|> TemplateBuffer
          IterationRangesEntry --|> IterationRanges
          IterationRangesRoot --|> IterationRanges
          SIMDKernel --|> Kernel
          SIMDScheduling --|> BaseScheduling
          DisableReduction --|> NodeScheduleMarker
          EnableReduction --|> NodeScheduleMarker
          TritonCSE --|> CSE
          TritonCSEVariable --|> CSEVariable
          TritonKernel --|> SIMDKernel
          TritonKernelOverrides --|> TritonOverrides
          TritonOverrides --|> OpOverrides
          TritonPrinter --|> PythonPrinter
          TritonScheduling --|> SIMDScheduling
          ComboKernel --|> Kernel
          TritonSplitScanKernel --|> TritonKernel
          AllocateLine --|> MemoryPlanningLine
          CommBufferAllocateLine --|> CommBufferLine
          CommBufferFreeLine --|> CommBufferLine
          CommBufferLine --|> WrapperLine
          EnterDeviceContextManagerLine --|> WrapperLine
          EnterSubgraphLine --|> WrapperLine
          ExitDeviceContextManagerLine --|> WrapperLine
          ExitSubgraphLine --|> WrapperLine
          FreeIfNotReusedLine --|> MemoryPlanningLine
          MemoryPlanningLine --|> WrapperLine
          NullLine --|> MemoryPlanningLine
          PythonWrapperCodegen --|> CodeGen
          ReuseLine --|> MemoryPlanningLine
          SubgraphPythonWrapperCodegen --|> PythonWrapperCodegen
          XPUDeviceOpOverrides --|> DeviceOpOverrides
          _InProcessFxCompile --|> FxCompile
          BinarySubsystem --|> Subsystem
          BisectSubsystem --|> Subsystem
          ConfigChange --|> BinarySubsystem
          ConstantFolder --|> Interpreter
          CppOptions --|> BuildOptionsBase
          CppTorchDeviceOptions --|> CppTorchOptions
          CppTorchOptions --|> CppOptions
          InvalidVecISA --|> VecISA
          VecAMX --|> VecAVX512
          VecAVX2 --|> VecISA
          VecAVX512 --|> VecISA
          VecNEON --|> VecISA
          VecSVE --|> VecISA
          VecVSX --|> VecISA
          VecZVECTOR --|> VecISA
          AliasesNewOutput --|> OutputAliasInfo
          AliasesPriorGraphOutput --|> OutputAliasInfo
          _UnaliasedStorage --|> OutputAliasInfo
          MemoryDep --|> Dep
          RecordLoadStore --|> KernelFormatterHandler
          StarDep --|> Dep
          WeakDep --|> Dep
          _RecordLoadStoreInner --|> MockHandler
          CUDACompileError --|> CppCompileError
          LoweringException --|> OperatorIssue
          MissingOperatorWithDecomp --|> OperatorIssue
          MissingOperatorWithoutDecomp --|> OperatorIssue
          ErasedTensor --|> Tensor
          BatchAddPostGradFusion --|> BatchPointwiseMathOpsPostGradFusion
          BatchClampPreGradFusion --|> BatchMathOpsPreGradFusion
          BatchDetachPreGradFusion --|> BatchMathOpsPreGradFusion
          BatchDivPostGradFusion --|> BatchPointwiseMathOpsPostGradFusion
          BatchFusion --|> GroupBatchFusionBase
          BatchLayernormFusion --|> BatchFusion
          BatchLinearLHSFusion --|> BatchFusion
          BatchMathOpsPreGradFusion --|> BatchPointwiseOpsFusionFactory
          BatchMulPostGradFusion --|> BatchPointwiseMathOpsPostGradFusion
          BatchNanToNumPreGradFusion --|> BatchMathOpsPreGradFusion
          BatchPointwiseMathOpsPostGradFusion --|> BatchPointwiseOpsFusionFactory
          BatchPointwiseOpsFusionFactory --|> BatchFusion
          BatchPointwiseOpsPostGradFusion --|> BatchPointwiseOpsFusionFactory
          BatchPointwiseOpsPreGradFusion --|> BatchPointwiseOpsFusionFactory
          BatchReLuPostGradFusion --|> BatchPointwiseOpsPostGradFusion
          BatchReLuPreGradFusion --|> BatchPointwiseOpsPreGradFusion
          BatchSigmoidPostGradFusion --|> BatchPointwiseOpsPostGradFusion
          BatchSigmoidPreGradFusion --|> BatchPointwiseOpsPreGradFusion
          BatchSubPostGradFusion --|> BatchPointwiseMathOpsPostGradFusion
          BatchTanhPostGradFusion --|> BatchPointwiseOpsPostGradFusion
          BatchTanhPreGradFusion --|> BatchPointwiseOpsPreGradFusion
          GroupFusion --|> GroupBatchFusionBase
          GroupLinearFusion --|> GroupFusion
          PostGradBatchLinearFusion --|> BatchFusion
          PreGradBatchLinearFusion --|> BatchFusion
          UniformValueConstantFolder --|> ConstantFolder
          _ScaledMatmul --|> _Matmul
          IdentityRemover --|> Transformer
          GetItem --|> CallFunction
          TorchSplit --|> CallFunction
          UnbindCatRemover --|> SplitCatSimplifier
          GraphLowering --|> Interpreter
          SubgraphLowering --|> GraphLowering
          AssertScalar --|> ExternKernel
          BaseConstant --|> IRNode
          BaseView --|> IRNode
          Buffer --|> IRNode
          CUDATemplateBuffer --|> TemplateBuffer
          CommBufferLayout --|> FixedLayout
          ComplexView --|> FallbackKernel
          ComputedBuffer --|> OperationBuffer
          ConcatKernel --|> NopKernel
          Conditional --|> ExternKernel
          Constant --|> BaseConstant
          ConstantBuffer --|> InputBuffer
          CppTemplateBuffer --|> TemplateBuffer
          DeviceCopy --|> ExternKernelOut
          DonatedBuffer --|> InputBuffer
          DtypeView --|> BaseView
          DynamicScalar --|> ExternKernel
          EffectfulKernel --|> FallbackKernel
          ExpandView --|> BaseView
          ExternKernel --|> InputsKernel
          ExternKernelAlloc --|> ExternKernel
          ExternKernelOut --|> ExternKernel
          FallbackKernel --|> ExternKernelAlloc
          FixedLayout --|> Layout
          FlexibleLayout --|> Layout
          GenericView --|> BaseView
          IndexPutFallback --|> ExternKernel
          IndexingConstant --|> BaseConstant
          InplaceBernoulliFallback --|> ExternKernel
          InplaceCopyFallback --|> ExternKernel
          InputBuffer --|> Buffer
          InputsKernel --|> OperationBuffer
          InvokeSubgraph --|> ExternKernel
          Layout --|> OutputSpec
          Loops --|> IRNode
          MultiOutput --|> ExternKernel
          MultiOutputLayout --|> OutputSpec
          MultiTemplateBuffer --|> TritonTemplateBuffer
          MutableBox --|> IRNode
          MutatingFirstArgExternKernel --|> ExternKernel
          MutationLayoutSHOULDREMOVE --|> Layout
          MutationOutput --|> Buffer
          NonOwningLayout --|> Layout
          NoneAsConstantBuffer --|> IRNode
          NoneLayout --|> OutputSpec
          NopKernel --|> InputsKernel
          OperationBuffer --|> Buffer
          OperationBuffer --|> Operation
          PermuteView --|> BaseView
          Pointwise --|> Loops
          RandomSeeds --|> ExternKernelOut
          Reduction --|> Loops
          ReinterpretView --|> BaseView
          ResizeStorageBytes --|> MutatingFirstArgExternKernel
          Scan --|> Loops
          Scatter --|> Pointwise
          ScatterFallback --|> ExternKernel
          SetSourceTensorKernel --|> ExternKernelAlloc
          ShapeAsConstantBuffer --|> IRNode
          SliceView --|> View
          Sort --|> Loops
          SplitScan --|> Scan
          SqueezeView --|> BaseView
          StorageBox --|> MutableBox
          Subgraph --|> IRNode
          TMADescriptor --|> ExternKernel
          TemplateBuffer --|> OperationBuffer
          TensorBox --|> MutableBox
          TorchBindObject --|> IRNode
          TritonTemplateBuffer --|> TemplateBuffer
          TritonTemplateCallerBase --|> ChoiceCaller
          UserDefinedTritonKernel --|> ExternKernel
          View --|> GenericView
          WelfordReduction --|> Reduction
          WhileLoop --|> ExternKernel
          _CollectiveKernel --|> FallbackKernel
          _WaitKernel --|> _CollectiveKernel
          InterpreterShim --|> Interpreter
          LightTracer --|> TracerBase
          CaptureIndexing --|> WrapperHandler
          ConvolutionBinary --|> ExternKernelAlloc
          ConvolutionBinaryInplace --|> ExternKernelAlloc
          ConvolutionTransposeUnary --|> ExternKernelAlloc
          ConvolutionUnary --|> ExternKernelAlloc
          LinearBinary --|> ExternKernelAlloc
          LinearUnary --|> ExternKernelAlloc
          MKLPackedLinear --|> ExternKernelAlloc
          MkldnnRnnLayer --|> ExternKernelAlloc
          QConvPointWiseBinaryPT2E --|> ExternKernelAlloc
          QConvPointWisePT2E --|> ExternKernelAlloc
          QLinearPointwiseBinaryPT2E --|> ExternKernelAlloc
          QLinearPointwisePT2E --|> ExternKernelAlloc
          MockBackend --|> RemoteCacheBackend
          _GlobalItemStats --|> Stats
          AddParenHandler --|> WrapperHandler
          ExtractConstantsHandler --|> NoopHandler
          SimpleCSEHandler --|> WrapperHandler
          CompiledAOTI --|> OutputCode
          CompiledFxGraph --|> OutputCode
          CompiledFxGraphConstantsWithGm --|> CompiledFxGraphConstants
          MockFXGraphCacheOutput --|> OutputCode
          Arg --|> PatternExpr
          CallFunction --|> _TargetArgsExpr
          CallFunctionVarArgs --|> _TargetExprVarArgs
          CallMethod --|> _TargetArgsExpr
          CallMethodVarArgs --|> _TargetExprVarArgs
          CallModule --|> _TargetArgsExpr
          CallModuleVarArgs --|> _TargetExprVarArgs
          ExclusiveKeywordArg --|> PatternExpr
          GraphPatternEntry --|> PatternEntry
          Ignored --|> PatternExpr
          KeywordArg --|> PatternExpr
          ListOf --|> PatternExpr
          LoweringPatternEntry --|> PatternEntry
          MultiOutputPattern --|> PatternExpr
          RepeatedExpr --|> PatternExpr
          ReplacementPatternEntry --|> PatternEntry
          Replacer --|> Interpreter
          _TargetArgsExpr --|> _TargetExpr
          _TargetExpr --|> PatternExpr
          _TargetExprVarArgs --|> _TargetExpr
          CopyGraph --|> Transformer
          Converter --|> Interpreter
          RedisRemoteCache --|> RemoteCache
          RedisRemoteCacheBackend --|> RemoteCacheBackend
          RemoteAOTAutogradCache --|> RedisRemoteCache
          RemoteAutotuneCache --|> RedisRemoteCache
          RemoteBundledAutotuneCache --|> RedisRemoteCache
          RemoteCacheJsonSerde --|> RemoteCacheSerde
          RemoteCachePassthroughSerde --|> RemoteCacheSerde
          RemoteDynamoPGOCache --|> RedisRemoteCache
          RemoteFxGraphCache --|> RedisRemoteCache
          LocalAutotuneCache --|> RemoteCache
          _LocalAutotuneCacheBackend --|> RemoteCacheBackend
          TritonBenchmarker --|> Benchmarker
          DebugAutotuner --|> CachingAutotuner
          ExternKernelSchedulerNode --|> BaseSchedulerNode
          ForeachKernelSchedulerNode --|> FusedSchedulerNode
          FusedSchedulerNode --|> BaseSchedulerNode
          GroupedSchedulerNode --|> BaseSchedulerNode
          NopKernelSchedulerNode --|> BaseSchedulerNode
          SchedulerDonatedBuffer --|> SchedulerBuffer
          SchedulerNode --|> BaseSchedulerNode
          AlgorithmSelectorCache --|> PersistentCache
          ExternKernelCaller --|> ChoiceCaller
          ModificationWrapper --|> WrapperHandler
          TritonTemplate --|> KernelTemplate
          TritonTemplateCaller --|> TritonTemplateCallerBase
          TritonTemplateKernel --|> TritonKernel
          StoreOutputSubstitution --|> WrapperHandler
          SimplifyIndexing --|> WrapperHandler
          PointwiseSubgraphLowering --|> Interpreter
          TracingOpsHandler --|> WrapperHandler
          TestCase --|> TestCase
          Realize --|> Function
          DelayReplaceLine --|> DeferredLineBase
          FakeIndentedBuffer --|> IndentedBuffer
          NullKernelHandler --|> NullHandler
          AsyncClosureHandler --|> ClosureHandler
          LOBPCGAutogradFunction --|> Function
          bool_ --|> generic
          complex128 --|> complexfloating
          complex64 --|> complexfloating
          complexfloating --|> inexact
          float16 --|> floating
          float32 --|> floating
          float64 --|> floating
          floating --|> inexact
          inexact --|> number
          int16 --|> signedinteger
          int32 --|> signedinteger
          int64 --|> signedinteger
          int8 --|> signedinteger
          integer --|> number
          number --|> generic
          signedinteger --|> integer
          uint16 --|> unsignedinteger
          uint32 --|> signedinteger
          uint64 --|> signedinteger
          uint8 --|> unsignedinteger
          unsignedinteger --|> integer
          HigherOrderOperator --|> OperatorBase
          OpOverload --|> OperatorBase
          TorchBindOpOverload --|> OpOverload
          _PyOpNamespace --|> _OpNamespace
          TorchRefsMode --|> TorchFunctionMode
          RunAndSaveRngState --|> HigherOrderOperator
          RunWithRngState --|> HigherOrderOperator
          BackwardsNotSupported --|> Function
          FakeCopyMode --|> TorchFunctionMode
          FakeTensor --|> Tensor
          FakeTensorMode --|> TorchDispatchMode
          CrossRefFakeMode --|> TorchDispatchMode
          CppFunctionalizeAPI --|> BaseFunctionalizeAPI
          FunctionalTensor --|> Tensor
          FunctionalTensorMode --|> TorchDispatchMode
          FunctorchFunctionalizeAPI --|> BaseFunctionalizeAPI
          PythonFunctionalizeAPI --|> BaseFunctionalizeAPI
          _CustomViewFunc --|> ViewFunc
          _FakeTensorViewFunc --|> ViewFunc
          SchemaCheckMode --|> TorchDispatchMode
          Version --|> _BaseVersion
          BNReLU2d --|> _FusedModule
          BNReLU3d --|> _FusedModule
          ConvAdd2d --|> _FusedModule
          ConvAddReLU2d --|> _FusedModule
          ConvBn1d --|> _FusedModule
          ConvBn2d --|> _FusedModule
          ConvBn3d --|> _FusedModule
          ConvBnReLU1d --|> _FusedModule
          ConvBnReLU2d --|> _FusedModule
          ConvBnReLU3d --|> _FusedModule
          ConvReLU1d --|> _FusedModule
          ConvReLU2d --|> _FusedModule
          ConvReLU3d --|> _FusedModule
          LinearBn1d --|> _FusedModule
          LinearLeakyReLU --|> _FusedModule
          LinearReLU --|> _FusedModule
          LinearTanh --|> _FusedModule
          _FusedModule --|> Sequential
          ConvBn1d --|> _ConvBnNd
          ConvBn1d --|> Conv1d
          ConvBn2d --|> _ConvBnNd
          ConvBn2d --|> Conv2d
          ConvBn3d --|> _ConvBnNd
          ConvBn3d --|> Conv3d
          ConvBnReLU1d --|> ConvBn1d
          ConvBnReLU2d --|> ConvBn2d
          ConvBnReLU3d --|> ConvBn3d
          ConvReLU1d --|> _FusedModule
          ConvReLU1d --|> Conv1d
          ConvReLU2d --|> _FusedModule
          ConvReLU2d --|> Conv2d
          ConvReLU3d --|> _FusedModule
          ConvReLU3d --|> Conv3d
          _ConvBnNd --|> _FusedModule
          _ConvBnNd --|> _ConvNd
          LinearBn1d --|> _FusedModule
          LinearBn1d --|> Linear
          LinearReLU --|> _FusedModule
          LinearReLU --|> Linear
          LinearReLU --|> Linear
          BNReLU2d --|> BatchNorm2d
          BNReLU3d --|> BatchNorm3d
          ConvAdd2d --|> Conv2d
          ConvAddReLU2d --|> Conv2d
          ConvReLU1d --|> Conv1d
          ConvReLU2d --|> Conv2d
          ConvReLU3d --|> Conv3d
          LinearLeakyReLU --|> Linear
          LinearReLU --|> Linear
          LinearTanh --|> Linear
          Linear --|> Linear
          Conv1d --|> _ConvNd
          Conv1d --|> Conv1d
          Conv2d --|> _ConvNd
          Conv2d --|> Conv2d
          Conv3d --|> _ConvNd
          Conv3d --|> Conv3d
          _ConvNd --|> _ConvNd
          Embedding --|> Embedding
          EmbeddingBag --|> EmbeddingBag
          Linear --|> Linear
          MultiheadAttention --|> MultiheadAttention
          LSTM --|> Module
          LSTMCell --|> Module
          _LSTMLayer --|> Module
          _LSTMSingleLayer --|> Module
          Conv1d --|> Conv1d
          Conv2d --|> Conv2d
          Conv3d --|> Conv3d
          ConvTranspose1d --|> ConvTranspose1d
          ConvTranspose2d --|> ConvTranspose2d
          ConvTranspose3d --|> ConvTranspose3d
          Linear --|> Linear
          GRU --|> RNNBase
          GRUCell --|> RNNCellBase
          LSTM --|> RNNBase
          LSTMCell --|> RNNCellBase
          PackedParameter --|> Module
          RNNBase --|> Module
          RNNCell --|> RNNCellBase
          RNNCellBase --|> Module
          DeQuantize --|> Module
          Quantize --|> Module
          ELU --|> ELU
          Hardswish --|> Hardswish
          LeakyReLU --|> LeakyReLU
          MultiheadAttention --|> MultiheadAttention
          PReLU --|> Module
          ReLU6 --|> ReLU
          Sigmoid --|> Sigmoid
          Softmax --|> Softmax
          BatchNorm2d --|> _BatchNorm
          BatchNorm3d --|> _BatchNorm
          _BatchNorm --|> _BatchNorm
          Conv1d --|> _ConvNd
          Conv2d --|> _ConvNd
          Conv3d --|> _ConvNd
          ConvTranspose1d --|> _ConvTransposeNd
          ConvTranspose2d --|> _ConvTransposeNd
          ConvTranspose3d --|> _ConvTransposeNd
          _ConvNd --|> WeightedQuantizedModule
          _ConvTransposeNd --|> _ConvNd
          Dropout --|> Dropout
          Embedding --|> Module
          EmbeddingBag --|> Embedding
          EmbeddingPackedParams --|> Module
          FXFloatFunctional --|> Module
          FloatFunctional --|> Module
          QFunctional --|> Module
          Linear --|> WeightedQuantizedModule
          LinearPackedParams --|> Module
          GroupNorm --|> GroupNorm
          InstanceNorm1d --|> InstanceNorm1d
          InstanceNorm2d --|> InstanceNorm2d
          InstanceNorm3d --|> InstanceNorm3d
          LayerNorm --|> LayerNorm
          LSTM --|> LSTM
          WeightedQuantizedModule --|> Module
          Conv1d --|> _ConvNd
          Conv1d --|> Conv1d
          Conv2d --|> _ConvNd
          Conv2d --|> Conv2d
          Conv3d --|> _ConvNd
          Conv3d --|> Conv3d
          ConvTranspose1d --|> _ConvTransposeNd
          ConvTranspose1d --|> ConvTranspose1d
          ConvTranspose2d --|> _ConvTransposeNd
          ConvTranspose2d --|> ConvTranspose2d
          ConvTranspose3d --|> _ConvTransposeNd
          ConvTranspose3d --|> ConvTranspose3d
          _ConvNd --|> ReferenceQuantizedModule
          _ConvNd --|> _ConvNd
          _ConvTransposeNd --|> _ConvNd
          _ConvTransposeNd --|> _ConvTransposeNd
          Linear --|> ReferenceQuantizedModule
          Linear --|> Linear
          GRU --|> RNNBase
          GRUCell --|> RNNCellBase
          LSTM --|> RNNBase
          LSTMCell --|> RNNCellBase
          RNNBase --|> RNNBase
          RNNCell --|> RNNCellBase
          RNNCellBase --|> RNNCellBase
          Embedding --|> ReferenceQuantizedModule
          Embedding --|> Embedding
          EmbeddingBag --|> ReferenceQuantizedModule
          EmbeddingBag --|> EmbeddingBag
          ReferenceQuantizedModule --|> Module
          Linear --|> Module
          Linear --|> Module
          LinearPackedParams --|> Module
          Logger --|> Module
          OutputLogger --|> Logger
          Shadow --|> Module
          ShadowLogger --|> Logger
          NSTracer --|> QuantizationTracer
          OutputComparisonLogger --|> OutputLogger
          OutputLogger --|> Module
          M --|> Module
          BaseDataSparsifier --|> BaseSparsifier
          _Container --|> Module
          DataNormSparsifier --|> BaseDataSparsifier
          FPGMPruner --|> BaseStructuredSparsifier
          BaseStructuredSparsifier --|> BaseSparsifier
          LSTMSaliencyPruner --|> BaseStructuredSparsifier
          FakeStructuredSparsity --|> Module
          SaliencyPruner --|> BaseStructuredSparsifier
          CubicSL --|> BaseScheduler
          LambdaSL --|> BaseScheduler
          NearlyDiagonalSparsifier --|> BaseSparsifier
          FakeSparsity --|> Module
          WeightNormSparsifier --|> BaseSparsifier
          _DerivedObserverOrFakeQuantize --|> ObserverBase
          MeanShadowLogger --|> Logger
          _LearnableFakeQuantize --|> FakeQuantizeBase
          FakeQuantize --|> FakeQuantizeBase
          FakeQuantizeBase --|> Module
          FixedQParamsFakeQuantize --|> FakeQuantize
          FusedMovingAvgObsFakeQuantize --|> FakeQuantize
          FakeQuantPerChannel --|> Function
          _InputEqualizationObserver --|> Module
          _WeightEqualizationObserver --|> Module
          DynamicStaticDetector --|> DetectorBase
          InputWeightEqualizationDetector --|> DetectorBase
          OutlierDetector --|> DetectorBase
          PerChannelDetector --|> DetectorBase
          ModelReportObserver --|> ObserverBase
          DefaultFuseHandler --|> FuseHandler
          FusedGraphModule --|> GraphModule
          ObservedGraphModule --|> GraphModule
          ObservedStandaloneGraphModule --|> ObservedGraphModule
          QuantizedGraphModule --|> GraphModule
          BatchNormQuantizeHandler --|> QuantizeHandler
          BinaryOpQuantizeHandler --|> QuantizeHandler
          CatQuantizeHandler --|> QuantizeHandler
          ConvReluQuantizeHandler --|> QuantizeHandler
          CopyNodeQuantizeHandler --|> QuantizeHandler
          CustomModuleQuantizeHandler --|> QuantizeHandler
          DefaultNodeQuantizeHandler --|> QuantizeHandler
          EmbeddingQuantizeHandler --|> QuantizeHandler
          FixedQParamsOpQuantizeHandler --|> QuantizeHandler
          GeneralTensorShapeOpQuantizeHandler --|> QuantizeHandler
          LinearReLUQuantizeHandler --|> QuantizeHandler
          RNNDynamicQuantizeHandler --|> QuantizeHandler
          StandaloneModuleQuantizeHandler --|> QuantizeHandler
          ConfigurableQuantizeHandler --|> QuantizeHandler
          QuantizationTracer --|> Tracer
          ScopeContextManager --|> ScopeContextManager
          FixedQParamsObserver --|> ObserverBase
          HistogramObserver --|> UniformQuantizationObserverBase
          MinMaxObserver --|> UniformQuantizationObserverBase
          MovingAverageMinMaxObserver --|> MinMaxObserver
          MovingAveragePerChannelMinMaxObserver --|> PerChannelMinMaxObserver
          NoopObserver --|> ObserverBase
          ObserverBase --|> Module
          PerChannelMinMaxObserver --|> UniformQuantizationObserverBase
          PlaceholderObserver --|> ObserverBase
          RecordingObserver --|> ObserverBase
          ReuseInputObserver --|> ObserverBase
          UniformQuantizationObserverBase --|> ObserverBase
          OutputLogger --|> Module
          DuplicateDQPass --|> PassBase
          _WrapperModule --|> Module
          PortNodeMetaForQDQ --|> PassBase
          ComposableQuantizer --|> Quantizer
          EmbeddingQuantizer --|> Quantizer
          DerivedQuantizationSpec --|> QuantizationSpecBase
          FixedQParamsQuantizationSpec --|> QuantizationSpecBase
          QuantizationSpec --|> QuantizationSpecBase
          SharedQuantizationSpec --|> QuantizationSpecBase
          X86InductorQuantizer --|> Quantizer
          _X86InductorQuantizationAnnotation --|> QuantizationAnnotation
          XNNPACKQuantizer --|> Quantizer
          XPUInductorQuantizer --|> X86InductorQuantizer
          DeQuantStub --|> Module
          QuantStub --|> Module
          QuantWrapper --|> Module
          Resize --|> Function
          Type --|> Function
          _set_fwd_grad_enabled --|> _DecoratorContextManager
          dual_level --|> _DecoratorContextManager
          BackwardCFunction --|> FunctionCtx
          BackwardCFunction --|> _HookMixin
          Function --|> _SingleLevelFunction
          InplaceFunction --|> Function
          NestedIOFunction --|> Function
          _SingleLevelFunction --|> FunctionCtx
          _SingleLevelFunction --|> _HookMixin
          _force_original_view_tracking --|> _DecoratorContextManager
          _unsafe_preserve_version_counter --|> _DecoratorContextManager
          enable_grad --|> _NoParamDecoratorContextManager
          inference_mode --|> _DecoratorContextManager
          no_grad --|> _NoParamDecoratorContextManager
          set_grad_enabled --|> _DecoratorContextManager
          set_multithreading_enabled --|> _DecoratorContextManager
          _CloneArgBeforeMutateMode --|> TorchDispatchMode
          _MultiHandle --|> RemovableHandle
          _swap_with_cloned --|> saved_tensors_hooks
          save_on_cpu --|> saved_tensors_hooks
          BFloat16Storage --|> _CudaLegacyStorage
          BoolStorage --|> _CudaLegacyStorage
          ByteStorage --|> _CudaLegacyStorage
          CharStorage --|> _CudaLegacyStorage
          ComplexDoubleStorage --|> _CudaLegacyStorage
          ComplexFloatStorage --|> _CudaLegacyStorage
          DoubleStorage --|> _CudaLegacyStorage
          FloatStorage --|> _CudaLegacyStorage
          HalfStorage --|> _CudaLegacyStorage
          IntStorage --|> _CudaLegacyStorage
          LongStorage --|> _CudaLegacyStorage
          ShortStorage --|> _CudaLegacyStorage
          _CudaLegacyStorage --|> _LegacyStorage
          device_of --|> device
          FunctionEvent --|> FormattedTimesMixin
          FunctionEventAvg --|> FormattedTimesMixin
          NnapiModule --|> Module
          NnapiInterfaceWrapper --|> Module
          ShapeComputeModule --|> Module
          CudnnModule --|> PropModule
          MkldnnModule --|> PropModule
          OptEinsumModule --|> PropModule
          autocast --|> autocast
          GradScaler --|> GradScaler
          CUDASanitizerDispatchMode --|> TorchDispatchMode
          UnsynchronizedAccessError --|> SynchronizationError
          autocast --|> autocast
          GradScaler --|> GradScaler
          Graphed --|> Function
          CUDAPluggableAllocator --|> _CUDAAllocator
          ExternalStream --|> Stream
          _ReplicateState --|> _State
          AsyncCollectiveTensor --|> Tensor
          _FromTorchTensor --|> Function
          ShardedOptimizer --|> Optimizer
          ShardedTensor --|> ShardedTensorBase
          ShardedTensorBase --|> Tensor
          DevicePlacementSpec --|> PlacementSpec
          EnumerableShardingSpec --|> ShardingSpec
          ChunkShardingSpec --|> ShardingSpec
          FSDPMemTracker --|> MemTracker
          _FSDPModState --|> _State
          _FSDPRefType --|> _RefType
          Node --|> ModStats
          MemTracker --|> TorchDispatchMode
          _MemRefType --|> _RefType
          _ModState --|> _State
          MemoryProfileDispatchMode --|> TorchDispatchMode
          RuntimeEstimator --|> TorchDispatchMode
          SACEstimator --|> TorchDispatchMode
          ActivationWrapper --|> Module
          CheckpointWrapper --|> ActivationWrapper
          OffloadWrapper --|> ActivationWrapper
          LowPrecisionState --|> DefaultState
          _OverlappedStandardOptimizer --|> OverlappedOptimizer
          PeriodicModelAverager --|> ModelAverager
          HierarchicalModelAverager --|> ModelAverager
          FileSystem --|> FileSystemBase
          FsspecReader --|> FileSystemReader
          FsspecWriter --|> FileSystemWriter
          DefaultLoadPlanner --|> LoadPlanner
          DefaultSavePlanner --|> SavePlanner
          _EmptyStateDictLoadPlanner --|> DefaultLoadPlanner
          FileSystem --|> FileSystemBase
          FileSystemReader --|> StorageReader
          FileSystemWriter --|> _FileSystemWriter
          FileSystemWriter --|> BlockingAsyncStager
          _FileSystemWriter --|> StorageWriter
          _OverlappingCpuLoader --|> _TensorLoader
          _SerialCpuLoader --|> _TensorLoader
          BroadcastingTorchSaveReader --|> StorageReader
          DynamicMetaLoadPlanner --|> DefaultLoadPlanner
          _ReaderWithOffset --|> DefaultLoadPlanner
          BlockingAsyncStager --|> AsyncStager
          _StateDictInfo --|> StateDictOptions
          SimpleElasticAgent --|> ElasticAgent
          LocalElasticAgent --|> SimpleElasticAgent
          ConsoleMetricHandler --|> MetricHandler
          NullMetricHandler --|> MetricHandler
          DefaultLogsSpecs --|> LogsSpecs
          MultiprocessContext --|> PContext
          SubprocessContext --|> PContext
          RendezvousClosedError --|> RendezvousError
          RendezvousConnectionError --|> RendezvousError
          RendezvousGracefulExitError --|> RendezvousError
          RendezvousStateError --|> RendezvousError
          RendezvousTimeoutError --|> RendezvousError
          C10dRendezvousBackend --|> RendezvousBackend
          DynamicRendezvousHandler --|> RendezvousHandler
          _BackendRendezvousStateHolder --|> _RendezvousStateHolder
          _DistributedRendezvousOpExecutor --|> _RendezvousOpExecutor
          EtcdRendezvousHandler --|> RendezvousHandler
          EtcdRendezvousBackend --|> RendezvousBackend
          StaticTCPRendezvous --|> RendezvousHandler
          FileTimerClient --|> TimerClient
          FileTimerRequest --|> TimerRequest
          LocalTimerClient --|> TimerClient
          LocalTimerServer --|> TimerServer
          MultiprocessingRequestQueue --|> RequestQueue
          ElasticDistributedSampler --|> DistributedSampler
          _FSDPState --|> _State
          _UninitializedDeviceHandle --|> _FSDPDeviceHandle
          FlatParameter --|> Parameter
          _FlatParameterMeta --|> _ParameterMeta
          CPUOffloadPolicy --|> OffloadPolicy
          DDPMeshInfo --|> DataParallelMeshInfo
          FSDPMeshInfo --|> DataParallelMeshInfo
          HSDPMeshInfo --|> DDPMeshInfo
          HSDPMeshInfo --|> FSDPMeshInfo
          RegisterPostBackwardFunction --|> Function
          FSDPState --|> _State
          _UnshardHandleImpl --|> UnshardHandle
          FullOptimStateDictConfig --|> OptimStateDictConfig
          FullStateDictConfig --|> StateDictConfig
          LocalOptimStateDictConfig --|> OptimStateDictConfig
          LocalStateDictConfig --|> StateDictConfig
          ShardedOptimStateDictConfig --|> OptimStateDictConfig
          ShardedStateDictConfig --|> StateDictConfig
          FullyShardedDataParallel --|> _FSDPState
          FullyShardedDataParallel --|> Module
          ShardedGradScaler --|> GradScaler
          _GeneralMultiDeviceReplicator --|> _MultiDeviceReplicator
          CustomPolicy --|> _Policy
          ModuleWrapPolicy --|> _Policy
          RemoteModule --|> _RemoteModule
          _RemoteModule --|> Module
          _AllGather --|> Function
          _AllGatherBase --|> Function
          _AllReduce --|> Function
          _AlltoAll --|> Function
          _AlltoAllSingle --|> Function
          _Broadcast --|> Function
          _Gather --|> Function
          _Reduce --|> Function
          _Reduce_Scatter --|> Function
          _Scatter --|> Function
          _NamedOptimizer --|> Optimizer
          _ScriptLocalOptimizer --|> Module
          PostLocalSGDOptimizer --|> Optimizer
          ZeroRedundancyOptimizer --|> Joinable
          ZeroRedundancyOptimizer --|> Optimizer
          _ZeROJoinHook --|> JoinHook
          DetachExecutor --|> Interpreter
          LossWrapper --|> Module
          Pipe --|> Module
          PipeSequential --|> Sequential
          TrivialLossWrapper --|> LossWrapper
          DummyModule --|> Module
          _LossReducer --|> _CustomReducer
          PipelineScheduleMulti --|> _PipelineSchedule
          PipelineScheduleSingle --|> _PipelineSchedule
          Schedule1F1B --|> PipelineScheduleSingle
          ScheduleGPipe --|> PipelineScheduleSingle
          ScheduleInterleaved1F1B --|> PipelineScheduleMulti
          ScheduleInterleavedZeroBubble --|> PipelineScheduleMulti
          ScheduleLoopedBFS --|> PipelineScheduleMulti
          ScheduleZBVZeroBubble --|> PipelineScheduleMulti
          _PipelineScheduleRuntime --|> PipelineScheduleMulti
          _ScheduleForwardOnly --|> PipelineScheduleSingle
          PipelineStage --|> _PipelineStageBase
          _PipelineStage --|> _PipelineStageBase
          _server_process_global_profile --|> profile
          DTensor --|> Tensor
          _FromTorchTensor --|> Function
          _ToTorchTensor --|> Function
          OpStrategy --|> StrategyType
          TupleStrategy --|> StrategyType
          _MaskPartial --|> Partial
          _NormPartial --|> Partial
          Broadcast --|> DimSpec
          Flatten --|> DimSpec
          InputDim --|> DimSpec
          NewDim --|> DimSpec
          Repeat --|> DimSpec
          Singleton --|> DimSpec
          Split --|> DimSpec
          OffsetBasedRNGTracker --|> _RNGStateTracker
          Redistribute --|> Function
          LocalShardsWrapper --|> Tensor
          CommDebugMode --|> TorchDispatchMode
          _CommModeModuleTracker --|> ModTracker
          _AllGatherRotater --|> _RingRotater
          _AllToAllRotater --|> _RingRotater
          _AttentionContextParallel --|> ParallelStyle
          _RoundRobinLoadBalancer --|> _LoadBalancer
          _SequentialSharder --|> _LoadBalancer
          _TensorParallelTransformPass --|> PassBase
          DTensorExtensions --|> FSDPExtensions
          ColwiseParallel --|> ParallelStyle
          PrepareModuleInput --|> ParallelStyle
          PrepareModuleOutput --|> ParallelStyle
          RowwiseParallel --|> ParallelStyle
          SequenceParallel --|> ParallelStyle
          Partial --|> Placement
          Replicate --|> Placement
          Shard --|> Placement
          _StridedShard --|> Shard
          Bernoulli --|> ExponentialFamily
          Beta --|> ExponentialFamily
          Binomial --|> Distribution
          Categorical --|> Distribution
          Cauchy --|> Distribution
          Chi2 --|> Gamma
          _Boolean --|> Constraint
          _Cat --|> Constraint
          _CorrCholesky --|> Constraint
          _Dependent --|> Constraint
          _DependentProperty --|> _Dependent
          _GreaterThan --|> Constraint
          _GreaterThanEq --|> Constraint
          _HalfOpenInterval --|> Constraint
          _IndependentConstraint --|> Constraint
          _IntegerGreaterThan --|> Constraint
          _IntegerInterval --|> Constraint
          _IntegerLessThan --|> Constraint
          _Interval --|> Constraint
          _LessThan --|> Constraint
          _LowerCholesky --|> Constraint
          _LowerTriangular --|> Constraint
          _Multinomial --|> Constraint
          _OneHot --|> Constraint
          _PositiveDefinite --|> _Symmetric
          _PositiveSemidefinite --|> _Symmetric
          _Real --|> Constraint
          _Simplex --|> Constraint
          _Square --|> Constraint
          _Stack --|> Constraint
          _Symmetric --|> _Square
          ContinuousBernoulli --|> ExponentialFamily
          Dirichlet --|> ExponentialFamily
          _Dirichlet --|> Function
          ExponentialFamily --|> Distribution
          Exponential --|> ExponentialFamily
          FisherSnedecor --|> Distribution
          Gamma --|> ExponentialFamily
          Geometric --|> Distribution
          Gumbel --|> TransformedDistribution
          HalfCauchy --|> TransformedDistribution
          HalfNormal --|> TransformedDistribution
          Independent --|> Distribution
          InverseGamma --|> TransformedDistribution
          Kumaraswamy --|> TransformedDistribution
          Laplace --|> Distribution
          LKJCholesky --|> Distribution
          LogNormal --|> TransformedDistribution
          LogisticNormal --|> TransformedDistribution
          LowRankMultivariateNormal --|> Distribution
          MixtureSameFamily --|> Distribution
          Multinomial --|> Distribution
          MultivariateNormal --|> Distribution
          NegativeBinomial --|> Distribution
          Normal --|> ExponentialFamily
          OneHotCategorical --|> Distribution
          OneHotCategoricalStraightThrough --|> OneHotCategorical
          Pareto --|> TransformedDistribution
          Poisson --|> ExponentialFamily
          LogitRelaxedBernoulli --|> Distribution
          RelaxedBernoulli --|> TransformedDistribution
          ExpRelaxedCategorical --|> Distribution
          RelaxedOneHotCategorical --|> TransformedDistribution
          StudentT --|> Distribution
          TransformedDistribution --|> Distribution
          AbsTransform --|> Transform
          AffineTransform --|> Transform
          CatTransform --|> Transform
          ComposeTransform --|> Transform
          CorrCholeskyTransform --|> Transform
          CumulativeDistributionTransform --|> Transform
          ExpTransform --|> Transform
          IndependentTransform --|> Transform
          LowerCholeskyTransform --|> Transform
          PositiveDefiniteTransform --|> Transform
          PowerTransform --|> Transform
          ReshapeTransform --|> Transform
          SigmoidTransform --|> Transform
          SoftmaxTransform --|> Transform
          SoftplusTransform --|> Transform
          StackTransform --|> Transform
          StickBreakingTransform --|> Transform
          TanhTransform --|> Transform
          _InverseTransform --|> Transform
          Uniform --|> Distribution
          _lazy_property_and_property --|> lazy_property
          VonMises --|> Distribution
          Weibull --|> TransformedDistribution
          Wishart --|> ExponentialFamily
          AutogradStateOpsFailSafeguard --|> TorchFunctionMode
          _WrapperModule --|> Module
          Wrapper --|> Module
          _StatefulGraphModule --|> GraphModule
          _Constraint --|> _ConstraintTarget
          _DerivedConstraint --|> _ConstraintTarget
          _DerivedDim --|> _Dim
          _RelaxedConstraint --|> _ConstraintTarget
          _StaticDim --|> _Dim
          InterpreterModule --|> Module
          InterpreterModuleDispatcher --|> Module
          UnflattenedModule --|> Module
          _LazyGraphModule --|> GraphModule
          PHWithMeta --|> PHBase
          Tracer --|> TracerBase
          FoldedGraphModule --|> GraphModule
          MetaAttribute --|> MetaProxy
          MetaDeviceAttribute --|> MetaAttribute
          MetaProxy --|> Proxy
          MetaTracer --|> Tracer
          ApplyBroadcasting --|> Constraint
          BinConstraintD --|> BinaryConstraint
          BinConstraintT --|> BinaryConstraint
          BinaryConstraint --|> Constraint
          CalcConv --|> Constraint
          CalcMaxPool --|> Constraint
          CalcProduct --|> Constraint
          CanReshape --|> Constraint
          Conj --|> Constraint
          DGreatestUpperBound --|> Constraint
          Disj --|> Constraint
          F --|> Constraint
          GetItem --|> Constraint
          GetItemTensor --|> Constraint
          IndexSelect --|> Constraint
          Prod --|> Constraint
          T --|> Constraint
          TGreatestUpperBound --|> Constraint
          Transpose --|> Constraint
          NormalizeArgs --|> Transformer
          NormalizeOperators --|> AnnotateTypesWithSchema
          DropoutRemover --|> Transformer
          DecompositionInterpreter --|> Interpreter
          PreDispatchTorchFunctionMode --|> TorchFunctionMode
          ProxyTorchDispatchMode --|> TorchDispatchMode
          PythonKeyTracer --|> Tracer
          TorchFunctionMetadataMode --|> TorchFunctionMode
          _GraphAppendingTracerEx --|> GraphAppendingTracer
          _ModuleStackTracer --|> PythonKeyTracer
          AttrProxy --|> _AttrProxy
          RewritingTracer --|> Tracer
          RewrittenModule --|> Module
          AnnotateTypesWithSchema --|> Transformer
          DynamicDimConstraintPrinter --|> PythonPrinter
          EqualityConstraint --|> Constraint
          LoggingShapeGuardPrinter --|> ShapeGuardPythonPrinter
          PropagateUnbackedSymInts --|> Interpreter
          RelaxedUnspecConstraint --|> Constraint
          ShapeGuardPrinter --|> ShapeGuardPythonPrinter
          ShapeGuardPythonPrinter --|> _ShapeGuardPrinter
          ShapeGuardPythonPrinter --|> PythonPrinter
          StatefulSymbolicContext --|> StatelessSymbolicContext
          StatelessSymbolicContext --|> SymbolicContext
          StrictMinMaxConstraint --|> Constraint
          SubclassSymbolicContext --|> StatefulSymbolicContext
          SymExprPrinter --|> PythonPrinter
          _PythonMsgPrinter --|> PythonPrinter
          VarDispatcher --|> Dispatcher
          MethodDispatcher --|> Dispatcher
          BisectValidationException --|> TorchDynamoException
          PopulateValidator --|> Interpreter
          ValidationException --|> TorchDynamoException
          _PyTreeCodeGen --|> CodeGen
          GraphModule --|> Module
          GraphModuleImpl --|> GraphModule
          GraphModuleImpl --|> Module
          _CodeOnlyModule --|> Module
          KeepModules --|> Tracer
          Transformer --|> Interpreter
          TransformerTracer --|> Tracer
          CudaGraphsSupport --|> OperatorSupport
          CSEPass --|> PassBase
          FakeTensorProp --|> Interpreter
          OperatorSupport --|> OperatorSupportBase
          FunctionalOperatorSupport --|> OperatorSupportBase
          _FunctionalizationMetadataProp --|> Interpreter
          ShapeProp --|> Interpreter
          CustomDrawer --|> FxGraphDrawer
          HolderModule --|> Module
          SubgraphMatcherWithNameNodeMap --|> SubgraphMatcher
          Attribute --|> Proxy
          GraphAppendingTracer --|> TracerBase
          MetaProxy --|> Proxy
          ParameterProxy --|> Proxy
          OrderedModuleDict --|> OrderedDictWrapper
          RecursiveScriptModule --|> ScriptModule
          ScriptModule --|> Module
          ONNXTracedModule --|> Module
          TopLevelTracedModule --|> TracedModule
          TracedModule --|> ScriptModule
          QualnameWrapper --|> Module
          ExprBuilder --|> Builder
          FrontendTypeError --|> FrontendError
          NotSupportedError --|> FrontendError
          StmtBuilder --|> Builder
          UnsupportedNodeError --|> NotSupportedError
          WithItemBuilder --|> Builder
          QuantizedGRU --|> QuantizedRNNBase
          QuantizedGRUCell --|> QuantizedRNNCellBase
          QuantizedLSTM --|> QuantizedRNNBase
          QuantizedLSTMCell --|> QuantizedRNNCellBase
          QuantizedLinear --|> ScriptModule
          QuantizedLinearFP16 --|> ScriptModule
          QuantizedRNNBase --|> ScriptModule
          QuantizedRNNCell --|> QuantizedRNNCellBase
          QuantizedRNNCellBase --|> ScriptModule
          Combine --|> Function
          _MaskedContiguous --|> Function
          _MaskedToDense --|> Function
          _MaskedToSparse --|> Function
          _MaskedToSparseCsr --|> Function
          _MaskedWhere --|> Function
          MaskedTensor --|> Tensor
          Constructor --|> Function
          GetData --|> Function
          ProcessExitedException --|> ProcessException
          ProcessRaisedException --|> ProcessException
          SpawnContext --|> ProcessContext
          NestedTensor --|> Tensor
          ViewBufferFromNested --|> Function
          ViewNestedFromBuffer --|> Function
          CausalBias --|> Tensor
          ModuleWrapper --|> Module
          BackwardHookFunction --|> Function
          CrossMapLRN2d --|> Function
          SyncBatchNorm --|> Function
          CELU --|> Module
          ELU --|> Module
          GELU --|> Module
          GLU --|> Module
          Hardshrink --|> Module
          Hardsigmoid --|> Module
          Hardswish --|> Module
          Hardtanh --|> Module
          LeakyReLU --|> Module
          LogSigmoid --|> Module
          LogSoftmax --|> Module
          Mish --|> Module
          MultiheadAttention --|> Module
          PReLU --|> Module
          RReLU --|> Module
          ReLU --|> Module
          ReLU6 --|> Hardtanh
          SELU --|> Module
          SiLU --|> Module
          Sigmoid --|> Module
          Softmax --|> Module
          Softmax2d --|> Module
          Softmin --|> Module
          Softplus --|> Module
          Softshrink --|> Module
          Softsign --|> Module
          Tanh --|> Module
          Tanhshrink --|> Module
          Threshold --|> Module
          AdaptiveLogSoftmaxWithLoss --|> Module
          BatchNorm1d --|> _BatchNorm
          BatchNorm2d --|> _BatchNorm
          BatchNorm3d --|> _BatchNorm
          LazyBatchNorm1d --|> _BatchNorm
          LazyBatchNorm1d --|> _LazyNormBase
          LazyBatchNorm2d --|> _BatchNorm
          LazyBatchNorm2d --|> _LazyNormBase
          LazyBatchNorm3d --|> _BatchNorm
          LazyBatchNorm3d --|> _LazyNormBase
          SyncBatchNorm --|> _BatchNorm
          _BatchNorm --|> _NormBase
          _LazyNormBase --|> _NormBase
          _LazyNormBase --|> LazyModuleMixin
          _NormBase --|> Module
          ChannelShuffle --|> Module
          Container --|> Module
          ModuleDict --|> Module
          ModuleList --|> Module
          ParameterDict --|> Module
          ParameterList --|> Module
          Sequential --|> Module
          Conv1d --|> _ConvNd
          Conv2d --|> _ConvNd
          Conv3d --|> _ConvNd
          ConvTranspose1d --|> _ConvTransposeNd
          ConvTranspose2d --|> _ConvTransposeNd
          ConvTranspose3d --|> _ConvTransposeNd
          LazyConv1d --|> Conv1d
          LazyConv1d --|> _LazyConvXdMixin
          LazyConv2d --|> Conv2d
          LazyConv2d --|> _LazyConvXdMixin
          LazyConv3d --|> Conv3d
          LazyConv3d --|> _LazyConvXdMixin
          LazyConvTranspose1d --|> ConvTranspose1d
          LazyConvTranspose1d --|> _LazyConvXdMixin
          LazyConvTranspose2d --|> ConvTranspose2d
          LazyConvTranspose2d --|> _LazyConvXdMixin
          LazyConvTranspose3d --|> ConvTranspose3d
          LazyConvTranspose3d --|> _LazyConvXdMixin
          _ConvNd --|> Module
          _ConvTransposeMixin --|> _ConvTransposeNd
          _ConvTransposeNd --|> _ConvNd
          _LazyConvXdMixin --|> LazyModuleMixin
          CosineSimilarity --|> Module
          PairwiseDistance --|> Module
          AlphaDropout --|> _DropoutNd
          Dropout --|> _DropoutNd
          Dropout1d --|> _DropoutNd
          Dropout2d --|> _DropoutNd
          Dropout3d --|> _DropoutNd
          FeatureAlphaDropout --|> _DropoutNd
          _DropoutNd --|> Module
          Flatten --|> Module
          Unflatten --|> Module
          Fold --|> Module
          Unfold --|> Module
          InstanceNorm1d --|> _InstanceNorm
          InstanceNorm2d --|> _InstanceNorm
          InstanceNorm3d --|> _InstanceNorm
          LazyInstanceNorm1d --|> _LazyNormBase
          LazyInstanceNorm1d --|> _InstanceNorm
          LazyInstanceNorm2d --|> _LazyNormBase
          LazyInstanceNorm2d --|> _InstanceNorm
          LazyInstanceNorm3d --|> _LazyNormBase
          LazyInstanceNorm3d --|> _InstanceNorm
          _InstanceNorm --|> _NormBase
          Bilinear --|> Module
          Identity --|> Module
          LazyLinear --|> LazyModuleMixin
          LazyLinear --|> Linear
          Linear --|> Module
          NonDynamicallyQuantizableLinear --|> Linear
          BCELoss --|> _WeightedLoss
          BCEWithLogitsLoss --|> _Loss
          CTCLoss --|> _Loss
          CosineEmbeddingLoss --|> _Loss
          CrossEntropyLoss --|> _WeightedLoss
          GaussianNLLLoss --|> _Loss
          HingeEmbeddingLoss --|> _Loss
          HuberLoss --|> _Loss
          KLDivLoss --|> _Loss
          L1Loss --|> _Loss
          MSELoss --|> _Loss
          MarginRankingLoss --|> _Loss
          MultiLabelMarginLoss --|> _Loss
          MultiLabelSoftMarginLoss --|> _WeightedLoss
          MultiMarginLoss --|> _WeightedLoss
          NLLLoss --|> _WeightedLoss
          NLLLoss2d --|> NLLLoss
          PoissonNLLLoss --|> _Loss
          SmoothL1Loss --|> _Loss
          SoftMarginLoss --|> _Loss
          TripletMarginLoss --|> _Loss
          TripletMarginWithDistanceLoss --|> _Loss
          _Loss --|> Module
          _WeightedLoss --|> _Loss
          CrossMapLRN2d --|> Module
          GroupNorm --|> Module
          LayerNorm --|> Module
          LocalResponseNorm --|> Module
          RMSNorm --|> Module
          CircularPad1d --|> _CircularPadNd
          CircularPad2d --|> _CircularPadNd
          CircularPad3d --|> _CircularPadNd
          ConstantPad1d --|> _ConstantPadNd
          ConstantPad2d --|> _ConstantPadNd
          ConstantPad3d --|> _ConstantPadNd
          ReflectionPad1d --|> _ReflectionPadNd
          ReflectionPad2d --|> _ReflectionPadNd
          ReflectionPad3d --|> _ReflectionPadNd
          ReplicationPad1d --|> _ReplicationPadNd
          ReplicationPad2d --|> _ReplicationPadNd
          ReplicationPad3d --|> _ReplicationPadNd
          ZeroPad1d --|> ConstantPad1d
          ZeroPad2d --|> ConstantPad2d
          ZeroPad3d --|> ConstantPad3d
          _CircularPadNd --|> Module
          _ConstantPadNd --|> Module
          _ReflectionPadNd --|> Module
          _ReplicationPadNd --|> Module
          PixelShuffle --|> Module
          PixelUnshuffle --|> Module
          AdaptiveAvgPool1d --|> _AdaptiveAvgPoolNd
          AdaptiveAvgPool2d --|> _AdaptiveAvgPoolNd
          AdaptiveAvgPool3d --|> _AdaptiveAvgPoolNd
          AdaptiveMaxPool1d --|> _AdaptiveMaxPoolNd
          AdaptiveMaxPool2d --|> _AdaptiveMaxPoolNd
          AdaptiveMaxPool3d --|> _AdaptiveMaxPoolNd
          AvgPool1d --|> _AvgPoolNd
          AvgPool2d --|> _AvgPoolNd
          AvgPool3d --|> _AvgPoolNd
          FractionalMaxPool2d --|> Module
          FractionalMaxPool3d --|> Module
          LPPool1d --|> _LPPoolNd
          LPPool2d --|> _LPPoolNd
          LPPool3d --|> _LPPoolNd
          MaxPool1d --|> _MaxPoolNd
          MaxPool2d --|> _MaxPoolNd
          MaxPool3d --|> _MaxPoolNd
          MaxUnpool1d --|> _MaxUnpoolNd
          MaxUnpool2d --|> _MaxUnpoolNd
          MaxUnpool3d --|> _MaxUnpoolNd
          _AdaptiveAvgPoolNd --|> Module
          _AdaptiveMaxPoolNd --|> Module
          _AvgPoolNd --|> Module
          _LPPoolNd --|> Module
          _MaxPoolNd --|> Module
          _MaxUnpoolNd --|> Module
          GRU --|> RNNBase
          GRUCell --|> RNNCellBase
          LSTM --|> RNNBase
          LSTMCell --|> RNNCellBase
          RNN --|> RNNBase
          RNNBase --|> Module
          RNNCell --|> RNNCellBase
          RNNCellBase --|> Module
          Embedding --|> Module
          EmbeddingBag --|> Module
          Transformer --|> Module
          TransformerDecoder --|> Module
          TransformerDecoderLayer --|> Module
          TransformerEncoder --|> Module
          TransformerEncoderLayer --|> Module
          Upsample --|> Module
          UpsamplingBilinear2d --|> Upsample
          UpsamplingNearest2d --|> Upsample
          DistributedDataParallelCPU --|> DistributedDataParallel
          Broadcast --|> Function
          Gather --|> Function
          ReduceAddCoalesced --|> Function
          Scatter --|> Function
          DataParallel --|> Module
          DistributedDataParallel --|> Joinable
          DistributedDataParallel --|> Module
          _DDPJoinHook --|> JoinHook
          _DDPSink --|> Function
          Buffer --|> Tensor
          Parameter --|> Tensor
          UninitializedBuffer --|> Tensor
          UninitializedBuffer --|> UninitializedTensorMixin
          UninitializedParameter --|> Parameter
          UninitializedParameter --|> UninitializedTensorMixin
          ConvPerSampleGrad --|> Function
          EmbeddingPerSampleGrad --|> Function
          ExpandedWeight --|> Tensor
          GroupNormPerSampleGrad --|> Function
          InstanceNormPerSampleGrad --|> Function
          LayerNormPerSampleGrad --|> Function
          LinearPerSampleGrad --|> Function
          _Orthogonal --|> Module
          _SpectralNorm --|> Module
          _WeightNorm --|> Module
          ParametrizationList --|> ModuleList
          CustomFromMask --|> BasePruningMethod
          Identity --|> BasePruningMethod
          L1Unstructured --|> BasePruningMethod
          LnStructured --|> BasePruningMethod
          PruningContainer --|> BasePruningMethod
          RandomStructured --|> BasePruningMethod
          RandomUnstructured --|> BasePruningMethod
          ResolvedExportOptions --|> ExportOptions
          TorchScriptOnnxExportDiagnostic --|> Diagnostic
          _FindOperatorOverloadsInOnnxRegistry --|> Rule
          _FindOpschemaMatchedSymbolicFunction --|> Rule
          _FxGraphToOnnx --|> Rule
          _FxNodeInsertTypePromotion --|> Rule
          _FxNodeToOnnx --|> Rule
          _FxPass --|> Rule
          _MissingCustomSymbolicFunction --|> Rule
          _MissingStandardSymbolicFunction --|> Rule
          _NoSymbolicFunctionForCallFunction --|> Rule
          _NodeMissingOnnxShapeInference --|> Rule
          _OpLevelDebugging --|> Rule
          _OperatorSupportedInNewerOpsetVersion --|> Rule
          _POERules --|> RuleCollection
          _UnsupportedFxNodeAnalysis --|> Rule
          PatchedPropertyBag --|> PropertyBag
          JitTraceConvertStrategy --|> CaptureStrategy
          WrappedModel --|> Module
          LegacyDynamoStrategy --|> CaptureStrategy
          TorchExportNonStrictStrategy --|> CaptureStrategy
          TorchExportStrategy --|> CaptureStrategy
          ConversionError --|> OnnxExporterError
          DispatchError --|> ConversionError
          GraphConstructionError --|> ConversionError
          TorchExportError --|> OnnxExporterError
          UnsupportedFxNodesAnalysis --|> Analysis
          UnsupportedFxNodesAnalysisResult --|> AnalysisResult
          InstanceNormDecompSkip --|> DecompSkip
          UpsampleBilinear2DDecompSkip --|> DecompSkip
          UpsampleTrilinear3DDecompSkip --|> DecompSkip
          Diagnostic --|> Diagnostic
          DiagnosticContext --|> DiagnosticContext
          UnsupportedFxNodeDiagnostic --|> Diagnostic
          DynamoExport --|> FXGraphExtractor
          DynamoFlattenOutputStep --|> FlattenOutputStep
          FXSymbolicTracer --|> FXGraphExtractor
          ModuleExpansionTracer --|> Tracer
          Decompose --|> Transform
          Functionalize --|> Transform
          RemoveInputMutation --|> Transform
          Modularize --|> Transform
          _LeafNode --|> _IRNode
          _ModuleNode --|> _IRNode
          RestoreParameterAndBufferNames --|> Transform
          AllOrAnyReductionTypePromotionRule --|> ReductionTypePromotionRule
          DivElementwiseTypePromotionRule --|> ElementwiseTypePromotionRule
          ElementwiseTypePromotionRule --|> TypePromotionRule
          InsertTypePromotion --|> Transform
          ReductionTypePromotionRule --|> TypePromotionRule
          SumLikeReductionTypePromotionRule --|> ReductionTypePromotionRule
          _OpTraceDispatchMode --|> TorchDispatchMode
          _TypePromotionInterpreter --|> Interpreter
          MovePlaceholderToFront --|> Transform
          ReplaceGetAttrWithPlaceholder --|> Transform
          BindInputStep --|> InputAdaptStep
          ConvertComplexToRealRepresentationInputStep --|> InputAdaptStep
          ConvertComplexToRealRepresentationOutputStep --|> OutputAdaptStep
          FlattenInputWithTreeSpecValidationInputStep --|> InputAdaptStep
          FlattenOutputStep --|> OutputAdaptStep
          FlattenOutputWithTreeSpecValidationOutputStep --|> OutputAdaptStep
          LiftParametersAndBuffersIntoArgsInputStep --|> InputAdaptStep
          MergeKwargsIntoArgsInputStep --|> InputAdaptStep
          PrependParamsAndBuffersAotAutogradOutputStep --|> OutputAdaptStep
          PrependParamsBuffersConstantAotAutogradInputStep --|> InputAdaptStep
          RemoveNonTensorInputStep --|> InputAdaptStep
          RemoveNoneInputStep --|> InputAdaptStep
          OrtOperatorSupport --|> OperatorSupport
          SymbolicValueError --|> OnnxExporterError
          UnsupportedOperatorError --|> OnnxExporterError
          Adafactor --|> Optimizer
          Adadelta --|> Optimizer
          Adagrad --|> Optimizer
          Adam --|> Optimizer
          Adamax --|> Optimizer
          AdamW --|> Optimizer
          ASGD --|> Optimizer
          LBFGS --|> Optimizer
          ChainedScheduler --|> LRScheduler
          ConstantLR --|> LRScheduler
          CosineAnnealingLR --|> LRScheduler
          CosineAnnealingWarmRestarts --|> LRScheduler
          CyclicLR --|> LRScheduler
          ExponentialLR --|> LRScheduler
          LambdaLR --|> LRScheduler
          LinearLR --|> LRScheduler
          MultiStepLR --|> LRScheduler
          MultiplicativeLR --|> LRScheduler
          OneCycleLR --|> LRScheduler
          PolynomialLR --|> LRScheduler
          ReduceLROnPlateau --|> LRScheduler
          SequentialLR --|> LRScheduler
          StepLR --|> LRScheduler
          _LRScheduler --|> LRScheduler
          NAdam --|> Optimizer
          RAdam --|> Optimizer
          RMSprop --|> Optimizer
          Rprop --|> Optimizer
          SGD --|> Optimizer
          SparseAdam --|> Optimizer
          AveragedModel --|> Module
          SWALR --|> LRScheduler
          BaseTorchFunctionMode --|> TorchFunctionMode
          PackagePickler --|> _PyTorchLegacyPickler
          OrderedImporter --|> Importer
          _SysImporter --|> Importer
          PackageImporter --|> Importer
          _ExternNode --|> _PathNode
          _ModuleNode --|> _PathNode
          _PackageNode --|> _PathNode
          TensorKey --|> Key
          Conv2dBiasFollowedByBatchNorm2dPattern --|> Pattern
          ExtraCUDACopyPattern --|> Pattern
          FP32MatMulPattern --|> Pattern
          ForLoopIndexingPattern --|> Pattern
          GradNotSetToNonePattern --|> Pattern
          MatMulDimInFP16Pattern --|> Pattern
          NamePattern --|> Pattern
          OptimizerSingleTensorPattern --|> Pattern
          SynchronizedDataLoaderPattern --|> Pattern
          ExecutionTraceObserver --|> _ITraceObserver
          profile --|> _KinetoProfile
          _open_buffer_reader --|> _opener
          _open_buffer_writer --|> _opener
          _open_file --|> _opener
          _open_zipfile_reader --|> _opener
          _open_zipfile_writer_buffer --|> _opener
          _open_zipfile_writer_file --|> _opener
          safe_globals --|> _safe_globals
          SparseSemiStructuredTensor --|> Tensor
          SparseSemiStructuredTensorCUSPARSELT --|> SparseSemiStructuredTensor
          SparseSemiStructuredTensorCUTLASS --|> SparseSemiStructuredTensor
          UntypedStorage --|> _StorageBase
          _LegacyStorage --|> TypedStorage
          BooleanPair --|> Pair
          NonePair --|> Pair
          NumberPair --|> Pair
          ObjectPair --|> Pair
          TensorLikePair --|> Pair
          TestAutocast --|> TestCase
          CubeGenVmap --|> Function
          ForwardHasDefaultArgs --|> Function
          MulGenVmap --|> Function
          NumpyCube --|> Function
          NumpyCubeNotComposable --|> Function
          NumpyExp_ --|> Function
          NumpyMul --|> Function
          NumpySort --|> Function
          NumpyTake --|> Function
          ScaleGradGenVmap --|> Function
          Select --|> Function
          SelectGenVmap --|> Function
          SortGenVmap --|> Function
          TakeGenVmap --|> Function
          ZeroGradientsGenVmap --|> Function
          CPUTestBase --|> DeviceTypeTestBase
          CUDATestBase --|> DeviceTypeTestBase
          DeviceTypeTestBase --|> TestCase
          HPUTestBase --|> DeviceTypeTestBase
          LazyTestBase --|> DeviceTypeTestBase
          MPSTestBase --|> DeviceTypeTestBase
          PrivateUse1TestBase --|> DeviceTypeTestBase
          XPUTestBase --|> DeviceTypeTestBase
          dtypesIfCPU --|> dtypes
          dtypesIfCUDA --|> dtypes
          dtypesIfHPU --|> dtypes
          dtypesIfMPS --|> dtypes
          dtypesIfPRIVATEUSE1 --|> dtypes
          ops --|> _TestParametrizer
          skipCPUIf --|> skipIf
          skipCUDAIf --|> skipIf
          skipGPUIf --|> skipIf
          skipHPUIf --|> skipIf
          skipLazyIf --|> skipIf
          skipMPSIf --|> skipIf
          skipMetaIf --|> skipIf
          skipPRIVATEUSE1If --|> skipIf
          skipXLAIf --|> skipIf
          skipXPUIf --|> skipIf
          CompositeModel --|> Module
          CompositeParamModel --|> Module
          FakeSequential --|> Module
          NestedSequentialModel --|> Module
          UnitModule --|> Module
          UnitParamModule --|> Module
          DistributedTestBase --|> MultiProcessTestCase
          DynamoDistributedMultiProcTestCase --|> MultiProcessTestCase
          DynamoDistributedSingleProcTestCase --|> TestCase
          MultiProcContinousTest --|> TestCase
          MultiProcessTestCase --|> TestCase
          MultiThreadedTestCase --|> TestCase
          SaveForwardInputsModel --|> Module
          SaveForwardInputsModule --|> Module
          AlwaysWrapNestedWrappedModule --|> NestedWrappedModule
          DoubleLinear --|> Module
          DummyDDP --|> Module
          FSDPTest --|> MultiProcessTestCase
          FSDPTestModel --|> Module
          FSDPTestMultiThread --|> MultiThreadedTestCase
          MLP --|> Module
          MLPStack --|> Sequential
          MixtureOfExperts --|> NestedWrappedModule
          ModuleWithDelay --|> FSDPTestModel
          NestedLinear --|> Module
          NestedWrappedModule --|> FSDPTestModel
          NestedWrappedModuleWithDelay --|> ModuleWithDelay
          NonUniformReqGradNWM --|> NestedWrappedModule
          SkipModel --|> Module
          SkipModule --|> Module
          TransformerWithSharedParams --|> FSDPTestModel
          JitCommonTestCase --|> TestCase
          ForeachSampleInput --|> SampleInput
          _TestParamsMaxPool1d --|> _TestParamsMaxPoolBase
          _TestParamsMaxPool2d --|> _TestParamsMaxPoolBase
          _TestParamsMaxPool3d --|> _TestParamsMaxPoolBase
          foreach_max_sample_func --|> foreach_inputs_sample_func
          foreach_norm_sample_func --|> foreach_inputs_sample_func
          foreach_pointwise_sample_func --|> foreach_inputs_sample_func
          modules --|> _TestParametrizer
          CriterionTest --|> InputVariableMixin
          CriterionTest --|> TestBase
          ModuleTest --|> TestBase
          NNTestCase --|> TestCase
          NewModuleTest --|> InputVariableMixin
          NewModuleTest --|> ModuleTest
          Layer --|> Module
          Net --|> Module
          FunctionalModule --|> Module
          optims --|> _TestParametrizer
          Conv2dActivation --|> Module
          Conv2dBias --|> Module
          Conv2dPadBias --|> Module
          Conv2dPool --|> Module
          Conv2dPoolFlatten --|> Module
          Conv2dPoolFlattenFunctional --|> Module
          ImplementedSparsifier --|> BaseSparsifier
          LSTMLayerNormLinearModel --|> Module
          LSTMLinearModel --|> Module
          LinearActivation --|> Module
          LinearActivationFunctional --|> Module
          LinearBias --|> Module
          MockSparseLinear --|> Linear
          SimpleConv2d --|> Module
          SimpleLinear --|> Module
          ActivationsTestModel --|> Module
          AnnotatedConvBnModel --|> Module
          AnnotatedConvBnReLUModel --|> Module
          AnnotatedConvModel --|> Module
          AnnotatedConvTransposeModel --|> Module
          AnnotatedCustomConfigNestedModel --|> Module
          AnnotatedNestedModel --|> Module
          AnnotatedSingleLayerLinearModel --|> Module
          AnnotatedSkipQuantModel --|> Module
          AnnotatedSubNestedModel --|> Module
          AnnotatedTwoLayerLinearModel --|> Module
          ConvBNReLU --|> Sequential
          ConvBnAddReluModel --|> Module
          ConvBnModel --|> Module
          ConvBnReLUModel --|> Module
          ConvModel --|> Module
          ConvReluAddModel --|> Module
          ConvReluConvModel --|> Module
          ConvReluModel --|> Module
          ConvTransposeModel --|> Module
          DeFusedEmbeddingBagLinear --|> Module
          DenseTopMLP --|> Module
          DummyObserver --|> Module
          EmbBagWrapper --|> Module
          EmbeddingBagModule --|> Module
          EmbeddingModule --|> Module
          EmbeddingWithStaticLinear --|> Module
          FunctionalConv2d --|> Module
          FunctionalConvReluConvModel --|> Module
          FunctionalConvReluModel --|> Module
          FunctionalLinear --|> Module
          FunctionalLinearAddModel --|> Module
          FunctionalLinearReluLinearModel --|> Module
          FunctionalLinearReluModel --|> Module
          InnerModule --|> Module
          LSTMwithHiddenDynamicModel --|> Module
          LinearAddModel --|> Module
          LinearBnLeakyReluModel --|> Module
          LinearModelWithSubmodule --|> Module
          LinearReluAddModel --|> Module
          LinearReluLinearModel --|> Module
          LinearReluModel --|> Module
          LinearTanhModel --|> Module
          ManualConvLinearQATModel --|> Module
          ManualConvLinearSymmQATModel --|> ManualConvLinearQATModel
          ManualDropoutQATModel --|> Module
          ManualEmbeddingBagLinear --|> Module
          ManualLinearDynamicQATModel --|> Module
          ManualLinearQATModel --|> Module
          ModelForConvTransposeBNFusion --|> Module
          ModelForFusion --|> Module
          ModelForFusionWithBias --|> Module
          ModelForLinearBNFusion --|> Module
          ModelMultipleOps --|> Module
          ModelMultipleOpsNoAvgPool --|> Module
          ModelWithFunctionals --|> Module
          ModelWithSequentialFusion --|> Module
          NestedModel --|> Module
          NormalizationTestModel --|> Module
          PT2EQuantizationTestCase --|> QuantizationTestCase
          M --|> Module
          QuantStubModel --|> Module
          QuantSubModel --|> Module
          QuantizationLiteTestCase --|> QuantizationTestCase
          QuantizationTestCase --|> TestCase
          RNNCellDynamicModel --|> Module
          RNNDynamicModel --|> Module
          ResNetBase --|> Module
          SingleLayerFunctionalConvModel --|> Module
          SingleLayerFunctionalLinearModel --|> Module
          SingleLayerLinearDynamicModel --|> Module
          SingleLayerLinearModel --|> Module
          SkipQuantModel --|> Module
          SparseNNModel --|> Module
          SubModelForFusion --|> Module
          SubModelWithoutFusion --|> Module
          AddInplaceAdd --|> Module
          AddMulScalar --|> Module
          ControlFlow --|> Module
          Conv2dPropAnnotaton --|> Module
          Conv2dThenConv1d --|> Module
          Conv2dWithCat --|> Module
          Conv2dWithObsSharingOps --|> Module
          Conv2dWithTwoCat --|> Module
          Conv2dWithTwoLinear --|> Module
          Conv2dWithTwoLinearPermute --|> Module
          ConvBnReLU2dAndLinearReLU --|> Module
          ConvLinearWPermute --|> Module
          ConvMaxPool2d --|> Module
          ConvTWithBNRelu --|> Module
          ConvWithAdaptiveAvgPool2d --|> Module
          ConvWithBNRelu --|> Module
          EmbeddingConvLinearModule --|> Module
          EmbeddingModule --|> Module
          GroupwiseConv2d --|> Module
          LinearReluModel --|> Module
          MulInplaceMul --|> Module
          ThreeAdd --|> Module
          TwoLinearModule --|> Module
          TwoLayerConvModel --|> Module
          TwoLayerFunctionalConvModel --|> Module
          TwoLayerFunctionalLinearModel --|> Module
          TwoLayerLinearModel --|> Module
          DiagTensorBelow --|> WrapperTensor
          NonWrapperTensor --|> Tensor
          SparseTensor --|> WrapperTensor
          SubclassWithTensorFactory --|> Tensor
          WrapperTensor --|> Tensor
          WrapperTensorWithCustomSizes --|> WrapperTensor
          WrapperTensorWithCustomStrides --|> WrapperTensor
          CrossRefMode --|> TorchFunctionMode
          NestedTensorTestCase --|> TestCase
          ObjectPair --|> UnittestPair
          RelaxedBooleanPair --|> BooleanPair
          RelaxedNumberPair --|> NumberPair
          SetPair --|> UnittestPair
          StringPair --|> UnittestPair
          TensorOrArrayPair --|> TensorLikePair
          TestCaseBase --|> TestCase
          TestGradients --|> TestCase
          TypePair --|> UnittestPair
          TypedStoragePair --|> TensorLikePair
          UnittestPair --|> Pair
          decorateIf --|> _TestParametrizer
          parametrize --|> _TestParametrizer
          reparametrize --|> _TestParametrizer
          swap --|> _TestParametrizer
          CompositeCompliantTensor --|> Tensor
          CompositeCompliantTensorMode --|> TorchDispatchMode
          ConstantExtraMetadataTensor --|> Tensor
          Net --|> Module
          Net --|> Module
          ShardedTensorTestBase --|> MultiProcessTestCase
          MyShardedModel1 --|> Module
          MyShardedModel2 --|> Module
          SimpleMegatronLM --|> Module
          Attention --|> Module
          DTensorOpTestBase --|> MultiThreadedTestCase
          DTensorTestBase --|> MultiProcessTestCase
          FeedForward --|> Module
          MLPModule --|> Module
          MLPStacked --|> Module
          RMSNormPython --|> Module
          Transformer --|> Module
          TransformerBlock --|> Module
          CommonDdpComparisonTest --|> RpcAgentTestFixture
          CudaDdpComparisonTest --|> CommonDdpComparisonTest
          DdpComparisonTest --|> CommonDdpComparisonTest
          DdpUnderDistAutogradTest --|> RpcAgentTestFixture
          HybridModel --|> Module
          RemoteEM --|> Module
          RemoteNet --|> Module
          BatchNormNet --|> Module
          ControlFlowToyModel --|> Module
          DictOutputModule --|> Module
          TestModel --|> Module
          MyModel --|> Module
          SubModule --|> Module
          MyModel --|> Module
          MyModel --|> Module
          ToyModel --|> Module
          NetWithBuffers --|> Module
          Net --|> Module
          ToyModel --|> Module
          Model --|> Module
          ToyModel --|> Module
          DummyTestModel --|> Module
          MyModel --|> Module
          NamedTupleModule --|> Module
          MyModel --|> Module
          SimulateError --|> Function
          MyModel --|> Module
          ToyModel --|> Module
          MyModel --|> Module
          OpPatcher --|> TorchDispatchMode
          NestedOutputModule --|> Module
          ExceptionModule --|> Module
          UnusedParamModule --|> Module
          ModelWithComm --|> Module
          ToyModel --|> Module
          ToyModel --|> Module
          ToyModel --|> Module
          SimpleConditionalModel --|> Module
          MockModule --|> Module
          Net --|> Module
          EmbeddingNetDifferentParams --|> Module
          LargeNet --|> Module
          Net --|> Module
          NetWithBuffers --|> Module
          Task --|> Module
          TestDistBackend --|> MultiProcessTestCase
          TwoLinLayerNet --|> Module
          UnusedParamTwoLinLayerNet --|> Module
          _FC2 --|> Module
          CommonRemoteModuleTest --|> RpcAgentTestFixture
          CudaRemoteModuleTest --|> CommonRemoteModuleTest
          MyModule --|> Module
          RemoteModuleTest --|> CommonRemoteModuleTest
          ThreeWorkersRemoteModuleTest --|> CommonRemoteModuleTest
          CommonDistAutogradTest --|> RpcAgentTestFixture
          CudaDistAutogradTest --|> CommonDistAutogradTest
          DistAutogradTest --|> CommonDistAutogradTest
          MyBackwardFunc --|> Function
          TestDebugInfoFunc --|> Function
          MyFunc --|> Function
          MyFunc --|> Function
          MyFuncSingleGrad --|> Function
          NonContGradFunc --|> Function
          MyFunc --|> Function
          NonContGradFunc --|> Function
          FaultyAgentDistAutogradTest --|> RpcAgentTestFixture
          SimulateBackwardError --|> Function
          TensorPipeAgentDistAutogradTest --|> CommonDistAutogradTest
          TensorPipeCudaDistAutogradTest --|> RpcAgentTestFixture
          MyLocalCompute --|> Module
          MyRemoteCompute --|> Module
          WrapperModule --|> Module
          DistOptimizerTest --|> RpcAgentTestFixture
          FailingOptimizer --|> Optimizer
          OptimizerFailingOnConstructor --|> Optimizer
          ParameterServerTest --|> RpcAgentTestFixture
          Policy --|> Module
          ReinforcementLearningRpcTest --|> RpcAgentTestFixture
          FaultyAgentRpcTest --|> RpcAgentTestFixture
          FaultyRpcAgentTestFixture --|> RpcAgentTestFixture
          JitDistAutogradTest --|> RpcAgentTestFixture
          JitRpcTest --|> FutureTypingTest
          JitRpcTest --|> JitRpcOpTest
          JitRpcTest --|> LocalRRefTest
          JitRpcTest --|> RRefAPITest
          JitRpcTest --|> RRefTypingTest
          JitRpcTest --|> RpcAgentTestFixture
          MyModuleInterface --|> Module
          MyScriptModule --|> ScriptModule
          MyScriptModuleWithRRefs --|> ScriptModule
          JitFaultyAgentRpcTest --|> RpcAgentTestFixture
          CudaRpcTest --|> RpcAgentTestFixture
          MyConvNetForMNIST --|> Module
          MyEmbeddingBagModel --|> Module
          RpcTest --|> RpcAgentTestFixture
          RpcTest --|> RpcTestCommon
          MyModel --|> Module
          TensorPipeAgentCudaRpcTest --|> RpcAgentTestFixture
          TensorPipeAgentCudaRpcTest --|> RpcTestCommon
          TensorPipeAgentRpcTest --|> RpcAgentTestFixture
          TensorPipeAgentRpcTest --|> RpcTestCommon
          TensorPipeRpcAgentTestFixture --|> RpcAgentTestFixture
          SpawnHelper --|> MultiProcessTestCase
          TheModule --|> ScriptModule
          JitTestCase --|> JitCommonTestCase
          LoggingTensor --|> Tensor
          LoggingTensorMode --|> TorchDispatchMode
          LoggingTensorReentrant --|> LoggingTensor
          LoggingTestCase --|> TestCase
          BinaryUfuncInfo --|> OpInfo
          ForeachFuncInfo --|> OpInfo
          ReductionOpInfo --|> OpInfo
          ShapeFuncInfo --|> OpInfo
          SkipRule --|> SampleRule
          SpectralFuncInfo --|> OpInfo
          UnaryUfuncInfo --|> OpInfo
          XFailRule --|> SampleRule
          SpectralFuncPythonRefInfo --|> SpectralFuncInfo
          ElementwiseBinaryPythonRefInfo --|> BinaryUfuncInfo
          ElementwiseUnaryPythonRefInfo --|> UnaryUfuncInfo
          PythonRefInfo --|> OpInfo
          ReductionPythonRefInfo --|> ReductionOpInfo
          _dynamic_dispatch_dtypes --|> _dispatch_dtypes
          OpCheckMode --|> TorchFunctionMode
          LinearReluFunctional --|> Module
          LinearReluFunctionalChild --|> Module
          WrapperSubclass --|> Tensor
          TwoTensor --|> Tensor
          TwoTensorMode --|> TorchDispatchMode
          ConfigPatch --|> ContextDecorator
          ConfigModuleInstance --|> ConfigModule
          _NoParamDecoratorContextManager --|> _DecoratorContextManager
          DeviceContext --|> TorchFunctionMode
          BaseTorchDispatchMode --|> TorchDispatchMode
          LeafSpec --|> TreeSpec
          CleanDiv --|> FloorDiv
          Max --|> MinMaxBase
          Min --|> MinMaxBase
          CppPrinter --|> ExprPrinter
          PythonPrinter --|> ExprPrinter
          OptimizedPythonReferenceAnalysis --|> PythonReferenceAnalysis
          PythonReferenceAnalysis --|> ReferenceAnalysis
          ValueRangeAnalysis --|> SymPyValueRangeAnalysis
          BinaryOpFuzzer --|> Fuzzer
          BinaryOpSparseFuzzer --|> Fuzzer
          UnaryOpSparseFuzzer --|> Fuzzer
          SpectralOpFuzzer --|> Fuzzer
          UnaryOpFuzzer --|> Fuzzer
          FuzzedSparseTensor --|> FuzzedTensor
          CheckpointFunction --|> Function
          _CachedTorchDispatchMode --|> TorchDispatchMode
          _CachingTorchDispatchMode --|> TorchDispatchMode
          _NoopSaveInputs --|> Function
          _checkpoint_hook --|> saved_tensors_hooks
          _recomputation_hook --|> saved_tensors_hooks
          cls_with_options --|> BuildExtension
          _IterableDatasetFetcher --|> _BaseDatasetFetcher
          _MapDatasetFetcher --|> _BaseDatasetFetcher
          _InfiniteConstantSampler --|> Sampler
          _MultiProcessingDataLoaderIter --|> _BaseDataLoaderIter
          _SingleProcessDataLoaderIter --|> _BaseDataLoaderIter
          _DataPipeMeta --|> GenericMeta
          _IterDataPipeMeta --|> _DataPipeMeta
          CaptureA --|> CaptureF
          CaptureAdd --|> Capture
          CaptureCall --|> Capture
          CaptureDataFrame --|> CaptureInitial
          CaptureDataFrameWithDataPipeOps --|> CaptureDataFrame
          CaptureF --|> Capture
          CaptureGetAttr --|> Capture
          CaptureGetItem --|> Capture
          CaptureInitial --|> CaptureVariable
          CaptureMul --|> Capture
          CaptureSetItem --|> Capture
          CaptureSub --|> Capture
          CaptureVariable --|> Capture
          CaptureVariableAssign --|> CaptureF
          DataFrameTracedOps --|> DFIterDataPipe
          DataFrameTracer --|> CaptureDataFrameWithDataPipeOps
          DataFrameTracer --|> IterDataPipe
          ConcatDataFramesPipe --|> DFIterDataPipe
          DataFramesAsTuplesPipe --|> IterDataPipe
          ExampleAggregateAsDataFrames --|> DFIterDataPipe
          FilterDataFramesPipe --|> DFIterDataPipe
          PerRowDataFramesPipe --|> DFIterDataPipe
          ShuffleDataFramesPipe --|> DFIterDataPipe
          DataChunkDF --|> DataChunk
          DFIterDataPipe --|> IterDataPipe
          IterDataPipe --|> IterableDataset
          MapDataPipe --|> Dataset
          _IterDataPipeSerializationWrapper --|> IterDataPipe
          _IterDataPipeSerializationWrapper --|> _DataPipeSerializationWrapper
          _MapDataPipeSerializationWrapper --|> MapDataPipe
          _MapDataPipeSerializationWrapper --|> _DataPipeSerializationWrapper
          CollatorIterDataPipe --|> MapperIterDataPipe
          MapperIterDataPipe --|> IterDataPipe
          SamplerIterDataPipe --|> IterDataPipe
          ShufflerIterDataPipe --|> IterDataPipe
          ConcaterIterDataPipe --|> IterDataPipe
          DemultiplexerIterDataPipe --|> IterDataPipe
          ForkerIterDataPipe --|> IterDataPipe
          MultiplexerIterDataPipe --|> IterDataPipe
          ZipperIterDataPipe --|> IterDataPipe
          _ChildDataPipe --|> IterDataPipe
          _DemultiplexerIterDataPipe --|> IterDataPipe
          _DemultiplexerIterDataPipe --|> _ContainerTemplate
          _ForkerIterDataPipe --|> IterDataPipe
          _ForkerIterDataPipe --|> _ContainerTemplate
          FileListerIterDataPipe --|> IterDataPipe
          FileOpenerIterDataPipe --|> IterDataPipe
          BatcherIterDataPipe --|> IterDataPipe
          GrouperIterDataPipe --|> IterDataPipe
          UnBatcherIterDataPipe --|> IterDataPipe
          RoutedDecoderIterDataPipe --|> IterDataPipe
          FilterIterDataPipe --|> IterDataPipe
          ShardingFilterIterDataPipe --|> _ShardingIterDataPipe
          _ShardingIterDataPipe --|> IterDataPipe
          StreamReaderIterDataPipe --|> IterDataPipe
          IterableWrapperIterDataPipe --|> IterDataPipe
          MapperMapDataPipe --|> MapDataPipe
          ShufflerIterDataPipe --|> IterDataPipe
          ConcaterMapDataPipe --|> MapDataPipe
          ZipperMapDataPipe --|> MapDataPipe
          BatcherMapDataPipe --|> MapDataPipe
          SequenceWrapperMapDataPipe --|> MapDataPipe
          ChainDataset --|> IterableDataset
          ConcatDataset --|> Dataset
          IterableDataset --|> Dataset
          StackDataset --|> Dataset
          Subset --|> Dataset
          TensorDataset --|> Dataset
          DistributedSampler --|> Sampler
          BatchSampler --|> Sampler
          RandomSampler --|> Sampler
          SequentialSampler --|> Sampler
          SubsetRandomSampler --|> Sampler
          WeightedRandomSampler --|> Sampler
          _FlopCounterMode --|> TorchDispatchMode
          MkldnnBatchNorm --|> ScriptModule
          MkldnnConv1d --|> _MkldnnConvNd
          MkldnnConv2d --|> _MkldnnConvNd
          MkldnnConv3d --|> _MkldnnConvNd
          MkldnnLinear --|> ScriptModule
          MkldnnPrelu --|> ScriptModule
          _MkldnnConvNd --|> ScriptModule
          NodePy --|> NodeBase
          NodePyIO --|> NodePy
          NodePyOP --|> NodePy
          device_of --|> device
          PhiloxState --* PhiloxStateTracker : running_state
          PhiloxState --* PhiloxStateTracker : fwd_state
          PhiloxState --* PhiloxStateTracker : bwd_state
          FixedPointBox --* StackSize : fixed_point
          Instruction --* BlockStackEntry : inst
          Instruction --* BlockStackEntry : target
          Instruction --* InstructionTranslatorBase : current_instruction
          Instruction --* SpeculationEntry : inst
          CatchErrorsWrapper --* _TorchDynamoContext : prior
          ConvertFrameAssert --* ConvertFrame : _inner_convert
          DisableContext --* DTensorExtensions : post_unflatten_transform
          DynamoStance --* set_stance : stance
          DynamoStance --* set_stance : prev
          OptimizedModule --* DTensorExtensions : post_unflatten_transform
          OptimizedModule --* Module : _compiled_call_impl
          GraphRegionTracker --* OutputGraph : region_tracker
          InputPickler --* GraphRegionTracker : input_pickler
          GuardManagerWrapper --* CheckFunctionManager : guard_manager
          Config --* PyExprCSEPass : _config
          OutputGraph --* InstructionTranslatorBase : output
          VariableTrackerCache --* OutputGraph : variable_tracker_cache
          ProfileMetrics --* ProfileResult : captured
          ProfileMetrics --* ProfileResult : total
          ExecutionRecorder --* InstructionTranslatorBase : exec_recorder
          SideEffects --* OutputGraph : side_effects
          AttrProxySource --* VariableBuilder : source
          AttrSource --* VariableBuilder : source
          AttrSource --* VariableBuilder : source
          OptimizerSource --* VariableBuilder : source
          TensorProperty --* TensorPropertySource : prop
          LocalState --* DistributedState : local_state
          GuardFn --* GuardedCode : guard_manager
          ExactWeakKeyDictionary --* CodeContextDict : code_context
          ExactWeakKeyDictionary --* GenerationTracker : dynamic_classes
          ExactWeakKeyDictionary --* GenerationTracker : generation_values
          ExactWeakKeyDictionary --* MutationTracker : db
          ExactWeakKeyDictionary --* ContinueExecutionCache : cache
          ExactWeakKeyDictionary --* ContinueExecutionCache : generated_code_metadata
          AttributeMutationExisting --* MutableMappingVariable : mutation_type
          VariableTracker --* GraphOutputEntry : variable
          ConstantVariable --* InliningInstructionTranslator : symbolic_result
          ConstantVariable --* InstructionTranslatorBase : kw_names
          ConstantVariable --* LazySymNodeFormatString : fmt_var
          ContextMangerState --* ContextWrappingVariable : state
          ConstDictVariable --* InspectBoundArgumentsVariable : bound_arguments_var
          ConstDictVariable --* MutableMappingVariable : generic_dict_vt
          FrozensetVariable --* InliningInstructionTranslator : symbolic_result
          FrozensetVariable --* InstructionTranslatorBase : kw_names
          FrozensetVariable --* LazyCache : vt
          FrozensetVariable --* LazySymNodeFormatString : fmt_var
          SetVariable --* InliningInstructionTranslator : symbolic_result
          SetVariable --* InstructionTranslatorBase : kw_names
          SetVariable --* LazyCache : vt
          SetVariable --* LazySymNodeFormatString : fmt_var
          LazyVariableTracker --* MutableMappingVariable : generic_dict_vt
          ListVariable --* SideEffects : ca_final_callbacks_var
          SizeVariable --* SymNodeVariable : _tensor_var
          TorchInGraphFunctionVariable --* SymNodeVariable : _tensor_var
          SymbolicTorchFunctionState --* InstructionTranslator : symbolic_torch_function_state
          SymbolicTorchFunctionState --* InstructionTranslatorBase : symbolic_torch_function_state
          UserDefinedObjectVariable --* LazyCache : vt
          UserDefinedObjectVariable --* SymNodeVariable : _tensor_var
          _DictMock --* ExplainTS2FXGraphConverter : name_to_node
          SupportLevel --* ExportCase : support_level
          MySubModule --* CondBranchClassMethod : subm
          ExportTracer --* _ExportPassBaseDeprecatedDoNotUse : tracer
          ExportTracer --* _ExportPassBaseDeprecatedDoNotUse : tracer
          ProxyValue --* _FunctionalizeSideEffectfulOpsPass : _dep_token
          ProxyValue --* _FunctionalizeSideEffectfulOpsPass : _dep_token
          ExportedProgram --* _SerializedProgram : exported_program
          Node --* ExternKernelNode : node
          Node --* ExternKernelNode : node
          GraphState --* GraphModuleSerializer : graph_state
          GraphState --* GraphModuleSerializer : graph_state
          _UnionTag --* _Union : _type
          CompiledForward --* AOTAutogradCacheEntry : compiled_fw
          ViewAndMutationMeta --* AOTAutogradCacheEntry : runtime_metadata
          ViewAndMutationMeta --* CompiledFunction : metadata
          ViewAndMutationMeta --* SubclassMeta : fw_metadata
          GlobalContext --* TracingContext : global_context
          Guard --* GuardCodeList : guard
          Guard --* DeterministicAlgorithmsVariable : _guards_singleton
          Guard --* DualLevelContextManager : _guards_singleton
          Guard --* FSDPParamGroupUseTrainingStateVariable : _guards_singleton
          Guard --* GradIncrementNestingCtxManagerVariable : _guards_singleton
          Guard --* GradModeVariable : _guards_singleton
          Guard --* JvpIncrementNestingCtxManagerVariable : _guards_singleton
          Guard --* TorchFunctionDisableVariable : _guards_singleton
          Guard --* VmapIncrementNestingCtxManagerVariable : _guards_singleton
          GuardsContext --* TracingContext : guards_context
          GuardsSet --* ExportResult : guards
          GuardsSet --* GuardsContext : dynamo_guards
          GuardsSet --* GuardsContext : dynamo_guards
          HopDispatchSetCache --* TracingContext : hop_dispatch_set_cache
          ModuleContext --* TracingContext : module_context
          SLoc --* ShapeGuard : sloc
          SLoc --* ValueRangesSLoc : lower
          SLoc --* ValueRangesSLoc : upper
          Source --* VariableTrackerCacheKey : source
          Source --* GraphArg : source
          Source --* TrackedFake : source
          Source --* ChainedSource : base
          Source --* DuplicateInputs : input_source_a
          Source --* DuplicateInputs : input_source_b
          Source --* Guard : originating_source
          TracingContext --* OutputGraph : tracing_context
          Intermediate --* Op : ret
          AHMetadata --* AutoHeuristic : metadata
          DLLWrapper --* CUDABenchmarkRequest : DLL
          DLLWrapper --* ROCmBenchmarkRequest : DLL
          BracesBuffer --* KernelGroup : loops_code
          CSE --* Kernel : cse
          CSE --* Kernel : cse
          CSE --* CppKernel : reduction_cse
          CSE --* CppKernel : weight_recps_cse
          CSE --* CppKernel : cse
          CSE --* CppKernel : cse
          CppWrapperKernelArgs --* CppWrapperKernelGroup : args
          CppWrapperKernelArgs --* KernelGroup : args
          CppWrapperKernelArgs --* CppTemplateKernel : args
          KernelArgs --* Kernel : args
          KernelArgs --* KernelGroup : args
          KernelArgs --* KernelGroup : args
          OpOverrides --* CUDAKernel : overrides
          OpOverrides --* ROCmKernel : overrides
          OptimizationContext --* RecordOptimizationContext : opt_ctx
          WorkspaceZeroMode --* WorkspaceArg : zero_mode
          CppOverrides --* CppKernel : overrides
          CppTile2DOverrides --* CppTile2DKernel : overrides
          CppVecOverrides --* CppVecKernel : overrides
          CppWrapperKernelGroup --* CppScheduling : kernel_group
          KernelGroup --* CppScheduling : kernel_group
          LoopNest --* CppKernelProxy : loop_nest
          WorkSharing --* KernelGroup : ws
          CUDACPPScheduling --* CUDACombinedScheduling : _cuda_cpp_scheduling
          DebugPrinterManager --* PythonWrapperCodegen : debug_printer
          IntermediateValueDebuggingLevel --* DebugPrinterManager : debug_printer_level
          HalideKernel --* HalideScheduling : kernel_type
          HalideOverrides --* HalideKernel : overrides
          Allocation --* BufferGroup : allocation
          AllocationPools --* MemoryPlanner : pools
          BufferGroup --* PoolMemoryPlanningLine : group
          LiveRange --* Allocation : live_range
          LiveRange --* BufferGroup : live_range
          LiveRange --* BufferGroup : live_range
          TemporalSplit --* AllocationPool : root
          TemporalSplit --* SpatialSplit : left
          TemporalSplit --* SpatialSplit : right
          MultiKernelState --* PythonWrapperCodegen : multi_kernel_state
          ROCmCPPScheduling --* CUDACombinedScheduling : _rocm_cpp_scheduling
          SIMDKernel --* SIMDScheduling : kernel_type
          BlockParameters --* BlockPtrOptions : params
          CooperativeReductionWorkspaceCache --* TritonKernel : cooperative_reduction_workspace_cache
          HelperFunctions --* TritonKernel : helper_functions
          TritonCSE --* TritonKernel : cse
          TritonKernelOverrides --* TritonKernel : overrides
          TritonScheduling --* CUDACombinedScheduling : _triton_scheduling
          RoundRobinDispatch --* ComboKernel : dispatch_class
          SequentialDispatch --* ComboKernel : dispatch_class
          PythonWrapperCodegen --* CommBufferLine : wrapper
          PythonWrapperCodegen --* EnterSubgraphLine : wrapper
          PythonWrapperCodegen --* ExitSubgraphLine : wrapper
          PythonWrapperCodegen --* MemoryPlanningLine : wrapper
          _CompileFxKwargs --* CompiledFxGraph : fx_kwargs
          InvalidVecISA --* CppVecKernel : vec_isa
          VecAVX2 --* CppVecKernel : vec_isa
          VecISA --* CppKernelProxy : picked_vec_isa
          CUDAGraphTreeManager --* TreeManagerContainer : tree_manager
          FunctionID --* WrappedFunction : id
          ReadWrites --* BaseSchedulerNode : read_writes
          _SymHashingDict --* UniformValueConstantFolder : symint_nodes
          GraphLowering --* EnterSubgraphLine : graph
          GraphLowering --* PointwiseSubgraphLowering : root_graph
          Buffer --* CppTemplate : output_node
          Buffer --* CUDATemplate : output_node
          Buffer --* ROCmTemplate : output_node
          Buffer --* CommBufferLine : node
          Buffer --* SchedulerBuffer : node
          CommBufferType --* CommBufferLayout : comm_buffer_type
          ComputedBuffer --* StorageBox : data
          ComputedBuffer --* JointOutputResult : grad_input
          IRNode --* LayoutArg : node
          IRNode --* BaseView : data
          IRNode --* MutableBox : data
          Layout --* ReinterpretView : layout
          Loops --* ComputedBuffer : data
          LoopBodyBlock --* LoopBody : root_block
          MemoryPlanningInfoForBuffer --* FreeableInputBuffer : mpi_buffer
          MemoryPlanningInfoForBuffer --* SchedulerBuffer : mpi_buffer
          MemoryPlanningInfoForNode --* BaseSchedulerNode : mpi_node
          CachedMetricsDeltas --* CompiledFxGraph : metrics_deltas
          _GlobalItemStats --* _GlobalStats : autotune_local
          _GlobalItemStats --* _GlobalStats : autotune_remote
          _GlobalItemStats --* _GlobalStats : bundled_autotune
          _GlobalItemStats --* _GlobalStats : fx_graph
          _GlobalItemStats --* _GlobalStats : triton
          _GlobalItemStats --* _GlobalStats : aot_autograd
          _GlobalItemStats --* _GlobalStats : dynamo_pgo
          KernelFormatterHandler --* _V : KernelFormatterHandler
          MockHandler --* SimpleCSEHandler : mock
          MockHandler --* _V : MockHandler
          WrapperHandler --* _V : WrapperHandler
          Match --* _AllGatherMatch : match
          Match --* _ReduceScatterMatch : match
          MatchContext --* Match : ctx
          PatternExpr --* Match : pattern
          PatternExpr --* PatternEntry : pattern
          CoordescTuner --* CachingAutotuner : coordesc_tuner
          ReductionHint --* Reduction : reduction_hint
          ReductionHint --* Scan : reduction_hint
          ReductionHint --* Sort : reduction_hint
          BaseSchedulerNode --* NodeWithPriority : node
          BaseSchedulerNode --* SchedulerBuffer : defining_op
          Scheduler --* GraphLowering : scheduler
          Scheduler --* GraphLowering : scheduler
          Scheduler --* GraphLowering : scheduler
          Scheduler --* SchedulerBuffer : scheduler
          BenchmarkTensors --* AutotuneArgs : triton
          BenchmarkTensors --* AutotuneArgs : extern
          SizeVarAllocator --* GraphLowering : sizevars
          IndentedBuffer --* Kernel : loads
          IndentedBuffer --* Kernel : loads
          IndentedBuffer --* Kernel : compute
          IndentedBuffer --* Kernel : stores
          IndentedBuffer --* CppKernel : reduction_prefix
          IndentedBuffer --* CppKernel : reduction_suffix
          IndentedBuffer --* CppKernel : parallel_reduction_prefix
          IndentedBuffer --* CppKernel : parallel_reduction_suffix
          IndentedBuffer --* CppKernel : local_reduction_init
          IndentedBuffer --* CppKernel : local_reduction_stores
          IndentedBuffer --* CppKernel : non_parallel_reduction_prefix
          IndentedBuffer --* CppKernel : preloads
          IndentedBuffer --* CppKernel : poststores
          IndentedBuffer --* CppKernel : loads
          IndentedBuffer --* CppKernel : loads
          IndentedBuffer --* CppKernel : compute
          IndentedBuffer --* CppKernel : compute
          IndentedBuffer --* CppKernel : stores
          IndentedBuffer --* CppKernel : stores
          IndentedBuffer --* CutlassEVTEpilogueArgumentFormatter : output
          IndentedBuffer --* CutlassEVTEpilogueTypeFormatter : output
          IndentedBuffer --* HalideKernel : compute
          IndentedBuffer --* HalideKernel : loads
          IndentedBuffer --* HalideKernel : stores
          IndentedBuffer --* HalideKernel : indexing_code_dom
          IndentedBuffer --* SIMDKernel : body
          IndentedBuffer --* SIMDKernel : indexing_code
          IndentedBuffer --* TritonKernel : post_loop_combine
          IndentedBuffer --* TritonKernel : post_loop_store
          IndentedBuffer --* PythonWrapperCodegen : imports
          IndentedBuffer --* PythonWrapperCodegen : header
          IndentedBuffer --* PythonWrapperCodegen : prefix
          IndentedBuffer --* PythonWrapperCodegen : suffix
          IndentedBuffer --* PythonWrapperCodegen : wrapper_call
          IndentedBuffer --* PythonWrapperCodegen : kernel_autotune_defs
          IndentedBuffer --* PythonWrapperCodegen : kernel_autotune_calls
          IndentedBuffer --* PythonWrapperCodegen : subgraph_definitions
          IndentedBuffer --* KernelFormatterHandler : output
          IndentedBuffer --* SubgraphInfo : body
          IndentedBuffer --* SubgraphInfo : compute
          IndentedBuffer --* SubgraphInfo : indexing_code
          IndentedBuffer --* SubgraphInfo : loads
          IndentedBuffer --* SubgraphInfo : stores
          IndentedBuffer --* TritonTemplateKernel : body
          IndentedBuffer --* TritonTemplateKernel : compute
          IndentedBuffer --* TritonTemplateKernel : indexing_code
          IndentedBuffer --* TritonTemplateKernel : loads
          IndentedBuffer --* TritonTemplateKernel : stores
          FakeImplHolder --* SimpleOperatorEntry : fake_impl
          GenericTorchDispatchRuleHolder --* SimpleOperatorEntry : torch_dispatch_rules
          Kernel --* FakeImplHolder : kernel
          OpOverload --* ViewOp : target
          OpOverload --* DataDependentOutputException : func
          OpOverload --* DynamicOutputShapeException : func
          OpOverload --* UnsupportedOperatorException : func
          OpOverload --* OpSchema : op
          _PyOpNamespace --* _Ops : _higher_order_op_namespace
          SourceContext --* ParsedDef : ctx
          _DeconstructedSymNode --* _DeconstructedSymType : node
          FakeTensorConverter --* FakeTensorMode : fake_tensor_converter
          FakeTensorMode --* AutogradCompilerInstance : fake_tensor_mode
          FakeTensorMode --* _ExportPassBaseDeprecatedDoNotUse : fake_tensor_mode
          FakeTensorMode --* ExportTracer : fake_tensor_mode
          FakeTensorMode --* ExportTracer : fake_tensor_mode
          FakeTensorMode --* GraphModuleDeserializer : fake_tensor_mode
          FakeTensorMode --* FakeTensor : fake_mode
          FakeTensorMode --* RuntimeEstimator : fake_mode
          FakeTensorMode --* ExportArtifact : fake_mode
          FakeTensorMode --* ONNXFakeContext : fake_mode
          SymNumberMemoDescriptor --* FakeTensor : nonzero_memo
          SymNumberMemoDescriptor --* FakeTensor : item_memo
          SymNumberMemoDescriptor --* FakeTensor : unique_memo
          TensorMetadata --* TensorMetadataAndValues : tensor_metadata
          FunctionalTensorMode --* PythonFunctionalizeAPI : mode
          MetaConverter --* FakeTensorConverter : meta_converter
          MetaTensorDescriber --* MetaConverter : describer
          Tensor --* TensorAlias : alias
          Tensor --* ndarray : tensor
          Tensor --* FunctionalTensor : elem
          Tensor --* Tensor : real
          Tensor --* Tensor : imag
          Tensor --* Quantize : scale
          Tensor --* Quantize : zero_point
          Tensor --* FakeQuantize : scale
          Tensor --* FakeQuantize : zero_point
          Tensor --* FakeQuantizeBase : fake_quant_enabled
          Tensor --* FakeQuantizeBase : observer_enabled
          Tensor --* ModelReportObserver : average_batch_activation_range
          Tensor --* ModelReportObserver : comp_percentile
          Tensor --* FixedQParamsObserver : scale
          Tensor --* FixedQParamsObserver : zero_point
          Tensor --* HistogramObserver : histogram
          Tensor --* HistogramObserver : min_val
          Tensor --* HistogramObserver : max_val
          Tensor --* MinMaxObserver : min_val
          Tensor --* MinMaxObserver : max_val
          Tensor --* UniformQuantizationObserverBase : eps
          Tensor --* QuantizationComparisonResult : actual
          Tensor --* QuantizationComparisonResult : ref
          Tensor --* AsyncCollectiveTensor : elem
          Tensor --* Shard : tensor
          Tensor --* AllGatherResult : all_gather_output
          Tensor --* AllReduceState : all_reduce_input
          Tensor --* ReduceScatterState : reduce_scatter_input
          Tensor --* DTensor : _local_tensor
          Tensor --* NestedTensor : _values
          Tensor --* NestedTensor : _offsets
          Tensor --* BlockMask : kv_num_blocks
          Tensor --* BlockMask : kv_indices
          Tensor --* UninitializedBuffer : cls_to_become
          Tensor --* _Orthogonal : base
          Tensor --* ParametrizationList : original
          Tensor --* PackedSequence_ : data
          Tensor --* PackedSequence_ : batch_sizes
          Tensor --* TorchTensor : raw
          Tensor --* CompositeCompliantTensor : elem
          Tensor --* FeatureSet : dense_features
          Tensor --* FeatureSet : values
          Tensor --* MyClass : obj
          Tensor --* LoggingTensor : elem
          Tensor --* WeightedRandomSampler : weights
          BNReLU2d --* BNReLU2d : _FLOAT_MODULE
          BNReLU2d --* BatchNorm2d : _NNI_BN_RELU_MODULE
          BNReLU3d --* BNReLU3d : _FLOAT_MODULE
          BNReLU3d --* BatchNorm3d : _NNI_BN_RELU_MODULE
          ConvAdd2d --* ConvAdd2d : _FLOAT_MODULE
          ConvAddReLU2d --* ConvAddReLU2d : _FLOAT_MODULE
          ConvReLU1d --* ConvReLU1d : _FLOAT_MODULE
          ConvReLU2d --* ConvReLU2d : _FLOAT_MODULE
          ConvReLU3d --* ConvReLU3d : _FLOAT_MODULE
          LinearLeakyReLU --* LinearLeakyReLU : _FLOAT_MODULE
          LinearReLU --* LinearReLU : _FLOAT_MODULE
          LinearReLU --* LinearReLU : _FLOAT_MODULE
          LinearReLU --* LinearReLU : _FLOAT_MODULE
          LinearTanh --* LinearTanh : _FLOAT_MODULE
          MultiheadAttention --* MultiheadAttention : _FLOAT_MODULE
          LSTM --* LSTM : _FLOAT_MODULE
          LSTMCell --* _LSTMSingleLayer : cell
          _LSTMSingleLayer --* _LSTMLayer : layer_fw
          _LSTMSingleLayer --* _LSTMLayer : layer_fw
          _LSTMSingleLayer --* _LSTMLayer : layer_bw
          _LSTMSingleLayer --* _LSTMLayer : layer_bw
          EmbeddingPackedParams --* Embedding : _packed_params
          FloatFunctional --* MultiheadAttention : q_scaling_product
          FloatFunctional --* LSTMCell : fgate_cx
          FloatFunctional --* LSTMCell : igate_cgate
          FloatFunctional --* LSTMCell : fgate_cx_igate_cgate
          FloatFunctional --* LSTMCell : ogate_cy
          FloatFunctional --* ModelMultipleOps : skip_add
          FloatFunctional --* ModelMultipleOps : cat
          FloatFunctional --* ModelMultipleOpsNoAvgPool : skip_add
          FloatFunctional --* ModelMultipleOpsNoAvgPool : cat
          FloatFunctional --* ModelWithFunctionals : mycat
          FloatFunctional --* ModelWithFunctionals : myadd
          FloatFunctional --* ModelWithFunctionals : myadd_relu
          FloatFunctional --* ModelWithFunctionals : mymatmul
          FloatFunctional --* ResNetBase : myop
          LinearPackedParams --* Linear : _packed_params
          LinearPackedParams --* Linear : _packed_params
          LinearPackedParams --* Linear : _packed_params
          _Container --* BaseDataSparsifier : _container
          _Container --* BaseDataSparsifier : _container
          _Container --* BaseDataSparsifier : _container
          DTypeWithConstraints --* DTypeConfig : input_dtype_with_constraints
          DTypeWithConstraints --* DTypeConfig : output_dtype_with_constraints
          DTypeWithConstraints --* DTypeConfig : weight_dtype_with_constraints
          PrepareCustomConfig --* ObservedGraphModuleAttrs : prepare_custom_config
          MovingAverageMinMaxObserver --* FakeQuantize : activation_post_process
          PerChannelMinMaxObserver --* _InputEqualizationObserver : input_obs
          PerChannelMinMaxObserver --* _WeightEqualizationObserver : weight_col_obs
          QConfig --* QuantWrapper : qconfig
          QConfig --* Embedding : qconfig
          QConfig --* EmbeddingBag : qconfig
          QConfig --* ActivationsTestModel : qconfig
          QConfig --* AnnotatedConvBnReLUModel : qconfig
          QConfig --* AnnotatedConvModel : qconfig
          QConfig --* AnnotatedConvTransposeModel : qconfig
          QConfig --* AnnotatedSingleLayerLinearModel : qconfig
          QConfig --* AnnotatedSkipQuantModel : qconfig
          QConfig --* DeFusedEmbeddingBagLinear : qconfig
          QConfig --* LSTMwithHiddenDynamicModel : qconfig
          QConfig --* ManualConvLinearQATModel : qconfig
          QConfig --* ManualDropoutQATModel : qconfig
          QConfig --* ManualEmbeddingBagLinear : qconfig
          QConfig --* ManualLinearQATModel : qconfig
          QConfig --* QuantStubModel : qconfig
          QConfig --* SingleLayerLinearDynamicModel : qconfig
          QConfigMapping --* ObservedGraphModuleAttrs : qconfig_mapping
          QuantizationConfig --* OperatorConfig : config
          DeQuantStub --* MultiheadAttention : dequant_q
          DeQuantStub --* MultiheadAttention : dequant_k
          DeQuantStub --* MultiheadAttention : dequant_v
          DeQuantStub --* QuantWrapper : dequant
          DeQuantStub --* ActivationsTestModel : dequant
          DeQuantStub --* AnnotatedConvBnModel : dequant
          DeQuantStub --* AnnotatedConvBnReLUModel : dequant
          DeQuantStub --* AnnotatedConvModel : dequant
          DeQuantStub --* AnnotatedConvTransposeModel : dequant
          DeQuantStub --* DeFusedEmbeddingBagLinear : dequant
          DeQuantStub --* EmbeddingWithStaticLinear : dequant
          DeQuantStub --* ManualConvLinearQATModel : dequant
          DeQuantStub --* ManualDropoutQATModel : dequant
          DeQuantStub --* ManualEmbeddingBagLinear : dequant
          DeQuantStub --* ManualLinearQATModel : dequant
          DeQuantStub --* ModelForFusion : dequant
          DeQuantStub --* ModelForFusionWithBias : dequant
          DeQuantStub --* ModelWithSequentialFusion : dequant
          DeQuantStub --* QuantStubModel : dequant
          QuantStub --* MultiheadAttention : quant_attn_output
          QuantStub --* MultiheadAttention : quant_attn_output_weights
          QuantStub --* QuantWrapper : quant
          QuantStub --* ActivationsTestModel : quant
          QuantStub --* AnnotatedConvBnModel : quant
          QuantStub --* AnnotatedConvBnReLUModel : quant
          QuantStub --* AnnotatedConvModel : quant
          QuantStub --* AnnotatedConvTransposeModel : quant
          QuantStub --* DeFusedEmbeddingBagLinear : quant
          QuantStub --* EmbeddingWithStaticLinear : quant
          QuantStub --* ManualConvLinearQATModel : quant
          QuantStub --* ManualDropoutQATModel : quant
          QuantStub --* ManualEmbeddingBagLinear : quant
          QuantStub --* ManualLinearQATModel : quant
          QuantStub --* ModelForFusion : quant
          QuantStub --* ModelForFusionWithBias : quant
          QuantStub --* ModelWithSequentialFusion : quant
          QuantStub --* NormalizationTestModel : quant
          QuantStub --* QuantStubModel : quant
          QuantWrapper --* AnnotatedCustomConfigNestedModel : fc3
          QuantWrapper --* AnnotatedNestedModel : fc3
          QuantWrapper --* AnnotatedSingleLayerLinearModel : fc1
          QuantWrapper --* AnnotatedSkipQuantModel : sub
          QuantWrapper --* AnnotatedSubNestedModel : sub2
          QuantWrapper --* AnnotatedSubNestedModel : fc3
          QuantWrapper --* AnnotatedTwoLayerLinearModel : fc2
          QuantWrapper --* QuantSubModel : sub2
          QuantWrapper --* TwoLayerLinearModel : fc1
          QuantWrapper --* TwoLayerLinearModel : fc1
          QuantWrapper --* TwoLayerLinearModel : fc2
          set_multithreading_enabled --* ProcessLocalGroup : _ctx
          Node --* GradientEdge : node
          StreamContext --* graph : stream_ctx
          device --* CudaInterface : device
          saved_tensors_hooks --* SACEstimator : _saved_tensor_hook_ctx
          _ProfilerStats --* profile : _stats
          profile --* _KinetoProfile : profiler
          record_function --* profile : step_rec_fn
          record_function --* profile : step_rec_fn
          EventList --* profile : _function_events
          EventList --* profile : _old_function_events
          EventList --* profile : function_events
          EventList --* _server_process_global_profile : function_events
          Interval --* FunctionEvent : time_range
          ContextProp --* CudnnModule : enabled
          ContextProp --* CudnnModule : deterministic
          ContextProp --* CudnnModule : benchmark
          ContextProp --* CudnnModule : allow_tf32
          ContextProp --* MkldnnModule : enabled
          ContextProp --* MkldnnModule : deterministic
          ContextProp --* OptEinsumModule : enabled
          DimOrder --* Operand : dim_order
          cuFFTPlanCacheAttrContextProp --* cuFFTPlanCache : size
          cuFFTPlanCacheAttrContextProp --* cuFFTPlanCache : max_size
          _QEngineProp --* QuantizedEngine : engine
          _SupportedQEnginesProp --* QuantizedEngine : supported_engines
          _CPUinfo --* _Launcher : cpuinfo
          _XNNPACKEnabled --* XNNPACKEngine : enabled
          AccessType --* Access : type
          CUDASanitizerDispatchMode --* CUDASanitizer : dispatch
          EventHandler --* CUDASanitizerDispatchMode : event_handler
          StreamSynchronizations --* EventHandler : syncs
          _TensorsAccessed --* EventHandler : tensors_accessed
          AsyncCollectiveTensor --* _AllGatherRotater : _aggregated_buffer
          AsyncCollectiveTensor --* _AllToAllRotater : _buffer
          ShardMetadata --* Shard : metadata
          ShardedTensor --* MyShardedModel1 : sharded_tensor1
          ShardedTensor --* MyShardedModel2 : sharded_tensor2
          ShardedTensorMetadata --* ShardedTensorBase : _metadata
          TensorProperties --* ShardedTensorMetadata : tensor_properties
          ShardingSpec --* ShardedTensorBase : _sharding_spec
          ModOrder --* ModuleInfo : mod_order
          MemoryProfileDispatchMode --* MemoryTracker : profile_mode
          ModTracker --* MemTracker : _mod_tracker
          ModTracker --* RuntimeEstimator : _mod_tracker
          ModTracker --* SACEstimator : _mod_tracker
          _OptimizerHookState --* _OverlappedStandardOptimizer : _opt_hook_state
          FileSystem --* FsspecReader : fs
          FileSystem --* FsspecWriter : fs
          FileSystem --* FileSystemReader : fs
          FileSystem --* _FileSystemWriter : fs
          ChunkStorageMetadata --* TensorWriteData : chunk
          Metadata --* DynamicMetaLoadPlanner : metadata
          Metadata --* _ReaderWithOffset : metadata
          MetadataIndex --* ReadItem : dest_index
          MetadataIndex --* ReadItem : storage_index
          MetadataIndex --* WriteItem : index
          MetadataIndex --* WriteResult : index
          TensorProperties --* TensorStorageMetadata : properties
          TensorProperties --* TensorWriteData : properties
          TensorStorageMetadata --* LocalShardsWrapper : _storage_meta
          LoadItemType --* ReadItem : type
          WriteItemType --* WriteItem : type
          DeviceMesh --* DataParallelMeshInfo : mesh
          DeviceMesh --* MeshTopoInfo : mesh
          DeviceMesh --* DTensorSpec : mesh
          DeviceMesh --* OpInfo : mesh
          DeviceMesh --* _TensorParallelTransformPass : mesh
          WorkerGroup --* SimpleElasticAgent : _worker_group
          WorkerState --* RunResult : state
          HealthCheckServer --* LocalElasticAgent : _health_check_server
          EventSource --* Event : source
          NodeState --* RdzvEvent : node_state
          MultiprocessContext --* LocalElasticAgent : _pcontext
          SubprocessContext --* LocalElasticAgent : _pcontext
          TailLog --* PContext : _stdout_tail
          TailLog --* PContext : _stderr_tail
          RendezvousHandler --* WorkerSpec : rdzv_handler
          RendezvousStoreInfo --* DynamicRendezvousHandler : _bootstrap_store_info
          RendezvousBackend --* _BackendRendezvousStateHolder : _backend
          RendezvousSettings --* DynamicRendezvousHandler : _settings
          RendezvousSettings --* _BackendRendezvousStateHolder : _settings
          RendezvousSettings --* _DistributedRendezvousOpExecutor : _settings
          RendezvousSettings --* _RendezvousContext : settings
          RendezvousTimeout --* RendezvousSettings : timeout
          _DistributedRendezvousOpExecutor --* DynamicRendezvousHandler : _op_executor
          _NodeDesc --* DynamicRendezvousHandler : _this_node
          _NodeDesc --* _DistributedRendezvousOpExecutor : _node
          _NodeDesc --* _RendezvousContext : node
          _NodeDescGenerator --* DynamicRendezvousHandler : _node_desc_generator
          _RendezvousState --* _BackendRendezvousStateHolder : _state
          _RendezvousState --* _BackendRendezvousStateHolder : _state
          _RendezvousState --* _RendezvousContext : state
          _RendezvousStateHolder --* DynamicRendezvousHandler : _state_holder
          _RendezvousStateHolder --* _DistributedRendezvousOpExecutor : _state_holder
          _PeriodicTimer --* DynamicRendezvousHandler : _keep_alive_timer
          _Context --* _PeriodicTimer : _ctx
          FileTimerServer --* LocalElasticAgent : _worker_watchdog
          _FSDPDeviceHandle --* _FSDPState : _device_handle
          _FSDPDeviceHandle --* FlatParamHandle : _device_handle
          _FSDPState --* FSDPParamInfo : state
          FlatParamHandle --* FSDPParamInfo : handle
          FlatParameter --* FlatParamHandle : flat_param
          TrainingState --* FSDPState : _training_state
          ExtensionsData --* FSDPParam : _extensions_data
          FSDPCommContext --* FSDPParamGroup : comm_ctx
          FSDPCommContext --* FSDPState : _comm_ctx
          FSDPStateContext --* FSDPState : _state_ctx
          _ExecutionInfo --* _ExecOrderTracer : exec_info
          OptimStateDictConfig --* _FSDPState : _optim_state_dict_config
          OptimStateDictConfig --* StateDictSettings : optim_state_dict_config
          StateDictConfig --* _FSDPState : _state_dict_config
          StateDictConfig --* StateDictSettings : state_dict_config
          StateDictType --* _FSDPState : _state_dict_type
          StateDictType --* StateDictSettings : state_dict_type
          _OverlapInfo --* ZeroRedundancyOptimizer : _overlap_info
          DetachExecutor --* Pipe : executor
          SplitPoint --* PipeSplitWrapper : SplitPoint
          _ComputationType --* _Action : computation_type
          _remote_device --* DevicePlacementSpec : device
          RRef --* Agent : agent_rref
          OpDispatcher --* DTensor : _op_dispatcher
          DTensorSpec --* FSDPParam : _sharding_spec
          DTensorSpec --* FSDPParam : _sharding_spec
          DTensorSpec --* DTensor : _spec
          OpSchema --* OpInfo : schema
          OpSchema --* OutputSharding : redistribute_schema
          MaskBuffer --* _MaskPartial : mask_buffer
          DimSpec --* Broadcast : dim
          DimSpec --* Repeat : input_dim
          DimSpec --* Split : input_dim
          LocalLRUCache --* ShardingPropagator : propagate_op_sharding
          ShardingPropagator --* OpDispatcher : sharding_propagator
          _CommModeModuleTracker --* CommDebugMode : advanced_module_tracker
          _RotateMethod --* _ContextParallelOptions : rotate_method
          Beta --* LKJCholesky : _beta
          Beta --* LKJCholesky : _beta
          Binomial --* Multinomial : _binomial
          Categorical --* Multinomial : _categorical
          Categorical --* Multinomial : _categorical
          Categorical --* OneHotCategorical : _categorical
          Categorical --* OneHotCategorical : _categorical
          Categorical --* ExpRelaxedCategorical : _categorical
          Categorical --* ExpRelaxedCategorical : _categorical
          Chi2 --* StudentT : _chi2
          Chi2 --* Wishart : _dist_chi2
          Chi2 --* Wishart : _dist_chi2
          Dirichlet --* Beta : _dirichlet
          Dirichlet --* Beta : _dirichlet
          Gamma --* FisherSnedecor : _gamma1
          Gamma --* FisherSnedecor : _gamma1
          Gamma --* FisherSnedecor : _gamma2
          Gamma --* FisherSnedecor : _gamma2
          Gamma --* StudentT : _chi2
          Independent --* TransformedDistribution : base_dist
          ATenExportArtifact --* ExportArtifact : aten
          ExportGraphSignature --* GraphModuleDeserializer : signature
          ExportGraphSignature --* Result : signature
          ExportGraphSignature --* ATenExportArtifact : sig
          InputKind --* InputSpec : kind
          OutputKind --* OutputSpec : kind
          _IVals --* UnflattenedModule : ivals
          Future --* BatchUpdateParameterServer : future_model
          Tracer --* TracingConfig : tracer
          DAG --* PartitionResult : dag
          PartitionMode --* PartitionerConfig : mode
          PreDispatchTorchFunctionMode --* _MakefxTracer : proxy_function_mode
          ProxyTorchDispatchMode --* AutogradCompilerInstance : proxy_mode
          ProxyTorchDispatchMode --* DecompositionInterpreter : mode
          ProxyTorchDispatchMode --* _MakefxTracer : proxy_mode
          PythonKeyTracer --* AutogradCompilerInstance : fx_tracer
          PythonKeyTracer --* _MakefxTracer : fx_tracer
          PythonKeyTracer --* _MakefxTracer : fx_tracer
          TorchFunctionMetadataMode --* _MakefxTracer : torch_fn_metadata_mode
          _GraphAppendingTracerEx --* DecompositionInterpreter : tracer
          _ModuleStackTracer --* _MakefxTracer : fx_tracer
          _ModuleStackTracer --* _MakefxTracer : fx_tracer
          _SymNodeDict --* PythonKeyTracer : symnode_tracker
          DimConstraints --* ShapeEnv : dim_constraints
          DynamicDimConstraintPrinter --* DimConstraints : _dcp
          PropagateUnbackedSymInts --* _ExportPassBaseDeprecatedDoNotUse : interpreter
          ShapeEnv --* AutogradCompilerInstance : shape_env
          ShapeEnv --* GraphModuleDeserializer : shape_env
          ShapeEnvSettings --* ShapeEnv : settings
          TranslationValidator --* ShapeEnv : validator
          _Z3Ops --* SympyToZ3 : _ops
          CodeGen --* Graph : _codegen
          CodeGen --* Graph : _codegen
          Graph --* SubgraphTracer : graph
          Graph --* TS2FXGraphConverter : fx_graph
          Graph --* ExportTracer : graph
          Graph --* GraphModuleDeserializer : graph
          Graph --* GraphModuleDeserializer : graph
          Graph --* GraphModuleDeserializer : graph
          Graph --* ReproState : graph
          Graph --* LightTracer : graph
          Graph --* LoopBodyBlock : graph
          Graph --* MatchContext : graph
          Graph --* PipeInfo : graph
          Graph --* UnflattenedModule : graph
          Graph --* Tracer : graph
          Graph --* Tracer : graph
          Graph --* PythonKeyTracer : graph
          Graph --* ShapeEnv : graph
          Graph --* Transformer : new_graph
          Graph --* Partition : graph
          Graph --* Component : graph
          Graph --* TracerBase : graph
          _FindNodesLookupTable --* Graph : _find_nodes_lookup_table
          _Namespace --* PatternPrettyPrinter : namespace
          _Namespace --* Graph : _graph_namespace
          GraphModule --* ExportResult : graph_module
          GraphModule --* Result : graph_module
          GraphModule --* GraphLowering : orig_gm
          GraphModule --* Subgraph : graph_module
          GraphModule --* BaseStructuredSparsifier : traced
          GraphModule --* ATenExportArtifact : gm
          GraphModule --* ExportedProgram : _graph_module
          GraphModule --* PartitionResult : module_with_submodules
          GraphModule --* FoldedGraphModule : const_subgraph_module
          GraphModule --* SplitResult : split_module
          GraphModule --* Transform : module
          Interpreter --* _ExportPassBaseDeprecatedDoNotUse : interpreter
          TransformerTracer --* Transformer : tracer
          Node --* ExportInterpreter : node
          Node --* CommBlock : comm_node
          Node --* _AllGatherMatch : shard_node
          Node --* _AllGatherMatch : ag_node
          Node --* _AllGatherMatch : res_node
          Node --* _Matmul : A_node
          Node --* _Matmul : B_node
          Node --* _ReduceScatterMatch : input_node
          Node --* _ReduceScatterMatch : rs_node
          Node --* _ReduceScatterMatch : res_node
          Node --* _ScaledMatmul : A_scale_node
          Node --* _ScaledMatmul : B_scale_node
          Node --* GraphLowering : current_node
          Node --* _DeconstructedSymNode : fx_node
          Node --* NSSubgraph : start_node
          Node --* NSSubgraph : end_node
          Node --* NSSubgraph : base_op_node
          Node --* _SubmoduleEntry : parent_call_module
          Node --* Graph : _root
          Node --* Match : anchor
          Node --* ReplacedPatterns : anchor
          _DependencyViewer --* CapabilityBasedPartitioner : dependency_viewer
          Proxy --* OutputGraph : backward_state_proxy
          Proxy --* BackwardState : proxy
          Proxy --* _ProxyTensor : proxy
          Scope --* LightTracer : scope
          Scope --* QuantizationTracer : scope
          Scope --* Tracer : scope
          Scope --* GraphAppendingTracer : scope
          Scope --* TracerBase : scope
          NodeInfo --* NodeSource : node_info
          OrderedDictWrapper --* RecursiveScriptModule : _parameters
          OrderedDictWrapper --* RecursiveScriptModule : _buffers
          OrderedModuleDict --* RecursiveScriptModule : _modules
          RecursiveScriptModule --* GraphModuleDeserializer : constants
          RecursiveScriptModule --* GraphModuleDeserializer : example_inputs
          Library --* _TritonLibrary : lib
          Library --* CustomOpDef : _lib
          Library --* FakeImplHolder : lib
          ConnectionWrapper --* Queue : _reader
          ConnectionWrapper --* Queue : _writer
          ConnectionWrapper --* SimpleQueue : _reader
          ConnectionWrapper --* SimpleQueue : _writer
          SimpleQueue --* Pool : _inqueue
          SimpleQueue --* Pool : _outqueue
          StorageWeakRef --* StorageWeakRefWrapper : ref
          StorageWeakRef --* StorageWeakRefWrapper : ref
          ProcessContext --* MultiprocessContext : _pc
          OrderedDictWrapper --* ModuleWrapper : _parameters
          OrderedDictWrapper --* ModuleWrapper : _buffers
          OrderedDictWrapper --* ModuleWrapper : _modules
          ELU --* ActivationsTestModel : elu
          GELU --* SimpleMegatronLM : gelu
          GELU --* FeedForward : gelu
          Hardswish --* ActivationsTestModel : hardswish
          Hardtanh --* Conv2dWithObsSharingOps : hardtanh
          LeakyReLU --* LinearBnLeakyReluModel : leaky_relu
          MultiheadAttention --* MultiheadAttention : _FLOAT_MODULE
          MultiheadAttention --* TransformerDecoderLayer : self_attn
          MultiheadAttention --* TransformerDecoderLayer : multihead_attn
          MultiheadAttention --* TransformerEncoderLayer : self_attn
          ReLU --* DoubleLinear : relu
          ReLU --* Conv2dPadBias : act1
          ReLU --* Conv2dPool : af1
          ReLU --* Conv2dPoolFlatten : af1
          ReLU --* Conv2dPoolFlattenFunctional : af1
          ReLU --* LinearActivation : act1
          ReLU --* LinearActivationFunctional : act1
          ReLU --* AnnotatedConvBnReLUModel : relu
          ReLU --* ConvBnAddReluModel : relu
          ReLU --* ConvBnReLUModel : relu
          ReLU --* ConvReluAddModel : relu
          ReLU --* ConvReluAddModel : relu
          ReLU --* ConvReluConvModel : relu
          ReLU --* ConvReluModel : relu
          ReLU --* FunctionalConvReluConvModel : relu
          ReLU --* FunctionalLinearReluLinearModel : relu
          ReLU --* InnerModule : relu1
          ReLU --* InnerModule : relu2
          ReLU --* LinearReluAddModel : relu
          ReLU --* LinearReluAddModel : relu
          ReLU --* LinearReluLinearModel : relu
          ReLU --* LinearReluModel : relu
          ReLU --* ModelForFusion : relu1
          ReLU --* ModelForFusion : relu2
          ReLU --* ModelForFusion : relu3
          ReLU --* ModelForFusion : relu4
          ReLU --* ModelForFusionWithBias : relu1
          ReLU --* ModelMultipleOps : relu1
          ReLU --* ModelMultipleOps : relu2
          ReLU --* ModelMultipleOpsNoAvgPool : relu1
          ReLU --* ModelMultipleOpsNoAvgPool : relu2
          ReLU --* ModelWithSequentialFusion : relu1
          ReLU --* ResNetBase : relu1
          ReLU --* ResNetBase : relu2
          ReLU --* SubModelWithoutFusion : relu
          ReLU --* ConvBnReLU2dAndLinearReLU : relu
          ReLU --* ConvTWithBNRelu : relu
          ReLU --* ConvWithBNRelu : relu
          ReLU --* LinearReluModel : relu
          ReLU --* Net : relu
          ReLU --* MLPModule : relu
          ReLU --* RemoteNet : relu
          ReLU --* ToyModel : relu
          ReLU --* Net : relu
          ReLU --* Net : relu
          Sigmoid --* LSTMCell : input_gate
          Sigmoid --* LSTMCell : forget_gate
          Sigmoid --* LSTMCell : output_gate
          Tanh --* LSTMCell : cell_gate
          Tanh --* Conv2dPadBias : act2
          Tanh --* LinearActivation : act2
          Tanh --* LinearTanhModel : tanh
          BatchNorm1d --* LinearBn1d : bn
          BatchNorm1d --* LazyBatchNorm1d : cls_to_become
          BatchNorm1d --* TransformerWithSharedParams : bn
          BatchNorm1d --* LinearBnLeakyReluModel : bn1d
          BatchNorm1d --* ModelForConvTransposeBNFusion : bn1
          BatchNorm1d --* ModelForFusion : bn3
          BatchNorm1d --* ModelForLinearBNFusion : bn
          BatchNorm1d --* BatchNormNet : bn
          BatchNorm2d --* _ConvBnNd : bn
          BatchNorm2d --* LazyBatchNorm2d : cls_to_become
          BatchNorm2d --* AnnotatedConvBnModel : bn
          BatchNorm2d --* AnnotatedConvBnReLUModel : bn
          BatchNorm2d --* ConvBnAddReluModel : bn
          BatchNorm2d --* ConvBnModel : bn
          BatchNorm2d --* ConvBnReLUModel : bn
          BatchNorm2d --* ModelForConvTransposeBNFusion : bn2
          BatchNorm2d --* ModelForFusion : bn1
          BatchNorm2d --* ModelForFusionWithBias : bn1
          BatchNorm2d --* ModelForFusionWithBias : bn2
          BatchNorm2d --* ModelMultipleOps : bn1
          BatchNorm2d --* ModelMultipleOpsNoAvgPool : bn1
          BatchNorm2d --* ResNetBase : bn1
          BatchNorm2d --* SubModelForFusion : bn
          BatchNorm2d --* ConvTWithBNRelu : bn
          BatchNorm2d --* ConvWithBNRelu : bn
          BatchNorm3d --* LazyBatchNorm3d : cls_to_become
          BatchNorm3d --* ModelForConvTransposeBNFusion : bn3
          BatchNorm3d --* ModelForFusion : bn2
          ModuleDict --* LSTMCell : igates
          ModuleDict --* LSTMCell : hgates
          ModuleDict --* LSTMCell : gates
          ModuleList --* LSTM : layers
          ModuleList --* GRU : _all_weight_values
          ModuleList --* LSTM : _all_weight_values
          ModuleList --* RNNBase : _all_weight_values
          ModuleList --* RNNBase : _all_weight_values
          ModuleList --* AdaptiveLogSoftmaxWithLoss : tail
          ModuleList --* TransformerDecoder : layers
          ModuleList --* TransformerEncoder : layers
          ModuleList --* MLPStacked : layers
          ModuleList --* Transformer : layers
          ParameterList --* _ReplicateState : _param_list
          ParameterList --* _ReplicateState : _orig_module
          Sequential --* NestedSequentialModel : seq1
          Sequential --* NestedSequentialModel : seq2
          Sequential --* UnitModule : seq
          Sequential --* UnitParamModule : seq
          Sequential --* MixtureOfExperts : module
          Sequential --* NestedWrappedModule : module
          Sequential --* NonUniformReqGradNWM : module
          Sequential --* Conv2dActivation : seq
          Sequential --* Conv2dBias : seq
          Sequential --* Conv2dPadBias : seq
          Sequential --* Conv2dPool : seq
          Sequential --* Conv2dPoolFlatten : seq
          Sequential --* Conv2dPoolFlattenFunctional : seq
          Sequential --* LinearActivation : seq
          Sequential --* LinearActivationFunctional : seq
          Sequential --* LinearBias : seq
          Sequential --* SimpleConv2d : seq
          Sequential --* SimpleLinear : seq
          Sequential --* DenseTopMLP : dense_mlp
          Sequential --* DenseTopMLP : top_mlp
          Sequential --* ModelWithSequentialFusion : features
          Sequential --* ModelWithSequentialFusion : classifier
          Sequential --* ModelWithSequentialFusion : seq
          Sequential --* MyConvNetForMNIST : net
          Conv1d --* LazyConv1d : cls_to_become
          Conv1d --* ModelForFusion : conv3
          Conv1d --* Conv2dThenConv1d : conv1d
          Conv2d --* LazyConv2d : cls_to_become
          Conv2d --* Conv2dActivation : conv2d1
          Conv2d --* Conv2dActivation : conv2d2
          Conv2d --* Conv2dBias : conv2d1
          Conv2d --* Conv2dBias : conv2d2
          Conv2d --* Conv2dPadBias : conv2d1
          Conv2d --* Conv2dPadBias : conv2d2
          Conv2d --* Conv2dPool : conv2d1
          Conv2d --* Conv2dPool : conv2d2
          Conv2d --* Conv2dPool : conv2d3
          Conv2d --* Conv2dPoolFlatten : conv2d1
          Conv2d --* Conv2dPoolFlatten : conv2d2
          Conv2d --* Conv2dPoolFlattenFunctional : conv2d1
          Conv2d --* Conv2dPoolFlattenFunctional : conv2d2
          Conv2d --* SimpleConv2d : conv2d1
          Conv2d --* SimpleConv2d : conv2d2
          Conv2d --* AnnotatedConvBnModel : conv
          Conv2d --* AnnotatedConvBnReLUModel : conv
          Conv2d --* AnnotatedConvModel : conv
          Conv2d --* ConvBnAddReluModel : conv
          Conv2d --* ConvBnAddReluModel : conv2
          Conv2d --* ConvBnModel : conv
          Conv2d --* ConvBnReLUModel : conv
          Conv2d --* ConvModel : conv
          Conv2d --* ConvReluAddModel : fc1
          Conv2d --* ConvReluAddModel : fc2
          Conv2d --* ConvReluConvModel : fc1
          Conv2d --* ConvReluConvModel : fc2
          Conv2d --* ConvReluModel : fc
          Conv2d --* ManualConvLinearQATModel : conv
          Conv2d --* ModelForFusion : conv1
          Conv2d --* ModelForFusionWithBias : conv1
          Conv2d --* ModelForFusionWithBias : conv2
          Conv2d --* ModelMultipleOps : conv1
          Conv2d --* ModelMultipleOps : conv2
          Conv2d --* ModelMultipleOpsNoAvgPool : conv1
          Conv2d --* ModelMultipleOpsNoAvgPool : conv2
          Conv2d --* ModelWithSequentialFusion : conv1
          Conv2d --* ResNetBase : conv1
          Conv2d --* SubModelForFusion : conv
          Conv2d --* SubModelWithoutFusion : conv
          Conv2d --* Conv2dPropAnnotaton : conv
          Conv2d --* Conv2dThenConv1d : conv2d
          Conv2d --* Conv2dWithCat : conv1
          Conv2d --* Conv2dWithCat : conv2
          Conv2d --* Conv2dWithObsSharingOps : conv
          Conv2d --* Conv2dWithTwoCat : conv1
          Conv2d --* Conv2dWithTwoCat : conv2
          Conv2d --* Conv2dWithTwoLinear : conv
          Conv2d --* Conv2dWithTwoLinearPermute : conv
          Conv2d --* ConvLinearWPermute : conv
          Conv2d --* ConvMaxPool2d : conv
          Conv2d --* ConvWithAdaptiveAvgPool2d : conv
          Conv2d --* ConvWithBNRelu : conv
          Conv2d --* EmbeddingConvLinearModule : conv
          Conv2d --* GroupwiseConv2d : conv
          Conv2d --* TwoLayerConvModel : conv1
          Conv2d --* TwoLayerConvModel : conv2
          Conv3d --* LazyConv3d : cls_to_become
          Conv3d --* ModelForFusion : conv2
          ConvTranspose1d --* LazyConvTranspose1d : cls_to_become
          ConvTranspose1d --* ModelForConvTransposeBNFusion : conv1
          ConvTranspose2d --* LazyConvTranspose2d : cls_to_become
          ConvTranspose2d --* AnnotatedConvTransposeModel : conv
          ConvTranspose2d --* ConvTransposeModel : conv
          ConvTranspose2d --* ModelForConvTransposeBNFusion : conv2
          ConvTranspose2d --* ConvTWithBNRelu : convt
          ConvTranspose3d --* LazyConvTranspose3d : cls_to_become
          ConvTranspose3d --* ModelForConvTransposeBNFusion : conv3
          Dropout --* TransformerDecoderLayer : dropout
          Dropout --* TransformerDecoderLayer : dropout1
          Dropout --* TransformerDecoderLayer : dropout2
          Dropout --* TransformerDecoderLayer : dropout3
          Dropout --* TransformerEncoderLayer : dropout
          Dropout --* TransformerEncoderLayer : dropout1
          Dropout --* TransformerEncoderLayer : dropout2
          Dropout --* ManualDropoutQATModel : dropout
          Dropout --* Attention : resid_dropout
          Dropout --* FeedForward : resid_dropout
          Dropout --* Transformer : dropout
          Dropout --* Policy : dropout
          Flatten --* Conv2dPoolFlatten : flatten
          InstanceNorm1d --* LazyInstanceNorm1d : cls_to_become
          InstanceNorm1d --* NormalizationTestModel : instance_norm1d
          InstanceNorm2d --* LazyInstanceNorm2d : cls_to_become
          InstanceNorm2d --* NormalizationTestModel : instance_norm2d
          InstanceNorm3d --* LazyInstanceNorm3d : cls_to_become
          InstanceNorm3d --* NormalizationTestModel : instance_norm3d
          Identity --* FloatFunctional : activation_post_process
          Identity --* QFunctional : activation_post_process
          Identity --* TransformerWithSharedParams : bn
          Identity --* ModelMultipleOps : downsample
          Identity --* ResNetBase : downsample
          Identity --* ConvTWithBNRelu : bn
          Identity --* ConvTWithBNRelu : relu
          Identity --* ConvWithBNRelu : bn
          Identity --* ConvWithBNRelu : relu
          Linear --* ClassMethod : linear
          Linear --* Linear : _FLOAT_MODULE
          Linear --* MultiheadAttention : linear_Q
          Linear --* MultiheadAttention : linear_K
          Linear --* MultiheadAttention : linear_V
          Linear --* MultiheadAttention : out_proj
          Linear --* Linear : _FLOAT_MODULE
          Linear --* Linear : _FLOAT_MODULE
          Linear --* AdaptiveLogSoftmaxWithLoss : head
          Linear --* LazyLinear : cls_to_become
          Linear --* Linear : fc2
          Linear --* Linear : lin
          Linear --* TransformerDecoderLayer : linear1
          Linear --* TransformerDecoderLayer : linear2
          Linear --* TransformerEncoderLayer : linear1
          Linear --* TransformerEncoderLayer : linear2
          Linear --* DistributedDataParallel : module
          Linear --* CompositeModel : l1
          Linear --* CompositeModel : l2
          Linear --* CompositeParamModel : l
          Linear --* NestedSequentialModel : lin
          Linear --* UnitModule : l1
          Linear --* UnitModule : l2
          Linear --* UnitParamModule : l
          Linear --* SaveForwardInputsModule : l
          Linear --* DoubleLinear : lin1
          Linear --* DoubleLinear : lin2
          Linear --* MLP : in_proj
          Linear --* MLP : out_proj
          Linear --* NestedLinear : nested_linear
          Linear --* NestedLinear : nested_linear
          Linear --* SkipModel : linear
          Linear --* SkipModule : lin
          Linear --* TransformerWithSharedParams : output_proj
          Linear --* Conv2dPoolFlatten : fc
          Linear --* Conv2dPoolFlattenFunctional : fc
          Linear --* LSTMLayerNormLinearModel : linear
          Linear --* LSTMLinearModel : linear
          Linear --* LinearActivation : linear1
          Linear --* LinearActivation : linear2
          Linear --* LinearActivationFunctional : linear1
          Linear --* LinearActivationFunctional : linear2
          Linear --* LinearActivationFunctional : linear3
          Linear --* SimpleLinear : linear1
          Linear --* SimpleLinear : linear2
          Linear --* AnnotatedSkipQuantModel : fc
          Linear --* AnnotatedTwoLayerLinearModel : fc1
          Linear --* DeFusedEmbeddingBagLinear : linear
          Linear --* EmbeddingWithStaticLinear : fc
          Linear --* InnerModule : fc1
          Linear --* InnerModule : fc2
          Linear --* LinearAddModel : fc1
          Linear --* LinearAddModel : fc2
          Linear --* LinearBnLeakyReluModel : linear
          Linear --* LinearModelWithSubmodule : fc
          Linear --* LinearReluAddModel : fc1
          Linear --* LinearReluAddModel : fc2
          Linear --* LinearReluLinearModel : fc1
          Linear --* LinearReluLinearModel : fc2
          Linear --* LinearReluModel : fc
          Linear --* LinearTanhModel : linear
          Linear --* ManualConvLinearQATModel : fc1
          Linear --* ManualConvLinearQATModel : fc2
          Linear --* ManualDropoutQATModel : fc1
          Linear --* ManualEmbeddingBagLinear : linear
          Linear --* ManualLinearDynamicQATModel : fc1
          Linear --* ManualLinearDynamicQATModel : fc2
          Linear --* ManualLinearQATModel : fc1
          Linear --* ManualLinearQATModel : fc2
          Linear --* ModelForFusion : fc
          Linear --* ModelForLinearBNFusion : fc
          Linear --* ModelMultipleOps : fc
          Linear --* ModelMultipleOpsNoAvgPool : fc
          Linear --* NestedModel : fc3
          Linear --* NormalizationTestModel : fc1
          Linear --* M : linear
          Linear --* QuantStubModel : fc
          Linear --* QuantSubModel : fc3
          Linear --* ResNetBase : fc
          Linear --* SingleLayerLinearDynamicModel : fc1
          Linear --* SingleLayerLinearModel : fc1
          Linear --* SkipQuantModel : fc
          Linear --* Conv2dPropAnnotaton : linear
          Linear --* Conv2dWithTwoLinear : linear1
          Linear --* Conv2dWithTwoLinear : linear2
          Linear --* Conv2dWithTwoLinearPermute : linear1
          Linear --* Conv2dWithTwoLinearPermute : linear2
          Linear --* ConvBnReLU2dAndLinearReLU : linear
          Linear --* ConvLinearWPermute : linear1
          Linear --* EmbeddingConvLinearModule : linear
          Linear --* LinearReluModel : fc
          Linear --* TwoLinearModule : linear1
          Linear --* TwoLinearModule : linear2
          Linear --* TwoLayerLinearModel : fc1
          Linear --* TwoLayerLinearModel : fc2
          Linear --* Net : linear
          Linear --* Net : linear
          Linear --* SimpleMegatronLM : fc1
          Linear --* SimpleMegatronLM : fc2
          Linear --* Attention : wq
          Linear --* Attention : wk
          Linear --* Attention : wv
          Linear --* Attention : wo
          Linear --* FeedForward : w1
          Linear --* FeedForward : w2
          Linear --* MLPModule : net1
          Linear --* MLPModule : net2
          Linear --* Transformer : output
          Linear --* HybridModel : fc1
          Linear --* HybridModel : fc2
          Linear --* RemoteNet : fc
          Linear --* BatchNormNet : fc1
          Linear --* BatchNormNet : fc2
          Linear --* ControlFlowToyModel : lin1
          Linear --* ControlFlowToyModel : lin2
          Linear --* TestModel : fc1
          Linear --* TestModel : fc2
          Linear --* TestModel : fc2
          Linear --* SubModule : lin_layer
          Linear --* MyModel : m
          Linear --* MyModel : fc1
          Linear --* MyModel : fc2
          Linear --* ToyModel : lin1
          Linear --* ToyModel : lin2
          Linear --* NetWithBuffers : a
          Linear --* NetWithBuffers : b
          Linear --* Net : lin
          Linear --* ToyModel : lin1
          Linear --* ToyModel : lin2
          Linear --* ToyModel : lin
          Linear --* DummyTestModel : fc
          Linear --* MyModel : fc1
          Linear --* MyModel : fc2
          Linear --* NamedTupleModule : lin
          Linear --* MyModel : fc1
          Linear --* MyModel : fc1
          Linear --* MyModel : fc2
          Linear --* ToyModel : net1
          Linear --* ToyModel : net2
          Linear --* MyModel : fc
          Linear --* NestedOutputModule : lin
          Linear --* ModelWithComm : lin
          Linear --* ToyModel : net1
          Linear --* ToyModel : net2
          Linear --* ToyModel : net1
          Linear --* ToyModel : net2
          Linear --* ToyModel : net1
          Linear --* ToyModel : net2
          Linear --* SimpleConditionalModel : nn1
          Linear --* SimpleConditionalModel : nn2
          Linear --* SimpleConditionalModel : nn3
          Linear --* SimpleConditionalModel : nn4
          Linear --* MockModule : l1
          Linear --* Net : lin
          Linear --* EmbeddingNetDifferentParams : lin
          Linear --* EmbeddingNetDifferentParams : lin2
          Linear --* LargeNet : fc1
          Linear --* LargeNet : fc2
          Linear --* Net : fc1
          Linear --* Net : fc3
          Linear --* NetWithBuffers : a
          Linear --* NetWithBuffers : b
          Linear --* TwoLinLayerNet : a
          Linear --* TwoLinLayerNet : b
          Linear --* UnusedParamTwoLinLayerNet : a
          Linear --* UnusedParamTwoLinLayerNet : b
          Linear --* UnusedParamTwoLinLayerNet : c
          Linear --* _FC2 : fc
          Linear --* BatchUpdateParameterServer : model
          Linear --* Policy : affine1
          Linear --* Policy : affine2
          Linear --* MyModel : lin
          NonDynamicallyQuantizableLinear --* MultiheadAttention : out_proj
          L1Loss --* Trainer : loss_fn
          Module --* ExportCase : model
          Module --* ExportTracer : root
          Module --* GraphModuleDeserializer : module
          Module --* GraphModuleDeserializer : module
          Module --* GraphModuleDeserializer : module
          Module --* LSTMCell : igates
          Module --* LSTMCell : hgates
          Module --* LSTMCell : gates
          Module --* QuantWrapper : module
          Module --* _ReplicateState : module
          Module --* ParamInfo : module
          Module --* SharedParamInfo : module
          Module --* SharedParamInfo : prim_module
          Module --* ParamModuleInfo : module
          Module --* _ParamUsageInfo : module
          Module --* _SubmoduleEntry : parent_module
          Module --* _SubmoduleEntry : module
          Module --* Tracer : root
          Module --* PythonKeyTracer : root
          Module --* LSTMwithHiddenDynamicModel : lstm
          Module --* RNNDynamicModel : mod
          Module --* RNNDynamicModel : mod
          Module --* DDPUnevenTestInput : model
          GroupNorm --* NormalizationTestModel : group_norm
          LayerNorm --* TransformerDecoderLayer : norm1
          LayerNorm --* TransformerDecoderLayer : norm2
          LayerNorm --* TransformerDecoderLayer : norm3
          LayerNorm --* TransformerEncoderLayer : norm1
          LayerNorm --* TransformerEncoderLayer : norm2
          LayerNorm --* LSTMLayerNormLinearModel : norm
          LayerNorm --* NormalizationTestModel : layer_norm
          LayerNorm --* Transformer : norm
          LayerNorm --* TransformerBlock : attention_norm
          LayerNorm --* TransformerBlock : ffn_norm
          AdaptiveAvgPool2d --* Conv2dPoolFlatten : avg_pool
          AdaptiveAvgPool2d --* Conv2dPoolFlattenFunctional : avg_pool
          AdaptiveAvgPool2d --* ModelMultipleOps : avgpool
          AdaptiveAvgPool2d --* ResNetBase : avgpool
          AdaptiveAvgPool2d --* Conv2dWithObsSharingOps : adaptive_avg_pool2d
          AdaptiveAvgPool2d --* ConvWithAdaptiveAvgPool2d : adaptive_avg_pool2d
          MaxPool2d --* Conv2dPool : maxpool
          MaxPool2d --* ModelMultipleOpsNoAvgPool : maxpool
          MaxPool2d --* ConvMaxPool2d : pool
          GRU --* GRU : _FLOAT_MODULE
          GRUCell --* RNNCellDynamicModel : mod
          LSTM --* LSTM : _FLOAT_MODULE
          LSTM --* LSTM : _FLOAT_MODULE
          LSTM --* LSTMLayerNormLinearModel : lstm
          LSTM --* LSTMLinearModel : lstm
          LSTMCell --* LSTMCell : _FLOAT_MODULE
          LSTMCell --* RNNCellDynamicModel : mod
          RNNBase --* RNNBase : _FLOAT_MODULE
          RNNCell --* RNNCellDynamicModel : mod
          RNNCell --* RNNCellDynamicModel : mod
          Embedding --* Embedding : _FLOAT_MODULE
          Embedding --* TransformerWithSharedParams : embed_tokens
          Embedding --* DeFusedEmbeddingBagLinear : emb
          Embedding --* EmbeddingModule : emb
          Embedding --* EmbeddingConvLinearModule : emb
          Embedding --* EmbeddingModule : emb
          Embedding --* Transformer : tok_embeddings
          Embedding --* Transformer : pos_embeddings
          Embedding --* EmbeddingNetDifferentParams : embedding
          EmbeddingBag --* EmbeddingBag : _FLOAT_MODULE
          EmbeddingBag --* EmbBagWrapper : emb_bag
          EmbeddingBag --* EmbeddingBagModule : emb
          EmbeddingBag --* EmbeddingWithStaticLinear : emb
          EmbeddingBag --* ManualEmbeddingBagLinear : emb
          EmbeddingBag --* RemoteEM : em
          EmbeddingBag --* MyEmbeddingBagModel : eb
          Transformer --* TransformerWithSharedParams : transformer
          TransformerDecoder --* Transformer : decoder
          TransformerEncoder --* Transformer : encoder
          DistributedDataParallel --* _ReplicateState : _ddp
          DistributedDataParallel --* HybridModel : fc2
          DistributedDataParallel --* Trainer : hybrid_module
          _BufferCommHook --* DistributedDataParallel : buffer_hook
          _BufferCommHookLocation --* _BufferCommHook : buffer_comm_hook_location
          Buffer --* Layer : layer_dummy_buf
          Buffer --* Net : dummy_buf
          Parameter --* ListVariable : class_type
          Parameter --* SizeVariable : class_type
          Parameter --* TupleVariable : class_type
          Parameter --* TorchInGraphFunctionVariable : class_type
          Parameter --* UserDefinedObjectVariable : class_type
          Parameter --* _ConvBnNd : bias
          Parameter --* LinearBn1d : bias
          Parameter --* Linear : weight
          Parameter --* Linear : bias
          Parameter --* _LearnableFakeQuantize : scale
          Parameter --* _LearnableFakeQuantize : scale
          Parameter --* _LearnableFakeQuantize : zero_point
          Parameter --* _LearnableFakeQuantize : zero_point
          Parameter --* FSDPParam : sharded_param
          Parameter --* FSDPParam : _unsharded_param
          Parameter --* FSDPParam : _sharded_post_forward_param
          Parameter --* MultiheadAttention : q_proj_weight
          Parameter --* MultiheadAttention : q_proj_weight
          Parameter --* MultiheadAttention : k_proj_weight
          Parameter --* MultiheadAttention : k_proj_weight
          Parameter --* MultiheadAttention : v_proj_weight
          Parameter --* MultiheadAttention : v_proj_weight
          Parameter --* MultiheadAttention : in_proj_weight
          Parameter --* MultiheadAttention : in_proj_bias
          Parameter --* MultiheadAttention : bias_k
          Parameter --* MultiheadAttention : bias_k
          Parameter --* MultiheadAttention : bias_v
          Parameter --* MultiheadAttention : bias_v
          Parameter --* PReLU : weight
          Parameter --* _NormBase : weight
          Parameter --* _NormBase : bias
          Parameter --* ModuleDict : weight
          Parameter --* ModuleDict : weight
          Parameter --* ModuleDict : bias
          Parameter --* ModuleDict : bias
          Parameter --* _ConvNd : weight
          Parameter --* _ConvNd : weight
          Parameter --* _ConvNd : bias
          Parameter --* Bilinear : weight
          Parameter --* Bilinear : bias
          Parameter --* Linear : weight
          Parameter --* Linear : weight
          Parameter --* Linear : weight
          Parameter --* Linear : weight
          Parameter --* Linear : weight
          Parameter --* Linear : weight
          Parameter --* Linear : weight
          Parameter --* Linear : weight
          Parameter --* Linear : weight
          Parameter --* Linear : weight
          Parameter --* Linear : weight
          Parameter --* Linear : weight
          Parameter --* Linear : weight
          Parameter --* Linear : weight
          Parameter --* Linear : bias
          Parameter --* Linear : bias
          Parameter --* Linear : bias
          Parameter --* Linear : bias
          Parameter --* Linear : bias
          Parameter --* Linear : bias
          Parameter --* Linear : bias
          Parameter --* Linear : bias
          Parameter --* Linear : bias
          Parameter --* Linear : bias
          Parameter --* NonDynamicallyQuantizableLinear : weight
          Parameter --* NonDynamicallyQuantizableLinear : bias
          Parameter --* GroupNorm : weight
          Parameter --* GroupNorm : bias
          Parameter --* LayerNorm : weight
          Parameter --* LayerNorm : bias
          Parameter --* RMSNorm : weight
          Parameter --* RNNCellBase : weight_ih
          Parameter --* RNNCellBase : weight_hh
          Parameter --* RNNCellBase : bias_ih
          Parameter --* RNNCellBase : bias_hh
          Parameter --* Embedding : weight
          Parameter --* Embedding : weight
          Parameter --* Embedding : weight
          Parameter --* EmbeddingBag : weight
          Parameter --* EmbeddingBag : weight
          Parameter --* EmbeddingBag : weight
          Parameter --* UninitializedParameter : cls_to_become
          Parameter --* CompositeParamModel : p
          Parameter --* UnitParamModule : p
          Parameter --* Layer : layer_dummy_param
          Parameter --* Net : dummy_param
          Parameter --* MyShardedModel1 : random_tensor1
          Parameter --* MyShardedModel2 : random_tensor2
          Parameter --* RMSNormPython : weight
          Parameter --* MyModel : p
          Parameter --* Model : p
          Parameter --* ToyModel : bias
          Parameter --* ExceptionModule : param
          Parameter --* Net : no_grad_param
          Parameter --* Task : p
          Parameter --* LinearReluFunctional : w1
          Parameter --* LinearReluFunctional : b1
          Parameter --* LinearReluFunctionalChild : w1
          Parameter --* LinearReluFunctionalChild : b1
          UninitializedBuffer --* _LazyNormBase : running_mean
          UninitializedBuffer --* _LazyNormBase : running_var
          UninitializedParameter --* _LazyNormBase : weight
          UninitializedParameter --* _LazyNormBase : bias
          UninitializedParameter --* LazyConv1d : weight
          UninitializedParameter --* LazyConv1d : bias
          UninitializedParameter --* LazyConv2d : weight
          UninitializedParameter --* LazyConv2d : bias
          UninitializedParameter --* LazyConv3d : weight
          UninitializedParameter --* LazyConv3d : bias
          UninitializedParameter --* LazyConvTranspose1d : weight
          UninitializedParameter --* LazyConvTranspose1d : bias
          UninitializedParameter --* LazyConvTranspose2d : weight
          UninitializedParameter --* LazyConvTranspose2d : bias
          UninitializedParameter --* LazyConvTranspose3d : weight
          UninitializedParameter --* LazyConvTranspose3d : bias
          UninitializedParameter --* _LazyConvXdMixin : weight
          UninitializedParameter --* _LazyConvXdMixin : bias
          UninitializedParameter --* LazyLinear : weight
          UninitializedParameter --* LazyLinear : bias
          NamedMemberAccessor --* _ReparametrizeModule : accessor
          ExportOptions --* GraphInfo : export_options
          OnnxRegistry --* ResolvedExportOptions : onnx_registry
          ResolvedExportOptions --* OrtBackend : _resolved_onnx_exporter_options
          _FindOperatorOverloadsInOnnxRegistry --* _POERules : find_operator_overloads_in_onnx_registry
          _FindOpschemaMatchedSymbolicFunction --* _POERules : find_opschema_matched_symbolic_function
          _FxGraphToOnnx --* _POERules : fx_graph_to_onnx
          _FxNodeInsertTypePromotion --* _POERules : fx_node_insert_type_promotion
          _FxNodeToOnnx --* _POERules : fx_node_to_onnx
          _FxPass --* _POERules : fx_pass
          _MissingCustomSymbolicFunction --* _POERules : missing_custom_symbolic_function
          _MissingStandardSymbolicFunction --* _POERules : missing_standard_symbolic_function
          _NoSymbolicFunctionForCallFunction --* _POERules : no_symbolic_function_for_call_function
          _NodeMissingOnnxShapeInference --* _POERules : node_missing_onnx_shape_inference
          _OpLevelDebugging --* _POERules : op_level_debugging
          _OperatorSupportedInNewerOpsetVersion --* _POERules : operator_supported_in_newer_opset_version
          _UnsupportedFxNodeAnalysis --* _POERules : unsupported_fx_node_analysis
          DiagnosticOptions --* ExportOptions : diagnostic_options
          DiagnosticOptions --* ResolvedExportOptions : diagnostic_options
          DiagnosticOptions --* DiagnosticContext : options
          Level --* Diagnostic : level
          Location --* StackFrame : location
          Location --* ThreadFlowLocation : location
          Rule --* Diagnostic : rule
          Stack --* TorchScriptOnnxExportDiagnostic : python_call_stack
          Stack --* TorchScriptOnnxExportDiagnostic : cpp_call_stack
          DiagnosticContext --* ExportDiagnosticEngine : _background_context
          ArtifactLocation --* ArtifactChange : artifact_location
          ArtifactLocation --* Attachment : artifact_location
          Message --* Notification : message
          Message --* Result : message
          PropertyBag --* Result : properties
          Region --* Replacement : deleted_region
          ReportingConfiguration --* ConfigurationOverride : configuration
          ReportingDescriptorReference --* ConfigurationOverride : descriptor
          ReportingDescriptorReference --* ReportingDescriptorRelationship : target
          Tool --* Conversion : tool
          Tool --* Run : tool
          ToolComponent --* Tool : driver
          TypeConstraintParam --* Parameter : type_constraint
          PackageInfo --* GraphModuleOnnxMeta : package_info
          DiagnosticContext --* ResolvedExportOptions : diagnostic_context
          DiagnosticContext --* Transform : diagnostic_context
          DynamoExport --* ResolvedExportOptions : fx_tracer
          _PyTreeExtensionContext --* DynamoFlattenOutputStep : _pytree_extension_context
          OnnxFunctionDispatcher --* ResolvedExportOptions : onnxfunction_dispatcher
          _ModuleStackMeta --* _LeafNode : _stack_meta
          _TypePromotionInterpreter --* InsertTypePromotion : interpreter
          InputAdapter --* FXGraphExtractor : input_adapter
          OutputAdapter --* FXGraphExtractor : output_adapter
          OrtExecutionInfoForAllGraphModules --* OrtBackend : _all_ort_execution_info
          OrtOperatorSupport --* OrtBackend : _supported_ops
          GraphInfoPrettyPrinter --* GraphInfoPrettyPrinter : upper_printer
          GraphInfoPrettyPrinter --* GraphInfoPrettyPrinter : lower_printer
          OnnxBackend --* VerificationOptions : backend
          Adam --* Agent : optimizer
          SGD --* BatchUpdateParameterServer : optimizer
          DiGraph --* PackageExporter : dependency_graph
          DirectoryReader --* PackageImporter : zip_reader
          PackageMangler --* PackageImporter : _mangler
          OrderedImporter --* PackageExporter : importer
          _ModuleProviderAction --* _PatternInfo : action
          _PackageNode --* PackageImporter : root
          CategoryDict --* MemoryProfile : _categories
          DataFlowGraph --* MemoryProfile : _data_flow_graph
          MemoryProfileTimeline --* _KinetoProfile : mem_tl
          OpTree --* MemoryProfile : _op_tree
          SizeMap --* MemoryProfile : _size_map
          _Storage --* TensorKey : storage
          profile --* Profiler : prof
          UntypedStorage --* TypedStorage : _untyped_storage
          UntypedStorage --* TypedStorage : _untyped_storage
          UnitModule --* CompositeModel : u1
          UnitModule --* CompositeModel : u2
          UnitModule --* CompositeParamModel : u1
          UnitModule --* CompositeParamModel : u2
          SaveForwardInputsModule --* SaveForwardInputsModel : c1
          SaveForwardInputsModule --* SaveForwardInputsModel : c2
          _dispatch_dtypes --* ForeachFuncInfo : dtypes
          NestedLinear --* SkipModel : nested_linear
          SkipModule --* SkipModel : linear_skip
          Layer --* Net : l1
          DenseTopMLP --* SparseNNModel : dense_top
          EmbBagWrapper --* SparseNNModel : model_sparse
          FunctionalConv2d --* FunctionalConvReluConvModel : conv1
          FunctionalConv2d --* FunctionalConvReluConvModel : conv2
          FunctionalConv2d --* FunctionalConvReluModel : conv
          FunctionalConv2d --* SingleLayerFunctionalConvModel : conv1
          FunctionalConv2d --* TwoLayerFunctionalConvModel : conv1
          FunctionalConv2d --* TwoLayerFunctionalConvModel : conv2
          FunctionalLinear --* FunctionalLinearAddModel : linear1
          FunctionalLinear --* FunctionalLinearAddModel : linear2
          FunctionalLinear --* FunctionalLinearReluLinearModel : linear1
          FunctionalLinear --* FunctionalLinearReluLinearModel : linear2
          FunctionalLinear --* FunctionalLinearReluModel : linear
          FunctionalLinear --* SingleLayerFunctionalLinearModel : linear1
          FunctionalLinear --* TwoLayerFunctionalLinearModel : linear1
          FunctionalLinear --* TwoLayerFunctionalLinearModel : linear2
          InnerModule --* SkipQuantModel : sub
          LinearReluModel --* AnnotatedCustomConfigNestedModel : sub1
          LinearReluModel --* AnnotatedNestedModel : sub1
          LinearReluModel --* AnnotatedSubNestedModel : sub1
          LinearReluModel --* NestedModel : sub1
          LinearReluModel --* QuantSubModel : sub1
          SubModelForFusion --* ModelForFusion : sub1
          SubModelWithoutFusion --* ModelForFusion : sub2
          ConvWithBNRelu --* ConvBnReLU2dAndLinearReLU : conv_bn_relu
          TwoLayerLinearModel --* AnnotatedCustomConfigNestedModel : sub2
          TwoLayerLinearModel --* AnnotatedNestedModel : sub2
          TwoLayerLinearModel --* LinearModelWithSubmodule : subm
          TwoLayerLinearModel --* NestedModel : sub2
          MyShardedModel2 --* MyShardedModel1 : submodule
          Attention --* TransformerBlock : attention
          FeedForward --* TransformerBlock : feed_forward
          HybridModel --* Trainer : hybrid_module
          BatchNormNet --* SubModule : bn
          SubModule --* MyModel : sub_module
          EmbeddingNetDifferentParams --* SubModule : embedding_net
          Task --* UnusedParamModule : t0
          Task --* UnusedParamModule : t1
          TwoLinLayerNet --* SubModule : lin
          UnusedParamTwoLinLayerNet --* DictOutputModule : module
          _FC2 --* Net : fc2
          DummyEnv --* Observer : env
          Policy --* Agent : policy
          SpectralFuncInfo --* SpectralFuncPythonRefInfo : torch_opinfo
          LinearReluFunctionalChild --* LinearReluFunctional : child
          ContentStoreReader --* InputReader : store
          ContentStoreWriter --* InputWriter : store
          OrderedSet --* BoundVars : unbounded_vars
          OrderedSet --* CSE : invalidated_stores
          OrderedSet --* Kernel : must_keep_buffers
          OrderedSet --* Kernel : store_buffer_names
          OrderedSet --* Kernel : removed_buffers
          OrderedSet --* Kernel : inplaced_to_remove
          OrderedSet --* CppCSEVariable : dependent_itervars
          OrderedSet --* CppWrapperCpu : declared_int_array_vars
          OrderedSet --* CppWrapperCpu : used_cached_devices
          OrderedSet --* CppWrapperCpu : used_cached_dtypes
          OrderedSet --* CppWrapperCpu : used_cached_layouts
          OrderedSet --* CppWrapperCpu : used_cached_memory_formats
          OrderedSet --* CppWrapperCpu : used_cond_predicate
          OrderedSet --* TritonCSEVariable : mask_vars
          OrderedSet --* TritonKernel : outside_loop_vars
          OrderedSet --* TritonKernel : autotune_hints
          OrderedSet --* TritonSymbols : reduction_types
          OrderedSet --* TritonSymbols : block_types
          OrderedSet --* PythonWrapperCodegen : kernel_autotune_names
          OrderedSet --* PythonWrapperCodegen : unbacked_symbol_decls
          OrderedSet --* PythonWrapperCodegen : allocated
          OrderedSet --* PythonWrapperCodegen : freed
          OrderedSet --* PythonWrapperCodegen : _meta_vars
          OrderedSet --* PythonWrapperCodegen : already_codegened_subgraphs
          OrderedSet --* FreeUnbackedSymbolsOpsHandler : symbols
          OrderedSet --* UniformValueConstantFolder : indexing_op_packets
          OrderedSet --* FakeTensorUpdater : processed_hashes
          OrderedSet --* GraphLowering : bound_unbacked_symbols
          OrderedSet --* GraphLowering : zero_dim_cpu_tensor_list
          OrderedSet --* GraphLowering : removed_operations
          OrderedSet --* GraphLowering : removed_buffers
          OrderedSet --* GraphLowering : removed_inplace_buffers
          OrderedSet --* GraphLowering : mutated_buffers
          OrderedSet --* GraphLowering : never_reuse_buffers
          OrderedSet --* GraphLowering : inplaced_to_remove
          OrderedSet --* GraphLowering : mutated_inputs
          OrderedSet --* GraphLowering : nodes_prefer_channels_last
          OrderedSet --* GraphLowering : _warned_fallback
          OrderedSet --* GraphLowering : aligned_inputs
          OrderedSet --* GraphLowering : no_fuse_buffer_names
          OrderedSet --* GraphLowering : all_codegen_kernel_names
          OrderedSet --* TritonTemplateBuffer : allowed_prologue_inps
          OrderedSet --* OpCounterCSE : _used_ops
          OrderedSet --* _TargetExpr : fns_set
          OrderedSet --* BaseSchedulerNode : ancestors
          OrderedSet --* BaseSchedulerNode : last_usage
          OrderedSet --* BaseSchedulerNode : unmet_dependencies
          OrderedSet --* ForeachKernelSchedulerNode : origins
          OrderedSet --* OutputNode : unmet_dependencies
          OrderedSet --* Scheduler : completed_operations
          OrderedSet --* Scheduler : available_buffer_names
          OrderedSet --* Scheduler : logged_slow_fusion
          OrderedSet --* Scheduler : buffer_names_to_free
          OrderedSet --* TritonTemplateCaller : allowed_prologue_inps
          OrderedSet --* PointwiseSubgraphLowering : mutated_buffers
          OrderedSet --* NullKernelHandler : removed_buffers
          OrderedSet --* NullKernelHandler : inplaced_to_remove
          TreeSpec --* GraphSignature : in_spec
          TreeSpec --* GraphSignature : out_spec
          TreeSpec --* Metadata : input_spec
          TreeSpec --* ExportArtifact : out_spec
          TreeSpec --* ModuleCallSignature : in_spec
          TreeSpec --* ModuleCallSignature : out_spec
          TreeSpec --* _PyTreeInfo : in_spec
          FloorDiv --* CppVecKernel : weight_recp_vec_range
          ValueRangeAnalysis --* CSEProxy : vr_analysis
          ValueRanges --* StrictMinMaxConstraint : vr
          ValueRanges --* ValueRanges : ExprVR
          ValueRanges --* ValueRanges : BoolVR
          CapturedTraceback --* RuntimeAssert : stack
          TaskSpec --* Measurement : task_spec
          TaskSpec --* Timer : _task_spec
          TaskSpec --* CallgrindStats : task_spec
          CPPTimer --* Timer : _timer
          Language --* Timer : _language
          FunctionCounts --* CallgrindStats : baseline_inclusive_stats
          FunctionCounts --* CallgrindStats : baseline_exclusive_stats
          FunctionCounts --* CallgrindStats : stmt_inclusive_stats
          FunctionCounts --* CallgrindStats : stmt_exclusive_stats
          _IterableDatasetFetcher --* _SingleProcessDataLoaderIter : _dataset_fetcher
          _MapDatasetFetcher --* _SingleProcessDataLoaderIter : _dataset_fetcher
          _MultiProcessingDataLoaderIter --* DataLoader : _iterator
          _SingleProcessDataLoaderIter --* DataLoader : _iterator
          _SnapshotState --* IterDataPipe : _snapshot_state
          _DataPipeType --* _DataPipeMeta : type
          IterDataPipe --* MapperIterDataPipe : datapipe
          IterDataPipe --* SamplerIterDataPipe : datapipe
          IterDataPipe --* BatcherIterDataPipe : datapipe
          MapDataPipe --* MapperMapDataPipe : datapipe
          MapDataPipe --* BatcherMapDataPipe : datapipe
          _IterDataPipeSerializationWrapper --* DataLoader : dataset
          _MapDataPipeSerializationWrapper --* DataLoader : dataset
          Decoder --* RoutedDecoderIterDataPipe : decoder
          SequentialSampler --* SamplerIterDataPipe : sampler
          _FlopCounterMode --* FlopCounterMode : mode
          TrieNode --* Trie : root
          RemovableHandle --* ModTracker : _fw_pre_handle
          RemovableHandle --* ModTracker : _fw_post_handle
          RemovableHandle --* _CommModeModuleTracker : _fw_pre_handle
          RemovableHandle --* _CommModeModuleTracker : _fw_post_handle
          RemovableHandle --* _CommModeModuleTracker : _bw_handle
          RemovableHandle --* ModuleTracker : _fw_pre_handle
          RemovableHandle --* ModuleTracker : _fw_post_handle
          ModuleTracker --* FlopCounterMode : mod_tracker
          FileWriter --* SummaryWriter : file_writer
          WeakIdKeyDictionary --* TracingContext : tensor_to_context
          WeakIdKeyDictionary --* MetaTensorDescriber : lookup_tensor
          WeakIdKeyDictionary --* MetaTensorDescriber : lookup_storage
          WeakIdKeyDictionary --* FSDPMemTracker : _fsdp_mod_to_saved_methods
          WeakIdKeyDictionary --* MemTracker : memory_tracking
          WeakIdKeyDictionary --* MemTracker : _param_to_grad_hook_handles
          WeakIdKeyDictionary --* MemTracker : _WINFO
          WeakIdKeyDictionary --* PythonKeyTracer : tensor_tracker
          WeakIdKeyDictionary --* PythonKeyTracer : script_object_tracker
          WeakIdKeyDictionary --* _GraphAppendingTracerEx : tensor_tracker
          WeakIdKeyDictionary --* _GraphAppendingTracerEx : script_object_tracker
          WeakIdKeyDictionary --* LoggingTensorHandler : memo
          device --* XpuInterface : device
          Event --* XpuInterface : Event
          Stream --* XpuInterface : Stream
          Stream --* StreamContext : src_prev_stream
          Stream --* StreamContext : dst_prev_stream
          ConvertFrameProtocol --o CatchErrorsWrapper : _torchdynamo_orig_callable
          UserErrorType --o UserError : error_type
          CheckFunctionManager --o GuardBuilder : check_fn_manager
          DeletedGuardManagerWrapper --o CheckFunctionManager : guard_manager
          GuardManagerWrapper --o GuardBuilder : guard_manager
          Config --o ExprCounter : _config
          Config --o Replacer : _config
          Hooks --o CatchErrorsWrapper : hooks
          Hooks --o ConvertFrame : _hooks
          InstructionTranslatorBase --o InliningInstructionTranslator : parent
          SpeculationLog --o InstructionTranslatorBase : speculation_log
          VariableTracker --o BackwardHookVariable : module
          VariableTracker --o BackwardHookVariable : user_hooks
          VariableTracker --o BackwardHookVariable : user_pre_hooks
          VariableTracker --o FunctoolsPartialVariable : func
          VariableTracker --o FilterVariable : fn
          VariableTracker --o MapVariable : fn
          VariableTracker --o RepeatIteratorVariable : item
          VariableTracker --o InspectSignatureVariable : inspected
          ConstantVariable --o _HashableTracker : vt
          ConstantVariable --o DequeVariable : maxlen
          ConstDictVariable --o DictView : dv_dict
          FrozensetVariable --o _HashableTracker : vt
          FrozensetVariable --o DequeVariable : maxlen
          SetVariable --o _HashableTracker : vt
          SetVariable --o DequeVariable : maxlen
          IteratorVariable --o CycleIteratorVariable : iterator
          LazyCache --o LazyVariableTracker : _cache
          InspectSignatureVariable --o InspectBoundArgumentsVariable : signature
          SymNodeVariable --o LazySymNodeFormatString : sym_node_var
          TensorVariable --o DataPtrVariable : from_tensor
          TensorVariable --o UntypedStorageVariable : from_tensor
          AOTConfig --o AOTAutogradCacheDetails : aot_config
          Source --o VariableBuilder : source
          AHContext --o AutoHeuristic : context
          AHContext --o LearnedHeuristicController : context
          AHMetadata --o LearnedHeuristicController : metadata
          CUDABenchmarkRequest --o CUDATemplateCaller : bmreq
          CppBenchmarkRequest --o CppTemplateCaller : bmreq
          TritonBenchmarkRequest --o TritonTemplateCaller : bmreq
          BracesBuffer --o CppKernelProxy : reduction_suffix
          KernelArgs --o LocalBufferContext : kernel_args
          CUDATemplate --o CUDATemplateBuffer : template
          ROCmBenchmarkRequest --o ROCmTemplateCaller : bmreq
          IterationRanges --o IterationRangesEntry : parent
          SIMDKernelFeatures --o SIMDKernel : features
          CUDAGraphNode --o CUDAGraphTreeManager : current_node
          CUDAGraphNode --o CUDAGraphTreeManager : current_node
          CUDAWarmupNode --o CUDAGraphTreeManager : current_node
          GraphID --o CUDAGraphNode : id
          GraphID --o CUDAWarmupNode : id
          WrappedFunction --o CUDAGraphNode : wrapped_function
          WrappedFunction --o CUDAWarmupNode : wrapped_function
          DebugContext --o DebugFormatter : handler
          GraphLowering --o SubgraphLowering : parent
          ChoiceCaller --o ErrorFromChoice : choice
          IRNode --o Conditional : predicate
          IRNode --o MutationLayoutSHOULDREMOVE : target
          IRNode --o TMADescriptor : tensor
          Layout --o CppTemplate : layout
          Layout --o CUDATemplate : layout
          Layout --o ROCmTemplate : layout
          Layout --o ChoiceCaller : layout
          MultiOutput --o FallbackKernel : outputs
          Operation --o MutationOutput : mutating_node
          Subgraph --o Conditional : true_subgraph
          Subgraph --o Conditional : false_subgraph
          Subgraph --o InvokeSubgraph : subgraph
          Subgraph --o WhileLoop : cond_subgraph
          Subgraph --o WhileLoop : body_subgraph
          InterpreterShim --o InterpreterShim : module
          LoopBody --o BoundVars : loop_body
          LoopBody --o LoopBodyBlock : body
          PatternExpr --o ListOf : pattern
          _TargetExpr --o RepeatedExpr : inner_pattern
          BaseSchedulerNode --o WhyNoFuse : node1
          BaseSchedulerNode --o WhyNoFuse : node2
          Scheduler --o CUDACPPScheduling : scheduler
          Scheduler --o CUDACombinedScheduling : _scheduler
          Scheduler --o ROCmCPPScheduling : scheduler
          Scheduler --o BaseSchedulerNode : scheduler
          ExternKernelChoice --o ExternKernelCaller : choice
          StoreOutputSubstitution --o TritonTemplateKernel : ops_handler
          IndentedBuffer --o CppWrapperCpu : prefix
          AsyncClosureHandler --o DeviceContext : async_closure_handler
          ClosureHandler --o DeviceContext : closure_handler
          FakeScriptObject --o FakeScriptMethod : self_fake_obj
          HigherOrderOperator --o TorchHigherOrderOperatorVariable : value
          FakeTensorMode --o TracingContext : fake_mode
          FakeTensorMode --o FakeCopyMode : fake_mode
          FakeTensorMode --o _MakefxTracer : fake_tensor_mode
          FakeTensorMode --o _MakefxTracer : fake_tensor_mode
          Tensor --o FunctionalTensorMetadataEq : tensor
          Tensor --o LOBPCG : X
          Tensor --o _MultiDeviceReplicator : master
          Tensor --o PReLU : weight
          Tensor --o _unsafe_preserve_version_counter : tensor
          Tensor --o NnapiModule : ser_model
          Tensor --o P2POp : tensor
          Tensor --o _CollOp : tensor
          Tensor --o _GeneralMultiDeviceReplicator : master
          Tensor --o _RecvInfo : buffer
          Tensor --o FakeContainsTensor : t
          ObservationType --o BackendPatternConfig : observation_type
          QConfig --o Linear : qconfig
          QConfig --o Linear : qconfig
          QConfig --o Linear : qconfig
          QConfig --o Linear : qconfig
          QConfig --o AnnotatedConvBnModel : qconfig
          QConfig --o EmbeddingWithStaticLinear : qconfig
          QConfig --o RNNCellDynamicModel : qconfig
          QConfig --o RNNDynamicModel : qconfig
          QConfig --o TwoLayerLinearModel : qconfig
          QuantizationConfig --o X86InductorQuantizer : global_config
          QuantizationConfig --o XNNPACKQuantizer : global_config
          BackwardCFunction --o FakeBackwardCFunction : real
          _MultiHandle --o FSDPState : _pre_forward_hook_handle
          _MultiHandle --o FSDPState : _post_forward_hook_handle
          profile --o BasicEvaluation : profile
          Stream --o StreamContext : prev_stream
          Stream --o StreamContext : prev_stream
          Access --o UnsynchronizedAccessError : current_access
          Access --o UnsynchronizedAccessError : previous_access
          ShardingSpec --o ShardedTensor : _sharding_spec
          _RefType --o _WeakRefInfo : reftype
          CheckpointImpl --o CheckpointWrapper : checkpoint_impl
          ModelAverager --o PostLocalSGDOptimizer : averager
          Metadata --o DefaultSavePlanner : metadata
          SavePlan --o DefaultSavePlanner : plan
          SavePlan --o DefaultSavePlanner : plan
          StorageReader --o _Checkpointer : storage_reader
          StorageWriter --o _Checkpointer : storage_writer
          DeviceMesh --o DTensorConverter : mesh
          WorkerSpec --o WorkerGroup : spec
          MetricHandler --o MetricStream : handler
          LogsSpecs --o LocalElasticAgent : _logs_specs
          RendezvousStoreInfo --o RendezvousInfo : _bootstrap_store_info
          RequestQueue --o TimerServer : _request_queue
          TrainingState --o FullyShardedDataParallel : training_state
          HandleShardingStrategy --o FlatParamHandle : _sharding_strategy
          MixedPrecisionPolicy --o FSDPParam : mp_policy
          MixedPrecisionPolicy --o FSDPParamGroup : mp_policy
          MixedPrecisionPolicy --o FSDPState : _mp_policy
          OffloadPolicy --o FSDPParamGroup : offload_policy
          FSDPMeshInfo --o FSDPParam : mesh_info
          FSDPMeshInfo --o FSDPParamGroup : mesh_info
          TrainingState --o FSDPParamGroup : _training_state
          ParamModuleInfo --o FSDPParam : _module_info
          FSDPState --o FSDPStateContext : iter_forward_root
          LaunchConfig --o elastic_launch : _config
          PipeInfo --o _PipelineStage : pipe_info
          _PipelineStageBase --o PipelineScheduleSingle : _stage
          RRef --o HybridModel : remote_em_rref
          RRef --o HybridModel : remote_net_rref
          RRef --o Trainer : remote_em_rref
          RRef --o Trainer : remote_net_rref
          Shard --o FSDPParam : fsdp_placement
          Transform --o _InverseTransform : _inv
          DraftExportReport --o ExportedProgram : _report
          FailureType --o FailureReport : failure_type
          ExportGraphSignature --o CollectTracepointsPass : sig
          ExportGraphSignature --o GraphModuleSerializer : graph_signature
          ExportGraphSignature --o _TensorParallelTransformPass : graph_signature
          ExportGraphSignature --o ExportedProgram : _graph_signature
          UnflattenedModule --o Graph : owning_module
          Tracer --o TracingOpsHandler : tracer
          Tracer --o Graph : _tracer_cls
          AttrProxy --o _ModuleStackTracer : proxy_type
          ShapeEnv --o SizeVarAllocator : shape_env
          Graph --o FakeTensorUpdater : graph
          Graph --o InterpreterModule : graph
          Graph --o _ModuleFrame : flat_graph
          Graph --o MklSubgraph : fx_graph
          Graph --o DecompositionInterpreter : new_graph
          Graph --o GraphModule : graph
          Graph --o GraphModule : _graph
          Graph --o TransformerTracer : graph
          Graph --o SubgraphMatcher : pattern
          Graph --o GraphAppendingTracer : graph
          GraphModule --o WrapperBackend : gm
          GraphModule --o FxGraphHashDetails : gm
          GraphModule --o CompiledFxGraphConstantsWithGm : gm
          GraphModule --o _NSGraphMatchableSubgraphsIterator : gm
          GraphModule --o ModelReport : _model
          GraphModule --o Pipe : split_gm
          GraphModule --o Partitioner : graph_module
          GraphModule --o _ModuleStackTracer : scope_root
          GraphModule --o GraphTransformObserver : gm
          GraphModule --o CapabilityBasedPartitioner : graph_module
          GraphModule --o _MinimizerBase : module
          GraphModule --o FxNetAccNodesFinder : module
          GraphModule --o _SplitterBase : module
          GraphModule --o FxNetAccFusionsFinder : module
          GraphModule --o Analysis : module
          GraphModule --o Modularize : module
          GraphModule --o _ModuleNode : _reference_module
          Node --o NormalizedLinearNode : node
          Node --o NormalizedMatmulNode : node
          Node --o InterpreterShim : current_node
          Node --o DAGNode : submodule_node
          Node --o Proxy : node
          Node --o _LeafNode : _node
          _MinimizerSettingBase --o _MinimizerBase : settings
          OperatorSupportBase --o CapabilityBasedPartitioner : operator_support
          OperatorSupportBase --o FxNetAccNodesFinder : operator_support
          OperatorSupportBase --o _SplitterBase : operator_support
          _SplitterSettingBase --o _SplitterBase : settings
          Proxy --o BackwardHookVariable : proxy
          Proxy --o TensorVariable : proxy
          Proxy --o Attribute : root
          Scope --o ScopeContextManager : _scope
          TensorType --o TensorType : __origin__
          JitTypeTraceStore --o JitTypeTraceConfig : s
          Library --o CustomOp : _lib
          CausalVariant --o CausalBias : variant
          Module --o OptimizedModule : _orig_mod
          Module --o NNModuleVariable : value
          Module --o FunctionalModule : stateless_model
          Module --o FunctionalModuleWithBuffers : stateless_model
          Module --o ActivationSparsifier : model
          Module --o NnapiModule : shape_compute_module
          Module --o FSDPMemTracker : _root_mod
          Module --o FlatParamHandle : _fully_sharded_module
          Module --o _ExecutionInfo : curr_module
          Module --o FullyShardedDataParallel : _fsdp_wrapped_module
          Module --o _PipelineStageBase : submod
          Module --o Partitioner : torch_module
          Module --o Interpreter : module
          Module --o RestoreParameterAndBufferNames : original_nn_module
          Module --o ModuleWithDelay : module
          Parameter --o MyModule : param1
          _OrthMaps --o _Orthogonal : orthogonal_map
          OnnxRegistry --o OnnxFunctionDispatcher : onnx_registry
          ResolvedExportOptions --o Exporter : options
          Diagnostic --o RuntimeErrorWithDiagnostic : diagnostic
          DiagnosticContext --o Analysis : diagnostic_context
          DiagnosticContext --o FxOnnxInterpreter : diagnostic_context
          DiagnosticContext --o OnnxFunctionDispatcher : diagnostic_context
          DiagnosticContext --o _TypePromotionInterpreter : diagnostic_context
          _ModuleStackMeta --o _ModuleNode : _stack_meta
          TypePromotionTable --o _TypePromotionInterpreter : type_promotion_table
          LRScheduler --o _enable_get_lr_call : o
          Optimizer --o PostLocalSGDOptimizer : optim
          Optimizer --o CyclicLR : optimizer
          Optimizer --o LRScheduler : optimizer
          Optimizer --o LambdaLR : optimizer
          Optimizer --o MultiplicativeLR : optimizer
          Optimizer --o OneCycleLR : optimizer
          Optimizer --o ReduceLROnPlateau : optimizer
          Optimizer --o SequentialLR : optimizer
          Optimizer --o OptimizerInfo : optim_cls
          DiGraph --o PackagingError : dependency_graph
          Importer --o PackagePickler : importer
          Importer --o PackageUnpickler : _importer
          _SysImporter --o PackageExporter : importer
          OpTree --o DataFlowGraph : _op_tree
          profile --o Pattern : prof
          UntypedStorage --o UntypedStorageVariable : example_value
          _dispatch_dtypes --o ModuleInfo : dtypes
          ModelArgs --o Transformer : model_args
          MyClass --o MyClass : other
          MyClass --o MyClass : other
          LeafSpec --o FlattenInputWithTreeSpecValidationInputStep : _spec
          LeafSpec --o FlattenOutputWithTreeSpecValidationOutputStep : _spec
          TreeSpec --o PytreeThunk : spec
          TreeSpec --o FlattenInputWithTreeSpecValidationInputStep : _spec
          TreeSpec --o FlattenOutputWithTreeSpecValidationOutputStep : _spec
          TreeSpec --o DTensorConverter : flatten_args_spec
          TreeSpec --o DTensorConverter : flatten_kwargs_spec
          Colorize --o Table : _colorize
          Serialization --o CopyIfCallgrind : _serialization
          DataChunk --o GrouperIterDataPipe : wrapper_class
          IterDataPipe --o _ChildDataPipe : main_datapipe
          IterDataPipe --o _ForkerIterDataPipe : main_datapipe
          IterDataPipe --o FileListerIterDataPipe : datapipe
          IterDataPipe --o UnBatcherIterDataPipe : datapipe
          IterDataPipe --o ShardingFilterIterDataPipe : source_datapipe
          Dataset --o DistributedSampler : dataset
          FlopCounterMode --o _FlopCounterMode : counter
          WeakIdRef --o WeakIdKeyDictionary : ref_type
  
       </div>
  </body>
</html>
