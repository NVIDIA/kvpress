{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from transformers import pipeline\n",
    "\n",
    "from kvpress import SimLayerKVPress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pipeline\n",
    "device = \"cuda:0\"\n",
    "ckpt = \"Qwen/Qwen2.5-1.5B-Instruct\"  \n",
    "pipe = pipeline(\n",
    "    \"kv-press-text-generation\", \n",
    "    model=ckpt, \n",
    "    device=device, \n",
    "    torch_dtype=\"auto\", \n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test data for both prefilling and decoding\n",
    "context = \"\"\"SimLayerKV is a method for efficient transformer inference that identifies and optimizes \n",
    "lazy attention layers. It works in two phases: prefilling and decoding. During prefilling, it analyzes \n",
    "the last w_last tokens to identify lazy layers. During decoding, it examines the attention patterns of \n",
    "the first generated token.\"\"\"\n",
    "\n",
    "question = \"\\nWhat are the two phases of SimLayerKV?\"\n",
    "\n",
    "# Tokenize\n",
    "tokens = pipe.tokenizer(context, return_tensors=\"pt\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Test prefilling phase\n",
    "press = SimLayerKVPress(\n",
    "    initial_tokens=4,\n",
    "    recent_tokens=1024,\n",
    "    w_last=32,\n",
    "    window_size=32,\n",
    "    compression_ratio=0.85 # according to Original implmentation for qwen model 0.85 compression or threshold \n",
    ")\n",
    "\n",
    "print(\"Testing Prefilling Phase:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs_without_press = pipe.model(**tokens, output_hidden_states=True)\n",
    "\n",
    "with torch.no_grad(), press(pipe.model):\n",
    "    output_with_press = pipe.model(**tokens)\n",
    "\n",
    "print(f\"Original cache shape: {outputs_without_press.past_key_values[0][0].shape}\")\n",
    "print(f\"Compressed cache shape: {output_with_press.past_key_values[0][0].shape}\")\n",
    "\n",
    "\n",
    "\n",
    "# Test decoding phase\n",
    "print(\"\\nTesting Decoding Phase:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Generate with press\n",
    "output = pipe(\n",
    "    context, \n",
    "    question=question, \n",
    "    press=press,\n",
    "    max_new_tokens=150,\n",
    "  \n",
    ")\n",
    "print(\"Generated Answer:\")\n",
    "print(output[\"answer\"])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
